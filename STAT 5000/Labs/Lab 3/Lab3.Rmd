---
output:
  pdf_document: default
  html_document: default
  word_document: default
---
[Stat 5000]{.smallcaps}[Lab #3]{.smallcaps}\
[Fall 2024]{.smallcaps} [Due Tue Sep 17th]{.smallcaps}
[Name: Sam Olson]{.smallcaps} \

**[Assignment]{.underline}**

1. Conduct the t-test for the SMS speed example in SAS and complete the following exercises:

> 1. Using the formula from the notes, calculate by hand a 95% confidence interval for the difference in the two treatment means. Use $t_{28,0.975}=2.0484$.

Formula: 
$$\text{95\% Confidence Interval} \space = (\bar{Y_1} - \bar{Y_2}) \pm t_{28,0.975} S_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}$$
Where: 
$$S_p = \frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1+n_2-2}$$

Calculation
```{r}
# Load Data 
library(readr)
smsspeed_1 <- read_csv("C:/Users/samue/OneDrive/Desktop/Iowa_State_PS/STAT 5000/Labs/Lab 3/smsspeed-1.csv")
```

```{r}
library(dplyr)
data1 <- smsspeed_1 %>% 
  filter(smsspeed_1$AgeGroup == "Over30")

data2 <- smsspeed_1 %>% 
  filter(smsspeed_1$AgeGroup == "Teens")

sampleMean1 <- mean(data1$`Own Phone`)
sampleMean2 <- mean(data2$`Own Phone`)
difference <- sampleMean1 - sampleMean2 
difference
```

```{r}
#Step 2: Finding standard deviation 
s1 <- sd(data1$`Own Phone`)
s2 <- sd(data2$`Own Phone`)

#Step 3: Finding sample size 
n1 <- length(data1$`Own Phone`)
n2 <- length(data2$`Own Phone`)

numerator <- (n1-1)*(s1^2) + (n2-1)*(s2^2)
denom <- n1 + n2 - 2
pooled <- sqrt( numerator / denom )  
sqrtFactor <- sqrt(1/n1 + 1/n2)

tStatDf <-  2.0484

rightSide <- tStatDf*pooled*sqrtFactor 

pooled
rightSide 
```

```{r}
lb <- difference - rightSide
ub <- difference + rightSide

lb
ub
```
This gives a 95% Confidence Interval for the Difference to be between (30.167, 57.857), where units are the difference in the amount of time it took Over 30-year olds compared to teens (I believe in seconds, but that would be wild, wouldn't it?)

\newpage

> 2.  Provide a screenshot of the SAS output and use it to verify your calculation.

```{r CI95, echo=FALSE, fig.cap="95% Confidence Interval SAS", out.width = '100%'}
knitr::include_graphics("95CI.png")
```

\newpage 

> 3.  Interpret the confidence interval in the context of the problem.

A 95% Confidence Interval can be interpreted as a calculated range within which we can be 95% certain the true effect (true difference between two groups) lies. 

Within the context of this particular question, it can be interpreted as: We are 95% Confident that the true difference between people Over 30 and Teens texting speeds is between 30.167 and 57.857 seconds; or is would take people Over 30 30.167 to 57.857 more seconds to type a specified message compared to Teens (within the context of being 95% confident). 

This may also be interpreted as a commentary on the procedure of calculating the Confidence Interval: If we repeated this procedure of experimentation and calculation and constructed their respective 95% confidence intervals, these confidence intervals would contain the true difference between Over 30 and Teens texting times 95% of the time. 

\newpage

2.  Use SAS to explore sample size determinations for the bone loss example using the **hypothesis testing method** and complete the following exercises:

> 1.  Explore the effect of changing just the significance level - For $\alpha = 0.01, 0.05, 0.1$, what are the resulting sample sizes? Summarize your findings in one concise sentence.

```{r Alpha, echo=FALSE, fig.cap="Alpha", out.width = '100%'}
knitr::include_graphics("significance.PNG")
```
Results: The resulting sample sizes for a given "Alpha" are as follows, in the format of Sample Size (resp. Alpha): 39 (0.01), 26 (0.05), and 21 (0.10). 

Findings: For greater significance levels we require an increasing number of samples per Group, and the amount these sample sizes increase by is non-linear, i.e. each decrease of $0.01$ in $\alpha$ (greater significant level) requires a larger number of samples to be added per Group compared to its prior significance level. 

\newpage

> 2.  Explore the effect of changing just the power - For $1-\beta = 0.99, 0.95, 0.9, 0.8, 0.7$, what are the resulting sample sizes? Summarize your findings in one concise sentence.

```{r Beta, echo=FALSE, fig.cap="Beta", out.width = '100%'}
knitr::include_graphics("power.PNG")
```

Results: The resulting sample sizes for a given "Nominal Power" are as follows, in the format of Sample Size (resp. Nominal Power): 50 (0.99), 42 (0.95), 34 0.90, 26 (0.80), and 21 (0.70). 

Findings: When changing just the power in relation to the required sample size, we see that greater power (smaller $\beta$) requires larger sample sizes, and the increase in sample sizes between power levels becomes larger and larger for smaller and smaller $\beta$'s. 

\newpage

> 3.  Explore the effect of changing just the true effect size - For $\delta = 1, 2, 3, 4, 5, 6$, what are the resulting sample sizes? Summarize your findings in one concise sentence.

```{r Delta, echo=FALSE, fig.cap="Effect Size", out.width = '100%'}
knitr::include_graphics("effectSize.PNG")
```

Results: The resulting sample sizes for a given "Mean Difference in Effect" are as follows, in the format of Sample Size (resp. Mean Diff): 394 (1), 100 (2), 45 (3), 26 (4), 17 (5), and 12 (6). 

Findings: We require larger sample sizes to detect smaller effect sizes, i.e. the larger the effect size, the smaller the calculated sample size required, holding all else equal. 

\newpage

> 4.  Explore the effect of changing just the estimated population variance - For $S_p^2 = 1, 4, 9, 16, 25, 36$, what are the resulting sample sizes? Summarize your findings in one concise sentence.

```{r stdDev, echo=FALSE, fig.cap="95% Confidence Interval SAS", out.width = '100%'}
knitr::include_graphics("stdDev.PNG")
```

Results: The resulting sample sizes for a given "Std Dev" are as follows, in the format of Sample Size (resp. Std Dev): 3 (1), 6 (2), 10 (3), 17 (4), 26 (5), and 37 (6). 

Findings: As the estimated population variance increases, we need increasing larger sample sizes, and the rate at which these sample sizes increase is increasing, e.g. the difference (change in sample size) between 5 and 6 Std Dev is larger than the difference between 1 and 2 Std Dev. 


\newpage


3.  Use SAS to explore sample size determinations for the bone loss example using the **confidence interval method** and complete the following exercises:

> 1.  Explore the effect of changing just the significance level - For $\alpha = 0.01, 0.05, 0.1$, what are the resulting sample sizes? Summarize your findings in one concise sentence.

```{r CI1, echo=FALSE, fig.cap="", out.width = '100%'}
knitr::include_graphics("CI1.PNG")
```

Results: The resulting sample sizes for a given "Alpha" are as follows, with a fixed power level, in the format of Sample Size (resp. Alpha for "Nominal Power" of 0.975): 66 (.01), 50 (0.05), and 42 (0.10). 

Findings: Consistent with the findings of Q2, albeit via a different method: Higher significance levels (lower $\alpha$) require significantly larger sample sizes per group. 


\newpage


> 2.  Explore the effect of changing just the true effect size - For $\delta = 1, 2, 3, 4, 5, 6$, what are the resulting sample sizes? Summarize your findings in one concise sentence.

```{r CI2, echo=FALSE, fig.cap="", out.width = '100%'}
knitr::include_graphics("CI2.PNG")
```
Results: The resulting sample sizes for a given "Mean Difference Effect" are as follows, in the format of Sample Size (resp. Mean Diff): 770 (1), 194 (2), 87 (3), 50 (4), 32 (5), and 23 (6). 

Findings: Consistent with the findings of Q2, 3. albeit for a different method: We require much larger sample sizes per Group for smaller differences between groups, and smaller sample sizes per Group for larger differences between groups. 

\newpage

> 3.  Explore the effect of changing just the estimated population variance - For $S_p^2 = 1, 4, 9, 16, 25, 36$, what are the resulting sample sizes? Summarize your findings in one concise sentence.

```{r CI3, echo=FALSE, fig.cap="", out.width = '100%'}
knitr::include_graphics("CI3.PNG")
```

Results: The resulting sample sizes for a given "Std Dev" are as follows, in the format of Sample Size (resp. Std Dev): 4 (1), 9 (2), 19 (3), 32 (4), 50 (5), and 71 (6). 

Findings: Consistent with the findings of Q2, 4. albeit for a different method: Increasing estimated population variance results in much larger required sample sizes, and the rate of increase for these samples grows larger as the variance increases. 

\newpage

> 4. Think about how the sample size determination using the confidence interval method relates to the standard error method. Summarize your findings in one concise sentence.

Results:

>> Confidence Interval Method

$$n_0 = 8(\frac{z_{1-\frac{\alpha}{2}}S_p}{w})^2$$
$$n = 8(\frac{t_{2(n_0-1),1-\frac{\alpha}{2}}S_p}{w})^2$$

>> Standard Error Method

$$n = \frac{2S_P^2}{(s.e.)^2}$$

Findings: One may consider the confidence interval method as a modification of the standard error method, in that both methods require and take as input the **pooled estimate of the population variance** to estimate the required sample size, the standard error is closely related to the "width" of the confidence interval method, and furthermore, the confidence interval method distinguishes itself through use of the z and t distributions as well (takes additional inputs compared to the standard error method). 

**Total:** 50 points **\# correct:** **%:**
