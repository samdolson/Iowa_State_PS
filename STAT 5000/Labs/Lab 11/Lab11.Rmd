---
title: "Lab11"
output: pdf_document
date: "2024-11-24"
---

Thank you for allowing me to be a bit stress free about omitting a few plots. 

Despite the culling, this is a bit long still. 
My apologies. 

```{r}
library(readr)
bodyfat <- read_table("bodyfat.txt", col_types = cols(density = col_skip()))
```

# 1. 

Summarize your findings from examining the pairwise scatterplots and correlation matrix.

```{r}
pairs(bodyfat)
```

```{r}
cor(bodyfat)
```

For our response variable (Y) of "fat" (bodyfat), all of our potential explanatory variables have at least a magnitude of 0.15, and a majority (all but one) appear to be positively linearly related to "fat". The strongest correlation (largest in magnitude) is approximately 0.79 for the explanatory variable abdomen, such that we overall have reason to believe that a linear fit between our explanatory variables with the response would be appropriate at first glance. 

However, when we compare the correlations and pairwise plots between explanatory variables we also observe potentially strong linear relationships (not always, but a fair number, particularly for explanatory variables of hip, thigh, and knee). Generally speaking, we observe fairly strong correlations between parts of the body close in proximity to one another, such as the three variables mentioned previously. 

Despite having some potential concerns about multicollinearity (to be explored in later questions), we also see that a number of explanatory variables have a very weak linear relationship between one another, such as ankle and age having only a magnitude of correlation of 0.08, or other body combinations with the "age" explanatory variable. This means we have potential reason to believe including more than one explanatory variable could be helpful for our model without excessive risk of multicollinearity. 

That is all to say: I believe we are tasked with identifying "strong" correlation by whether |r| > 0.7. To that end the following are "strong" (or "significant") correlations: (1) Fat (response variable) and abdomen, for just comparisons amongst explanatory variables, there are many, including (2) weight and neck, chest, abdomen, hip, thigh, knee, biceps, and wrist, (3) neck and chest, abdomen, hip, and wrist, (4) chest and weight, neck, abdomen, hip, thigh, knee, and biceps, (5) abdomen and fat, weight, neck, chest, hip, thigh, and knee, (6) hip and weight, chest, abdomen, thigh, knee, and biceps, and more. 

\newpage

# 2. 

Discuss whether the VIFs indicate any explanatory variables exhibiting extreme multicollinearity.

```{r}
full.fat <- lm(fat~., data=bodyfat)
summary(full.fat)
```

```{r}
library(car)
vif.fat <- vif(full.fat)
barplot(vif.fat, xaxt = "n", main = "Variance Inflation Factor", ylab = "VIF")
# barplot(vif.fat)
abline(h=10, col="red", lty=2)
axis(1, at = seq_along(vif.fat), labels = names(vif.fat), las = 2)
```

Given the prompt, "VIF values larger than 10 indicate severe multicollinearity", we observe three explanatory variables having a VIF value larger than 10 and have reason to suspect potential issues of multicollinearity. The explanatory variables of note are "weight", "abdomen", and "hip". 

\newpage 

# 3. 

Summarize the backward elimination method of model selection by providing:

```{r}
back.fat <- step(full.fat, direction="backward")
summary(back.fat)
```

## (a) 

An ordered list of which variable was removed from the model at each step;

Step 1: "chest" removed
Step 2: "bicep" removed
Step 3: "knee" removed
Step 4: "height" removed 

## (b) 

A list of which variables remained in the final model;

Variables kept in final model: "age", "weight", "neck", "abdomen", "hip", "thigh", "ankle", "foearm", and "wrist". 

## (c) 

A summary of the partial regression coefficients effects tests for the final model.

Final model partial regression coefficients that meet statistical significance to reject null hypothesis at the $\alpha = 0.05$ level: "forearm", "wrist", "thigh", "abdomen", "neck", "weight", and the "intercept" term. 

Final model partial regression coefficients that do not meet statistical significance to reject null hypothesis at the $\alpha = 0.05$ level: "ankle", "hip", and "neck". 

The statistical significance test is to determine whether there is evidence to reject the null hypothesis that the estimated beta coefficient is equal to zero (statistical significance referring to being statistically significant from zero) in the model that is composed of all the explanatory variables ("age", "weight", "neck", "abdomen", "hip", "thigh", "ankle", "foearm", and "wrist", and the "intercept" term). This is a test of whether be have reason to suspect the estimated slope parameter of the particular explanatory variable is equal to zero in the multiple linear regression that includes the explanatory variables noted previously. 

\newpage 

# 4. 

Summarize the forward selection method of model selection by providing:

```{r}
null.fat <- lm(fat~1, data=bodyfat)
summary(null.fat)

for.fat <- step(null.fat, scope=formula(full.fat), direction="forward")
summary(for.fat)
```

## (a) 

An ordered list of which variable was added to the model at each step;

Step 1: "abdomen" added
Step 2: "weight" added
Step 3: "forearm" added
Step 4: "wrist" added
Step 5: "age" added
Step 6: "ankle" added
Step 7: "thigh" added 
Step 8: "neck" added
Step 9: "hip" added

## (b) 

A list of which variables never entered the final model;

Never entered the final model: "height", "chest", "knee", and "biceps". 

## (c) 

A summary of the partial regression coefficients effects tests for the final model.

Final model partial regression coefficients that meet statistical significance to reject null hypothesis at the $\alpha = 0.05$ level: "abdomen", "weight", "forearm", "wrist", "age", "thigh", and the "intercept" term.       

Final model partial regression coefficients that do not meet statistical significance to reject null hypothesis at the $\alpha = 0.05$ level: "ankle", "hip", and "neck". 

The statistical significance test is to determine whether there is evidence to reject the null hypothesis that the estimated beta coefficient is equal to zero (statistical significance referring to being statistically significant from zero) in the model that is composed of all the explanatory variables ("age", "weight", "neck", "abdomen", "hip", "thigh", "ankle", "foearm", and "wrist", and the "intercept" term). This is a test of whether be have reason to suspect the estimated slope parameter of the particular explanatory variable is equal to zero in the multiple linear regression that includes the explanatory variables noted previously. 

\newpage 

# 5. 

Summarize the all-possible-subsets method of model selection by providing:

```{r}
library(leaps)
all.subsets <- regsubsets(fat~., data=bodyfat, method="exhaustive")
summary(all.subsets)
summary(all.subsets)$adjr2
```

## (a) 

Which model would you choose based on the adjusted $R^2$ values?

The highest adjusted $R^2$ value is 0.7511467 for the model 8, the model that uses: "ankle", "forearm", "wrist", "thigh", "abdomen", "neck", "weight", and "age", including an intercept term

Note: Of the models considered using the above method, I stand by my answer. However, it is worth noting that we can consider models with more explanatory variables (13 possible explanatory variables to chose from, and from playing around I found a "better"/larger Adjusted R squared value for a model with 9 explanatory variables). The above code only looks at up to 8 explanatory variables in its output, without messing around with the `nvmax` parameter in the function.

## (b) 

Which model would you choose based on the Mallow's $C_p$ criteria?

```{r}
summary_object <- summary(all.subsets)

summary(all.subsets)$cp

included_matrix <- summary_object$which
num_vars_per_model <- rowSums(included_matrix) + 1
num_vars_per_model

abs(summary(all.subsets)$cp - num_vars_per_model)
```

The "best" Mallow's $C_p$ value is the value that corresponds the closest to the number of explanatory variables in the model (including an intercept, also more "magnitude" in terms of signage). Given the above output, we'd choose model 6, which has the explanatory variables: "ankle", "forearm", "wrist", "abdomen", "weight", and "age", including an intercept term. 

## (c) 

Which model would you choose based on the $BIC$ values?

```{r}
summary(all.subsets)$bic

min(summary(all.subsets)$bic)
which.min(summary(all.subsets)$bic)
```

The "best" BIC value is the one that is minimized across the models considered. The BIC value is minimized in model 5, the model that uses the explanatory variables: "forearm", "wrist", "abdomen",  "weight", and "age", including an intercept term. 

\newpage 

# 6. 

Interpret the values of the estimated regression coefficients in the context of the study for:

```{r}
ageCat <- vector(mode="character", length=length(bodyfat$age))
ageCat[bodyfat$age<39] = "under39" # lower quartile
ageCat[bodyfat$age>52] = "over52" # upper quartile
ageCat[bodyfat$age>38 & bodyfat$age<53] = "mid" # middle 50%
bodyfat = cbind(bodyfat, ageCat)
```

```{r}
cat.fat <- lm(fat ~ ageCat + weight + neck + abdomen + hip + thigh +
ankle + forearm + wrist, data = bodyfat)
summary(cat.fat)
```

## (a) 

The two values corresponding to the categorical age variable;

Note: 
  - "Baseline" for comparisons is male individuals Age between 39 to 52
  - "Individuals" in the context of the study are 133 men, such that I'd argue the "population" we interpret with regards to is males 
  - "Fat" is not noted to have a particular unit in the Lab notes, though "weight" is denoted in "lbs". For that reason I refer to units of "fat" and not "pounds of"

We reject the null hypothesis at the $\alpha = 0.05$ level that there is no difference in mean bodyfat (mean response) between male individuals under 39 years of age compared to mean bodyfat for individuals between 39 to 52 years of age (units were not provided for the response), and holding all other explanatory variables in our model constant. This is to say we have evidence in favor of the alternative hypothesis, specifically that all else being equal, we expect individuals under 39 years of age to have on average 2.06578 units of bodyfat less than individuals between 39 to 52 years of age. 

We reject the null hypothesis at the $\alpha = 0.05$ level that there is no difference in mean bodyfat (mean response) between male individuals over 52 years of age compared to mean bodyfat for individuals between 39 to 52 years of age (again, with units of response variable not specified), and holding all other variables constant. This is to say we have evidence in favor of the alternative hypothesis, specifically that all else being equal, we expect individuals over 52 years of age to have on average 1.98494 units of bodyfat more than individuals between 39 to 52 years of age. 

## (b) 

One of the values corresponding to the quantitative variable of your choice.

We have evidence at the $\alpha = 0.05$ level to reject the null hypothesis that increasing the circumference of the abdomen is not associated with a mean change in the bodyfat of a male individual. This is evidence in favor of the alternative hypothesis, specifically that increasing the circumference of the abdomen by 1 cm is associated with an increase in mean bodyfat of 0.98274 units, all else being equal (holding all other explanatory variables constant). 

\newpage 

# 7. 

Summarize your findings from examining all the residual plots used to diagnose the MLR model assumptions. Are there any assumptions that aren’t met for this analysis?

```{r}
best.fat <- back.fat
summary(best.fat)
anova(best.fat)
```

```{r}
qqnorm(best.fat$residuals)
qqline(best.fat$residuals, col="red")
```

```{r}
library(MASS)
stdresids <- stdres(best.fat)
stdresids[which(abs(stdresids)>2)]
plot(best.fat$fitted.values, stdresids, main="MLR for Body Fat Study",
xlab="Fitted Values", ylab="Studentized Residuals")
abline(h=0, col="gray")
abline(h=-2, col="red", lty=2)
abline(h=2, col="red", lty=2)
```

```{r}
plot(best.fat)
```

Overall, the assumptions we are diagnosing are the equal variance, linearity, and normality assumptions. To that end: 

Residual Plot: Constant variance and form of the model (linearity) assumptions appear to be met, as the overall spread and distribution of residuals across fitted values appears as a random spread. Specifically, we have a random spread and not a funnel shape (for assessing constant variance), or other types of trend that would indicate a deviation from linearity. Also, we tend to see the same number of positive residuals as we do negative residual values (for assessing form of the model). Overall, our assumptions of equal variance as well as form of the model do not appear to be violated. 

QQ Plot: Residuals track and align well against the reference line, with some slight deviations at the tails of the distribution. This is evidence in favor of the normality assumption not being violated. 

Additionally, when looking at residual plots by each of the explanatory variable, we do not observe any obvious "clustering" or points, such that we also do not have evidence to reject the "fixed X" assumption being violated in our data, notwithstanding the observations and conclusions drawn previously regarding outliers.  

Note: The above findings are consistent when also considering the above plots using the studentized residuals. Furthermore, when we look at the residual plots of each explanatory variable, we also see that there are possibly outliers, which we will further investigate to identify if it is in fact an outlier and possibly a leverage/influential point. 

Additional plots given below, maybe they're above, who knows with this knitting business: 

Saving some space and headache by including code and only a select few outputs. 

```{r}
library(MASS)
stdresids <- stdres(best.fat)
plot(best.fat$model$age, 
     stdresids, 
     main="MLR for Body Fat Study",
     xlab="Age", 
     ylab="Studentized Residuals")
abline(h=0, col="red")

plot(best.fat$model$weight, 
     stdresids, 
     main="MLR for Body Fat Study",
     xlab="Weight", 
     ylab="Studentized Residuals")
abline(h=0, col="red")

# plot(best.fat$model$neck, 
#      stdresids, 
#      main="MLR for Body Fat Study",
#      xlab="Neck", 
#      ylab="Studentized Residuals")
# abline(h=0, col="red")
# 
# plot(best.fat$model$abdomen, 
#      stdresids, 
#      main="MLR for Body Fat Study",
#      xlab="Abdomen", 
#      ylab="Studentized Residuals")
# abline(h=0, col="red")
# 
# plot(best.fat$model$hip, 
#      stdresids, 
#      main="MLR for Body Fat Study",
#      xlab="Hip", 
#      ylab="Studentized Residuals")
# abline(h=0, col="red")
# 
# plot(best.fat$model$thigh, 
#      stdresids, 
#      main="MLR for Body Fat Study",
#      xlab="Thigh", 
#      ylab="Studentized Residuals")
# abline(h=0, col="red")
# 
# plot(best.fat$model$ankle, 
#      stdresids, 
#      main="MLR for Body Fat Study",
#      xlab="Ankle", 
#      ylab="Studentized Residuals")
# abline(h=0, col="red")
# 
# plot(best.fat$model$forearm, 
#      stdresids, 
#      main="MLR for Body Fat Study",
#      xlab="Forearm", 
#      ylab="Studentized Residuals")
# abline(h=0, col="red")
# 
# plot(best.fat$model$wrist, 
#      stdresids, 
#      main="MLR for Body Fat Study",
#      xlab="Wrist", 
#      ylab="Studentized Residuals")
# abline(h=0, col="red")
```

\newpage 

# 8. 

Summarize your findings from examining the case diagnostic values/plots. Are there any outliers, leverage points, or influential observations?

```{r}
stdresids <- studres(best.fat)
stdresids[which(abs(stdresids)>2)]
plot(best.fat$fitted.values, stdresids, main="MLR for Body Fat Study",
xlab="Fitted Values", ylab="Studentized Residuals")
abline(h=0, col="gray")
abline(h=-2, col="red", lty=2)
abline(h=2, col="red", lty=2)
```

Using the above plot of studentized residuals, we are able to potentially identify outliers in our dataset. We do observe some potential outliers using this method, where we look for residuals with a magnitude greater than 2. 

```{r}
leverage <- hatvalues(best.fat)
leverage[which(abs(leverage)>(20/length(leverage)))]
plot(leverage, type = 'h', main="MLR for Body Fat Study",
ylab="Leverage (hi)")
abline(h=(20/length(leverage)), col="red", lty=2)
```

```{r}
20/length(leverage)
```

We can find which observations have leverage exceeding $\pm 2(k+1)/n$ (0.1503759) where k (9) is the number of explanatory variables and n (133) is the total number of observations:

Using the above threshold values for leverage, and the above plot, we see there are a number of observations in our dataset with high leverage (there are leverage points). 

```{r}
cooks <- cooks.distance(best.fat)
# cooks[which(abs(cooks)>(2*sqrt(2/length(cooks))))]
plot(cooks, type = 'h', main="MLR for Body Fat Study",
ylab="Cook’s Distance (Di)")
abline(h=2*sqrt(2/length(leverage)), col="red", lty=2)
```

Cook's Threshold value: $\pm 2\sqrt{2/n} \approx 0.2452557$

Cook's Distance is used to evaluate potential influence points. From the above plot and using the specified threshold given, we see there is at least one influence point via this method. 

```{r}
dff <- dffits(best.fat)
# dff[which(abs(dff) > 2*sqrt(20/length(dff)))]
plot(abs(dff), type = 'h', main="MLR for Body Fat Study",
ylab="Absolute Value of DFFITS")
abline(h=2*sqrt(20/length(dff)), col="red", lty=2)
```

Another way to check for potential influence points is using DFFITS, with the threshold value of $\pm2\sqrt{(k + 1)/n} \approx 0.1503759$

Using the above method, we see there are now potentially two influential points in the dataset used in this study/lab. 

```{r}
dfb <- dfbetas(best.fat)
# dfb[which(abs(dfb) > 2/sqrt(length(dfb)))]
plot(dfb[,1], type = 'h', main="MLR for Body Fat Study",
ylab="DFBETA", xlab="(Intercept)")
abline(h=2/sqrt(length(dfb)), col="red", lty=2)
abline(h=-2/sqrt(length(dfb)), col="red", lty=2)
```

For brevity, cutting off some of these graphs/plots. 

```{r}
# age, weight, neck, abdomen, hip, thigh, ankle, forearm, wrist

# dfb <- dfbetas(best.fat)
# # dfb[which(abs(dfb) > 2/sqrt(length(dfb)))]
# plot(dfb[,1], type = 'h', main="MLR for Body Fat Study",
# ylab="DFBETA", xlab="(Intercept)")
# abline(h=2/sqrt(length(dfb)), col="red", lty=2)
# abline(h=-2/sqrt(length(dfb)), col="red", lty=2)

plot(dfb[,2], 
     type = 'h', 
     main="MLR for Body Fat Study",
     ylab="DFBETA", 
     xlab="Age")
abline(h=2/sqrt(length(dfb)), col="red", lty=2)
abline(h=-2/sqrt(length(dfb)), col="red", lty=2)

plot(dfb[,3], 
     type = 'h', 
     main="MLR for Body Fat Study",
     ylab="DFBETA", 
     xlab="Weight")
abline(h=2/sqrt(length(dfb)), col="red", lty=2)
abline(h=-2/sqrt(length(dfb)), col="red", lty=2)

plot(dfb[,4], 
     type = 'h', 
     main="MLR for Body Fat Study",
     ylab="DFBETA", 
     xlab="Neck")
abline(h=2/sqrt(length(dfb)), col="red", lty=2)
abline(h=-2/sqrt(length(dfb)), col="red", lty=2)

plot(dfb[,5], 
     type = 'h', 
     main="MLR for Body Fat Study",
     ylab="DFBETA", 
     xlab="Abdomen")
abline(h=2/sqrt(length(dfb)), col="red", lty=2)
abline(h=-2/sqrt(length(dfb)), col="red", lty=2)

# plot(dfb[,6], 
#      type = 'h', 
#      main="MLR for Body Fat Study",
#      ylab="DFBETA", 
#      xlab="Hip")
# abline(h=2/sqrt(length(dfb)), col="red", lty=2)
# abline(h=-2/sqrt(length(dfb)), col="red", lty=2)
# 
# plot(dfb[,7], 
#      type = 'h', 
#      main="MLR for Body Fat Study",
#      ylab="DFBETA", 
#      xlab="Thigh")
# abline(h=2/sqrt(length(dfb)), col="red", lty=2)
# abline(h=-2/sqrt(length(dfb)), col="red", lty=2)
# 
# plot(dfb[,8], 
#      type = 'h', 
#      main="MLR for Body Fat Study",
#      ylab="DFBETA", 
#      xlab="Ankle")
# abline(h=2/sqrt(length(dfb)), col="red", lty=2)
# abline(h=-2/sqrt(length(dfb)), col="red", lty=2)
# 
# plot(dfb[,9], 
#      type = 'h', 
#      main="MLR for Body Fat Study",
#      ylab="DFBETA", 
#      xlab="Forearm")
# abline(h=2/sqrt(length(dfb)), col="red", lty=2)
# abline(h=-2/sqrt(length(dfb)), col="red", lty=2)
# 
# plot(dfb[,10], 
#      type = 'h', 
#      main="MLR for Body Fat Study",
#      ylab="DFBETA", 
#      xlab="Wrist")
# abline(h=2/sqrt(length(dfb)), col="red", lty=2)
# abline(h=-2/sqrt(length(dfb)), col="red", lty=2)
```

There is also the DFBETA method for determining influential points, using the threshold value of $\pm2/\sqrt{n}$ for each explanatory variable. From (a number of these plots), we also identify potential influential points in the dataset. 
Overall, we do appear to have a number of outliers, leverage points, and influential points in our data, as shown in a number of plots above. We should consider transforming our data and rerunning a number of our operations to identify a next "best" model and also to diagnose our assumptions for that model. 
