---
title: "HW12"
output: pdf_document
date: "2024-12-08"
---

[Stat 5000]{.smallcaps}
[Homework #11]{.smallcaps}\
[Fall 2024]{.smallcaps} 
[due Fri, December 6th @ 11:59 pm]{.smallcaps}
[Name: Sam Olson]{.smallcaps} \
[Collaborators: Andrew, Ethan, **The Hatman**]{.smallcaps} \

```{r, eval = T, results = F, echo = F, warning = F, message = F}
library(knitr)
```

# Q1 

For each of the following models $Y_i$ are the responses, $\beta_i$ are parameters, $X_i$ are fixed values, and $\epsilon_i$ denotes random errors with variance $\sigma^2$. Indicate if it is a linear model, a nonlinear model, or an intrinsically linear model (a nonlinear model that can be transformed into a linear model).

## (a) 

$$
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 \log(X_{2i}) + \beta_3 X_{3i} + \epsilon_i \text{ with }  E(\epsilon_i) = 0
$$

Linear Model: This is a Linear Model because the response $Y_i$ depends linearly on the parameters $\beta_0, \beta_1, \beta_2, \beta_3$, and the predictor transformations (e.g., $\log(X_{2i})$) are fixed functions of the independent variables.

## (b) 

$$
Y_i = \beta_0 \exp(\beta_1 X_{1i}) + \epsilon_i \text{ with }  E(\epsilon_i) = 0
$$

This is a Nonlinear Model because the parameter $\beta_1$ appears in the exponent, making the model nonlinearly dependent on $\beta_1$, and it cannot be transformed into a linear model. This is not intrinsically linear because the $\beta_0$ and $\epsilon_i$ terms are not within the exponent, such that we cannot easily transform it into a linear model via the log function (cannot separate the function into a combination of linear terms). 

## (c) 

$$
Y_i = \left[1 + \exp(\beta_0 + \beta_1 X_{1i} + \epsilon_i)\right]^{-1} \text{ with }  E(\epsilon_i) = 0
$$

This is an intrinsically linear model because the parameters $\beta_0$ and $\beta_1$ appear inside a nonlinear transformation ($\exp$) and within a complex function $\left[1 + \exp(\cdot)\right]^{-1}$, making the model nonlinearly dependent on the parameters. However, we can transform it into a linear model using the log function. 

## (d) 

$$
Y_i = (\beta_0 + \beta_1 X_{1i})\epsilon_i \text{ with } E(\epsilon_i) = 1
$$

This is a an intrinsically linear model because the error term $\epsilon_i$ is multiplied by the parameter-dependent expression $(\beta_0 + \beta_1 X_{1i})$, making the model nonlinearly dependent on the parameters. However, because $E(\epsilon_i) = 1$, it then holds that: 

$$
E(Y_i) = E((\beta_0 + \beta_1 X_{1i})\epsilon_i) = E(\beta_0 + \beta_1 X_{1i}) = \beta_0 + E(\beta_1X_{1, i})
$$

Such that we can transform the given function into a linear model. 

## (e) 

$$
Y_i = \epsilon_i \exp(\beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i}) \text{ with }  E(\epsilon_i) = 1
$$

This is a an intrinsically linear model because the error term $\epsilon_i$ is multiplied by the parameter-dependent expression $exp(\beta_0 + \beta_1 X_{1i})$, making the model nonlinearly dependent on the parameters. However, because $E(\epsilon_i) = 1$, and because we have the monotonic function log for which to transform the function, it then holds that: 

$$
E(Y_i) = E(exp(\beta_0 + \beta_1 X_{1i})\epsilon_i) = E(exp(\beta_0 + \beta_1 X_{1i}))
$$

Using the log transformation we are able to transform our given function into a linear model. 

\newpage 

# Q2 

Only square, nonsingular matrices have inverses, but every matrix has a generalized inverse. For example, let

$$
A = 
\begin{bmatrix}
1 \\
2 \\
5 \\
-2
\end{bmatrix}
$$

Show that $B = [1 \ 0 \ 0 \ 0]$ satisfies the definition of a generalized inverse for A.

To show that $B = [1 \ 0 \ 0 \ 0]$ satisfies the definition of a generalized inverse for A, we need to confirm that:

$$
ABA = A
$$

To that end, we are given:

$$
A = 
\begin{bmatrix}
1 \\
2 \\
5 \\
-2
\end{bmatrix}, \quad
B = [1 \ 0 \ 0 \ 0]
$$

Such that: 

$$
AB = 
\begin{bmatrix}
1 \\
2 \\
5 \\
-2
\end{bmatrix}
[1 \ 0 \ 0 \ 0] = 
\begin{bmatrix}
1 & 0 & 0 & 0 \\
2 & 0 & 0 & 0 \\
5 & 0 & 0 & 0 \\
-2 & 0 & 0 & 0
\end{bmatrix}
$$

$$
ABA = AB \cdot A =
\begin{bmatrix}
1 & 0 & 0 & 0 \\
2 & 0 & 0 & 0 \\
5 & 0 & 0 & 0 \\
-2 & 0 & 0 & 0
\end{bmatrix}
\begin{bmatrix}
1 \\
2 \\
5 \\
-2
\end{bmatrix}
$$

$$
ABA =
\begin{bmatrix}
1 ( 1) + 0 ( 2) + 0 (5) + 0 (-2) \\
2 ( 1) + 0 ( 2) + 0 (5) + 0 (-2) \\
5 ( 1) + 0 ( 2) + 0 (5) + 0 (-2) \\
-2 ( 1) + 0 ( 2) + 0 (5) + 0 (-2)
\end{bmatrix}
=
\begin{bmatrix}
1 \\
2 \\
5 \\
-2
\end{bmatrix} = A
$$

Our goal was to verify ABA = A. Since we confirmed this equation holds, we have confirmed B is a generalized inverse of A.

\newpage

# Q3 

Consider the linear model $\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$ where

$$
\mathbf{Y} = 
\begin{bmatrix}
Y_{11} \\
Y_{12} \\
Y_{21} \\
Y_{22} \\
Y_{23} \\
Y_{24} \\
Y_{31} \\
Y_{32}
\end{bmatrix}, \quad
\mathbf{X} = 
\begin{bmatrix}
1 & 1 & 0 & 0 \\
1 & 1 & 0 & 0 \\
1 & 0 & 1 & 0 \\
1 & 0 & 1 & 0 \\
1 & 0 & 1 & 0 \\
1 & 0 & 1 & 0 \\
1 & 0 & 0 & 1 \\
1 & 0 & 0 & 1
\end{bmatrix}, \quad
\boldsymbol{\beta} = 
\begin{bmatrix}
\mu \\
\alpha_1 \\
\alpha_2 \\
\alpha_3
\end{bmatrix}, \quad
\boldsymbol{\epsilon} \sim N(0, \sigma^2 \mathbf{I}).
$$

Determine which of the following linear functions, $c_i^T \boldsymbol{\beta}$, of the model parameters are estimable. Briefly justify your answer. For estimable functions only, find a constant matrix $\mathbf{A}_i$ such that $\mathbf{A}_i \mathbb{E}(\mathbf{Y}) = c_i^T \boldsymbol{\beta}$.

## (a) 

$c_i^T \boldsymbol{\beta} = \alpha_1 - \frac{1}{2}(\alpha_2 + \alpha_3)$

Estimable: The function $c_i^T \boldsymbol{\beta} = \alpha_1 - \frac{1}{2}(\alpha_2 + \alpha_3)$ can be expressed as a linear combination of the rows of the design matrix $\mathbf{X}$, specifically using weights $a_1 = 1, a_2 = -\frac{1}{2}, a_3 = -\frac{1}{2}$, which satisfy the row space condition.

Constant matrix:

$$
\mathbf{A}_i =
\begin{bmatrix}
1 & 0 & -\frac{1}{2} & 0 & 0 & 0 & -\frac{1}{2} & 0
\end{bmatrix}
$$

## (b) 

$c_i^T \boldsymbol{\beta} = 3\mu + \alpha_1 + 2\alpha_2$

Estimable: The function $c_i^T \boldsymbol{\beta} = 3\mu + \alpha_1 + 2\alpha_2$ is estimable because despite $\mu$ is confounded with the group means ($\alpha_1, \alpha_2, \alpha_3$), we are estimating a linear combination that includes the grand mean.  

Of note: 

$$
(\mu + \alpha_1) + 2(\mu + \alpha_2) = 3\mu + \alpha_1 + 2\alpha_2
$$

And 

$$
\mathbf{A}_i =
\begin{bmatrix}
1 & 0 & 2 & 0 & 0 & 0 & 0 & 0
\end{bmatrix}
$$

## (c) 

$c_i^T \boldsymbol{\beta} = \alpha_2 + \alpha_3$

Not Estimable: The function $c_i^T \boldsymbol{\beta} = \alpha_2 + \alpha_3$ is not estimable because we do not have full rank of our design matrix, meaning that estimates of $\alpha_2 + \alpha_3$ are confounded by the inclusion of $\mu$, the grand mean. 

For example: 

$$
(\mu + \alpha_2) + (\mu + \alpha_3) = 2\mu + \alpha_2 + \alpha_3 
$$

And

$$
(\mu + \alpha_2) - (\mu + \alpha_3) = \alpha_2 - \alpha_3
$$

## (d) 

$c_i^T \boldsymbol{\beta} = 3\mu - \alpha_1 - \alpha_2 - \alpha_3$

Not Estimable: The function $c_i^T \boldsymbol{\beta} = 3\mu - \alpha_1 - \alpha_2 - \alpha_3$ simplifies to $3\mu$, which is not estimable because $\mu$ is confounded with the group means ($\alpha_1, \alpha_2, \alpha_3$) due to the design matrix not having full rank. 

\newpage

# Q4 

A food scientist performed an experiment to study the effects of combining two different fats and three different surfactants on the specific volume of bread loaves. Two batches of dough were made for each of the six combinations of fat and surfactant. Ten loaves of bread were made from each batch of dough and the average volume of the ten loaves was recorded for each batch. In total, there are 12 observations. Consider the two-way ANOVA model

$$
Y_{ijk} = \mu + \alpha_i + \tau_j + (\alpha\tau)_{ij} + \epsilon_{ijk}
$$ 

where $\epsilon_{ijk} \sim N(0, \sigma^2)$,

and $Y_{ijk}$ denotes the average of the volumes of ten loaves of bread made from the k-th batch of dough using the i-th fat and the j-th surfactant. 

Determine which of the following linear functions of the model parameters are estimable. Briefly justify your answer.

## (a) 

$\mu$ 

Not Estimable: The parameter $\mu$ is not estimable because it is confounded with the main effects $\alpha_i$ (fat) and $\tau_j$ (surfactant), as well as the interaction effects $(\alpha \tau)_{ij}$. The design matrix has rank deficiency due to the identifiability constraints (e.g., sum-to-zero) imposed on these effects, preventing $\mu$ from being uniquely determined.

## (b) 

$\alpha_1 - \alpha_2$

Not Estimable: The function $\alpha_1 - \alpha_2$ is not estimable because we cannot isolate the main effects without the inclusion of the associated interaction terms, i.e. 

$$
E(Y_{1, 1,.}) - E(Y_{2,1,.}) = E(\mu + \alpha_1 + \tau_1 + (\alpha\tau_{1,1})) - E(\mu + \alpha_2 + \tau_1 + (\alpha\tau_{2,1})) = \alpha_1 - \alpha_2 +  (\alpha\tau_{1,1}) - (\alpha\tau_{2,1})
$$

## (c) 

$(\alpha \tau)_{12}$

Not Estimable: The interaction effect $(\alpha \tau)_{12}$ is not estimable because it cannot be uniquely separated from the other interaction effects or main effects due to the rank deficiency of the design matrix and the sum-to-zero constraints imposed on the interaction terms.

## (d) 

$(\alpha \tau)_{11} - (\alpha \tau)_{12}$

Not Estimable: The function $(\alpha \tau)_{11} - (\alpha \tau)_{12}$ is not estimable because we cannot isolate the interaction effects without the inclusion of the associated main terms, i.e. 

$$
E(Y_{1, 1,.}) - E(Y_{1,2,.}) = E(\mu + \alpha_1 + \tau_1 + (\alpha\tau_{1,1})) - E(\mu + \alpha_1 + \tau_2 + (\alpha\tau_{1,2})) = \tau_1 - \tau_2 +  (\alpha\tau_{1,1}) - (\alpha\tau_{1,2})
$$

## (e) 

$(\alpha \tau)_{11} - (\alpha \tau)_{12} - (\alpha \tau)_{21} + (\alpha \tau)_{22}$

Estimable: The function $(\alpha \tau)_{11} - (\alpha \tau)_{12} - (\alpha \tau)_{21} + (\alpha \tau)_{22}$ is estimable because it is a contrast of interaction effects, which are not affected by the rank deficiency imposed by the sum-to-zero constraints on the interaction terms.

$$
E(Y_{1, 1,.}) - E(Y_{1,2,.}) - E(Y_{2, 1,.}) + E(Y_{2,2,.}) = (\alpha \tau)_{11} - (\alpha \tau)_{12} - (\alpha \tau)_{21} + (\alpha \tau)_{22}
$$