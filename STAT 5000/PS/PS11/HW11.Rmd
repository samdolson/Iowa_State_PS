---
output: pdf_document
---

[Stat 5000]{.smallcaps}
[Homework #11]{.smallcaps}\
[Fall 2024]{.smallcaps} 
[due Fri, December 6th @ 11:59 pm]{.smallcaps}
[Name: Sam Olson]{.smallcaps} \
[Collaborators: Ethan, Ben, Craig, Sabrina, **The Hatman**]{.smallcaps} \

# Outline:
  - REDO 2, CHECK SAS 
  - 1 overall: include absolutely no outputs or plots? 
  - 1 (d) "best" model 
  - 2 (d) actually calculate and give precise threshold values?
  - 3 (c) iii, general interpretation; see lab when solutions posted
  - 3 (e) inclusion of intercept? 
  - 2 (b): "reduced model" choice of variables? 

```{r, eval = T, results = F, echo = F, warning=F, message=F}
library(knitr)
```

```{r, eval = F, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("XXX.png")
```

# 1

Berkeley Guidance Study 

The dataset is located in `BGSgirls2.txt`. It contains one line of data for each of 70 girls with the following variables:

- ID: Girl identification number
- WT2: Weight (kg) at 2 years
- HT2: Height (cm) at 2 years
- WT9: Weight (kg) at 9 years
- HT9: Height (cm) at 9 years
- LG9: Leg circumference (cm) at 9 years
- ST9: Strength (kg) at 9 years
- WT18: Weight (kg) at 18 years
- HT18: Height (cm) at 18 years
- LG18: Leg circumference (cm) at 18 years
- ST18: Strength (kg) at 18 years
- BMI: Body Mass Index at 18 years
- SOMA: Somatotype (SOMA), on a scale from 1 (very thin) to 7 (very obese)

USE SAS TO COMPLETE THE FOLLOWING EXERCISES: 

## (a) 

Fit a multiple regression model:

$$
BMI_i = \beta_0 + \beta_1 \text{WT2}_i + \beta_2 \text{HT2}_i + \beta_3 \text{WT9}_i + \beta_4 \text{HT9}_i + \beta_5 \text{ST9}_i + \epsilon_i
$$

for i=1, ..., 70. 

And use the following diagnostics to assess model assumptions. (Do not submit the output; just examine the results and briefly describe the insight provided by each).

### i. 

Normal Q-Q plot of residuals and the related Shapiro-Wilk test.

The QQ plot closely aligns with the reference line within the first theoretical quantile, but there are deviations past the first (positive/negative) quantile. 

The Shapiro-Wilk test provides a small p-value (<0.0001) such that we would have evidence to reject the null hypothesis that the residuals are normally distributed. 

Overall, we have reason to suspect our normality assumption is being violated. 

### ii. 

Plot of the residuals versus the estimates of the conditional means for BMI

We generally observe a random spread of residual values across fitted values. However, there are two negative residuals around predicted BMI 25+, such that we'd consider these points to either be candidates for removal (due to being outliers) or that we may in fact be violating our assumption of form of the model. 

This notwithstanding, we generally have reason to believe our form of the model and constant variance assumptions are not being violated. 

### iii. 

Individual plots of the residuals versus each of the five explanatory variables

Plots of residuals versus each of the five explanatory variables are generally consistent with the depictions present from the residual v. fitted values graph, insomuch as we may have a few problematic points to address but generally do not have reason to suspect our assumptions are being violated. 

## (b) 

Given that an outlier should be detected from part (a), refit the model and recheck the diagnostics listed in (a) to assess whether model assumptions are violated or not. (HINT: You can filter observations from the dataset using the where statement inside the reg procedure in SAS.)

The QQ plot looks better, insomuch as it more closely tracks with the reference line, and this is consistent with a larger Shapiro-Wilk test statistics, such that we would not have evidence to reject the null hypothesis that the residuals are normally distributed. As such we have reason not to suspect the normality assumption is being violated as it was in part (a). 

Furthermore, the residual plots consistently (across the x-axis of fitted values as well as individually across the explanatory variables) appear to be randomly spread, such that our constant variance and form of the model assumptions are likely not being violated either. 

However, it is worth noting that we still appear to have a potential outlier in our diagnostic plots. 

\newpage 

## (c) 

For the 69 observations (without the outlier that was detected from part (a)), use a backward selection procedure to search for a model using $\alpha_{stay}$ = 0.05. For this question, just consider the five variables mentioned in part (a): WT2, HT2, WT9, HT9, ST9. For your final model, report the estimated coefficients and their standard errors.

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q1c.png")
```

In the final model from the method described, we have: 

  Variable: Intercept
    Coefficient: 29.648
    Standard Error: 5.531
  
  Variable: HT2
    Coefficient: -0.190
    Standard Error: 0.070
  
  Variable: WT9
    Coefficient: 0.262
    Standard Error: 0.0409
  
\newpage 

## (d) 

For the 69 observations (without the outlier that was detected from part (a)), check all possible models that could be constructed using at most the five variables WT2, HT2, WT9, HT9, ST9 and then give the best one that you recommend. Justify your choice.

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q1d1.png")
```

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q1d2.png")
```

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q1d3.png")
```

My recommendation is to use the following model (choice of explanatory variables, while also including an intercept term): WT2, WT9, HT9 model. 

This model has the best Adjusted $R^2$ for a model with 3 explanatory variables, and is the second best based on Adjusted $R^2$ compared to all models, and is only 0.0029 less than the first best. However, this model has the best Mallow's Cp value overall. 

\newpage 

## (e) 

Are there concerns about multicollinearity for the explanatory variables of the model you picked in part (d)?

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q1e.png")
```

VIF values are not especially large (less than cutoff for "moderate" of 3/5), so minimal issue with multicollinearity based on VIF criteria. 

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q1e2.png")
```

However, when looking at the Condition Index for the Eigenvalues, we do observe a rather high value (larger than 30), and the 93.13 corresponds to a significant proportion of variation for Interccept, WT9, and HT9, or an indication of potential extreme multicollinearity between WT9 and HT9. 

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q1e3.png")
```

We additionally double check our observation of potential multicollinearity by looking at the correlation between our explanatory variables. To that end: The above is further corroborated by the correlation coefficient between WT9 and HT9 being greater than 0.7 (0.73096). 

So overall, yes we do have reasonable concerns about multicollinearity for the model we chose in part (d). Unfortunate. 

\newpage 

# 2

Ames Housing (+25)

A dataset (introduced in the previous homework assignment) was collected from home sales in Ames, Iowa between 2006 and 2010. The variables collected are:

  - Year Built: The year the house was built
  - Basement Area (in sq. ft): The amount of area in the house below ground level
  - Living Area (in sq. ft): The living area in the home (includes Basement Area)
  - Total Room: The number of rooms in the house
  - Garage Cars: The number of cars that can be placed in the garage
  - Year Sold: The year the home was sold
  - Sale Price: The sale price of the home (the response variable)
  - Garage Size: S = Small (Garage Cars = 0,1) or L = Large (Garage Cars = 2+)
  - Age (in yrs.): Age of house = Year Sold - Year Built

Use SAS to complete the following exercises:

The data from 999 sales can be found in the file housing train.csv and for the remaining 1,924 sales in the file housing eval.csv in our courseâ€™s shared folder in SAS Studio. You will determine a final multiple linear regression model for predicting sale price from the explanatory variables: Basement Area, Living Area, Total Room, Garage Size, and Age.

## (a) 

Fit the full model using all 5 explanatory variables listed above to the training data
(housing train.csv).

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q2a.png")
```

And so it was fit. 

### i. 

Find and interpret the $R^2$ value for the full model.

78.31% of variability in Sales price can be explained using the multiple linear regression using Basement Area, Living Area, Total Room, Garage Size, and Age as explanatory variables (and including an intercept term). 

### ii. 

Interpret the value of the estimated regression coefficient corresponding to the Garage Size variable for the full model.

Increasing Garage Size by 1 car is associated with an increased Sales Price of $15,833, all else being equal.

\newpage 

## (b) 

Use forward selection to fit a reduced model to the training data using some subset of the 5 explanatory variables listed above. Provide an equation for the estimated MLR model.

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q2b.png")
```

For the preceding problems, I used the model with "Age" removed, corresponding to the above output and the following equation: 

$$
\widehat{\text{Sale Price}} = -29334 + 63.08 \times \text{Basement Area} + 97.53 \times \text{Living Area} - 7527.76 \times \text{Total Room} + 26551 \times \text{Garage Size}
$$

\newpage 

## (c) 

How does the adjusted $R^2$ value for the reduced model compare to the full model?

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q2c.png")
```

Full: 0.7820 
Reduced: 0.7531 
Difference: 0.0289
Note: For Adjusted $R^2$

The difference of 0.0289 corresponds to a difference of 2.89% between the two models. So the full model had a somewhat larger adjusted $R^2$ value. 

\newpage 

## (d) 

Using the reduced model, check for:

### i. 

outliers 

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q2d1.png")
```

We do observe there being some potential outliers in the training data, where an outlier is a studentized residual value greater in magnitude to 2, i.e. |r| > 2 where r is a residual. 

### ii. 

high leverage points

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q2d2.png")
```

We do observe there being some leverage points in the training data, where our leverage threshold value is calculated to be $\approx 0.01$. 

### iii. 

potential influence points

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q2d3.png")
```

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q2d4.png")
```

We do observe there being some potential influence points in the training data, particularly when evaluating based on the DFFITS method and a threshold value $\approx 0.45$, as illustrated above. 

\newpage 

## (e) 

Fit the reduced model from part (b) to the evaluation data (housing eval.csv). Compare the mean squared error from fitting the model to the testing data to the mean squared error from fitting the model to the evaluation data. What does this imply?

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q2e1.png")
```

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q2e2.png")
```


$$
\text{MSE}_{\text{train}} = 1.693569893 \times 10^8
$$

$$
\text{MSE}_{\text{eval}} = 1.617144789 \times 10^8 
$$

Importantly, we see that $\text{MSE}_{\text{eval}} < \text{MSE}_{\text{train}}$, which is key to our insight! 

The fact that the evaluation MSE is slightly lower than the training MSE suggests that the model generalizes well to unseen data. This also indicates there is no significant overfitting  in our model, as the model performs similarly on both the training and evaluation datasets (is consistent across the datasets considered).

We may hope to generalize: The model we chose is consistent for predicting housing sale prices in the context of our research/study.

\newpage

# 3

The dataset for this exercise is called diamonds and it is available directly in the ggplot2 package in R. The data set contains prices (response variable â€“ in US dollars) of over 50,000 diamonds, which we will try to explain using the quantitative size measurements: 

  - carat â€“ weight, 
  - x â€“ length in mm, 
  - y â€“ width in mm, 
  - z â€“ depth in mm, 
  - depth â€“ total depth percentage = z / mean(x, y), 
  - table â€“ width of top of diamond relative to widest point) 
  
And categorical quality (cut, color, and clarity) of the diamonds. The R code used to create the figures below is provided in the diamonds Hmwk 11.R file posted in Canvas.

## (a) 

Summarize your findings from examining the pairwise scatterplots (on the next page) and correlation matrix (shown below).

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q3a1.png")
```

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("BIG.png")
```

Many of the variables are highly correlated with each other; this holds for combinations of variables with the response variable (price) as well as between explanatory variables. Using the "significance" threshold of |r| > 0.7, we have the following "signficant" correlations: Carat and price, carat and x, carat and y, carat and z, price and x, price and y, price and z, x and y, x and z, y and z. Of note is that all of the "significant" correlations are positive. Overall, we do corroborate these findings when looking at the pairwise scatter plots, insomuch as positive correlations generally show a positive linear relationship when looking at the respective graph. On the flip side, we see "small" correlations (|r| < 0.20, such as x and depth), exhibit a rather large "blob" of points, or for other pairs a general overall coverage of the plot area. This is understandable, as a small-in-magnitude correlation coefficient is evidence that there is not a significant linear relationship between the two variables.   

\newpage 

## (b) 

Discuss whether the VIFs, shown in the plot below, indicate any explanatory variables exhibiting moderate or extreme multicollinearity.

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q3b.png")
```

Carat, x, y, and z all have VIF > 4, indicating moderate multicollinearity. 

The output does not explicitly state if any of the VIF values are greater than 10, as values are cut off at the 7. So for "extreme multicolinearity" corresponding to a VIF greater than 10, we cannot determine explicitly if the "x" explanatory variable exhibits extreme multicollinearity (which is the only explanatory variable that *may* meet that criteria).

\newpage 

## (c) 

Summarize the backward elimination method of model selection by providing:

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q3c.png")
```

### i. 

an ordered list of which variable was removed from the model at each step;

Step 1: variable "y" was removed. 

There are no more explanatory variables eliminated. 

### ii. 

a list of which variables remained in the final model;

Explanatory variables that remained: Carat, cut, color, clarity, depth, and table. 

### iii. 

a summary of the partial regression coefficients effects tests for the final model.

All the final model partial regression coefficients meet statistical significance to reject null hypothesis at the $\alpha = 0.05$ level except for "z". There is only one partial regression coefficients in the final model that does not meet statistical significance to reject null hypothesis at the $\alpha = 0.05$ level, "z". 

The statistical significance test is to determine whether there is evidence to reject the null hypothesis that the estimated beta coefficient is equal to zero (statistical significance referring to being statistically significant from zero). 

\newpage 

## (d) 

Summarize the forward selection method of model selection by providing:

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q3f1.png")
```

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q3f2.png")
```

### i. 

an ordered list of which variable was added to the model at each step;

First Step: carat, 
Second Step: clarity, 
Third Step: color, 
Fourth Step: x, 
Fifth Step: cut, 
Sixth Step: depth, 
Seventh Step: table, 
Eighth Step: z

### ii. 

a list of which variables never entered the final model;

The explanatory variable "y" never entered the final model. 

### iii. 

a summary of the partial regression coefficients effects tests for the final model.

Consistent with the prior model, 

All the final model partial regression coefficients meet statistical significance to reject null hypothesis at the $\alpha = 0.05$ level except for "z". There is only one partial regression coefficients in the final model that does not meet statistical significance to reject null hypothesis at the $\alpha = 0.05$ level, "z". 

The statistical significance test is to determine whether there is evidence to reject the null hypothesis that the estimated beta coefficient is equal to zero (statistical significance referring to being statistically significant from zero). 

\newpage 

## (e) 

Summarize the all-possible-subsets method of model selection by providing:

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q3e.png")
```

### i. 

Which model would you choose based on the adjusted R2 values?

Model 8, using the explanatory variables of carat, color, and clarity (possibly with an intercept term as well). 

### ii. 

Which model would you choose based on the Mallowâ€™s Cp criteria?

Model 8, using the explanatory variables of carat, color, and clarity (possibly with an intercept term as well). 

### iii. 

Which model would you choose based on the BIC values?

Model 8, using the explanatory variables of carat, color, and clarity (possibly with an intercept term as well). 

\newpage 

## (f) 

Interpret the values of the estimated regression coefficients for the final model selected:

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q3f.png")
```

### i. 

one of the values corresponding to the categorical variable of your choice;

The conditional mean price of diamonds that are cut 2 are priced $580.325 more than diamonds that are cut 1, holding all other variables constant.

### ii. 

one of the values corresponding to the quantitative variable of your choice.

For a 1 carat increase in weight, the conditional mean price of diamonds will increase by $11257.752, holding all other variables constant.

\newpage 

## (g) 

Summarize your findings from examining all the residual plots used to diagnose the MLR model assumptions. Are there any assumptions that arenâ€™t met for this analysis? Briefly justify your response.

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q3g1.png")
```

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q3g2.png")
```

The key model assumptions we evaluate with the given plots and graphs are: Equal variance, linearity (form of the model), and normality. That being said: 

The residual plot has a clearly obvious non-random-scattering trend, suggesting the linearity and equal variance assumptions may be violated. (We observe a trend in the residual plot, particularly indicating a violation of linearity). 

Additionally, the QQ is roughly linear as it follows the reference line closely in the middle of the plot. However, there are deviations in the left tail and right of the middle, suggesting the normality assumption is violated.

Taken together, we have reason to be concerned that our key assumptions are being violated, with regards to the residuals. 

\newpage 

## (h) 

Summarize your findings from examining the case diagnostic values/plots. Are there any
outliers, leverage points, or influential observations?

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q3h1.png")
```

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q3h2.png")
```

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("q3h3.png")
```

Outliers: Looking at the studentized residual plot, there appear to be many outliers (looking for observations where the studentized residuals exceed 2, based on magnitude, i.e. |r| for r residuals, as indicated by the dashed red lines). 

Leverage: From the leverage plot, there appears to be many high leverage points (leverage values exceeding $2(8+1)/50000 \approx 0.00036$, as indicated by the red line).

From the cookâ€™s D plot, there appear to be several high influence points (influence value exceeding $2\sqrt{2/50000} \approx 0.01264911$, as indicated by the red line). 

Overall, we have reason to believe there are outliers, leverage points, and influential points, such that we may recommend considering other models or undergoing a transformation of our data and reevaluating our assumptions again. As-is, we have reason to be suspect. 
