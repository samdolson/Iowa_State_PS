---
title: "STAT 5460: Homework III (Technically II)"
author: "Sam Olson"
output: pdf_document
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
---

# Problem 1

Consider the kernel density estimator with $X_1, \ldots, X_n \overset{\text{i.i.d.}}{\sim} X$:

$$
\hat{f}_{h}(x) = \frac{1}{nh} \sum_{i=1}^n K\!\left(\frac{x - X_i}{h}\right) 
= \frac{1}{n} \sum_{i=1}^n K_h(x - X_i),
$$

and denote

$$
(f * g)(x) = \int f(x - y) g(y) \, dy.
$$

## a) 

Show that the exact bias of the kernel density estimator is given by

$$
\text{E}[\hat{f}_{h}(x)] - f(x) = (K_h * f)(x) - f(x).
$$

### Answer

$$
\begin{aligned}
\text E[\widehat f(x)]
&=\text E\left[\frac{1}{n}\sum_{i=1}^n K_h(x-X_i)\right] \\ 
&= \sum_{i=1}^{n} \frac{1}{n} \text E\left[K_h(x-X_i)\right] \quad\text{Expectation is a linear function}\ \\
&= \text E\left[K_h(x-X)\right] \quad\text{X's iid, specifically identical}\ \\
&= \int_{\mathbb R}K_h(x-y)f(y)dy \quad \text{See Note} \\ 
&= (K_h*f)(x) \quad \text{Convolution definition} \\ 
\end{aligned}
$$

Note: The penultimate step follows from the definition of expectation for a continuous R.V., where if $Y$ has density $f$, then $\text E{g(Y)}=\int g(y)f(y)\,dy$. Then, as noted we use the given convolution formula.  

Returning then to the bias formula, it then follows: 

$$
\text{E}[\hat{f}_{h}(x)] - f(x) = (K_h * f)(x) - f(x)
$$

## b) 

Show that the exact variance of the kernel density estimator equals

$$
\operatorname{Var}(\hat{f}_{h}(x)) 
= \frac{1}{n}\Big[(K_h^2 * f)(x) - (K_h * f)^2(x)\Big].
$$

### Answer

To make our lives easier, well maybe not you since you're grading this, define the R.V. $Z_i = K_h(x-X_i)$ (for notational convenience). 

Then the kernel density estimator is equivalent to $\hat f(x)=\frac{1}{n} \sum_{i=1}^n K_h(x - X_i)= \frac{1}{n}\sum_{i=1}^n Z_i$. 

Notably, as X's are iid, then the Z's are iid, as defined.

Evaluating the (exact) Variance then: 

$$
\begin{aligned}
\operatorname{Var}(\hat f(x))
&= \operatorname{Var}\!\left(\tfrac{1}{n}\sum_{i=1}^n Z_i\right) \\
&= \tfrac{1}{n}\operatorname{Var}(Z_1)
\quad\text{(sum of the variance of iid R.V.'s)} \\ 
&= \tfrac{1}{n}\Big(\text{E}[Z_1^2] - (\text{E}[Z_1])^2\Big) \quad \text{Variance definition/decomposition} \\
&= \tfrac{1}{n}\Big(\text{E}[K_h^2(x-X_1)] - \left( \text{E}[K_h(x-X_1)]\right)^2\Big) \quad \text{Substituting original definitionb of } Z_i \\
&= \tfrac{1}{n}\left(\int_{\mathbb R}K_h^2(x-y)\,f(y)\,dy 
       - \left\{\int_{\mathbb R}K_h(x-y)\,f(y)\,dy\right\}^2\right) \quad \text{Convolution definition} \\
&= \tfrac{1}{n}\Big[(K_h^{\,2}*f)(x) - (K_h*f)^2(x)\Big]
\end{aligned}
$$

Notably, the above also uses the definition of expectation for absolutely continuous R.V.’s like in part a).

## c) 

Calculate the exact mean squared error (MSE) of the kernel density estimator.

### Answer

The formula for the MSE is given by:

$$
\mathrm{MSE}(\hat f(x))
= \operatorname{Var}(\hat f(x)) + \text{Bias}^2 (\hat f(x))
$$

Plugging in the results from a) and b) gives us: 

$$
\mathrm{MSE}(\hat f(x))
= \frac{1}{n}\Big[(K_h^{,2}*f)(x)-(K_h*f)^2(x)\Big]
+ \big[(K_h*f)(x)-f(x)\big]^2
$$

You *could* simplify this somewhat, which would amount to: 

$$
\mathrm{MSE}(\hat f(x))
= \tfrac{1}{n}(K_h^{,2}*f)(x)
+ \Big(1 - \tfrac{1}{n}\Big)(K_h*f)^2(x)
- 2f(x)(K_h*f)(x)
+ f(x)^2
$$

But honestly, that doesn't seem as nice now, does it? 

## d)

Calculate the exact mean integrated squared error (MISE) of the kernel density estimator.

### Answer

$$
\mathrm{MISE}(\hat f)
= \int_{\mathbb R} \mathrm{MSE}(\hat f(x))dx
$$

Using the result from c), i.e., the original, "unsimplified version":

$$
\mathrm{MISE}(\hat f)
= \frac{1}{n}\left[\int_{\mathbb R}(K_h^{\,2}*f)(x)\,dx - \int_{\mathbb R}(K_h*f)^2(x)\,dx\right]
+ \int_{\mathbb R}\big[(K_h*f)(x)-f(x)\big]^2\,dx
$$

Evaluating the first integral of the above:

$$
\begin{aligned}
\int_{\mathbb R}(K_h^{\,2}*f)(x)\,dx
&= \int_{\mathbb R}\int_{\mathbb R}K_h^{\,2}(x-y)\, f(y)\,dy\,dx \\
&= \int_{\mathbb R}f(y)\left\{ \int_{\mathbb R}K_h^{\,2}(x-y)\,dx \right\}dy
\qquad\text{Fubini to swap order of integration} \\
&= \int_{\mathbb R}f(y)\left\{ \int_{\mathbb R}K_h^{\,2}(u)\,du \right\}dy
\qquad \text{u substitution where } u = x-y, du = dx \\
&= \left(\int_{\mathbb R}f(y)\,dy\right)\left(\int_{\mathbb R}K_h^{\,2}(u)\,du\right) \\
&= \int_{\mathbb R}K_h^{\,2}(u)\,du \quad \text{as we integrate f(y) over its support}
\end{aligned}
$$

Because we used Fubini we then are assuming that the function is Lebesgue integrable, which is a given when we assume f is a (valid) density. 

Note then, that the squared kernel density is of the form:

$$
\int_{\mathbb R}(K_h^{\,2}*f)(x)\,dx
= \int_{\mathbb R}K_h^{\,2}(u)\,du 
= \int_{\mathbb R}\frac{1}{h^2}\,K^2\left(\frac{u}{h}\right)\,du
$$

Consider an additional change of variables, where $v = u/h$, and $du = h\,dv$. 

Then:

$$
\int_{\mathbb R}\frac{1}{h^2} K^2\left(\frac{u}{h}\right)\,du
= \int_{\mathbb R}\frac{1}{h^2} \left( K^2(v)\,hdv \right)
= \frac{1}{h}\int_{\mathbb R}K^2(v)\,dv
$$

Notably, up until this point the simplification/evaluation was for the first integral of the original MISE expression. 

I do not believe the other two integrals evaluate/simplify nicely, and thus will be left to a form of simplification more akin to notational convenience.  

We then have the overall (exact) MISE is of the form: 

$$
\mathrm{MISE}(\hat f)
= \frac{1}{nh}\int_{\mathbb R}K^2(u)\,du
- \frac{1}{n}\int_{\mathbb R}(K_h*f)^2(x)\,dx
+ \int_{\mathbb R}\big[(K_h*f)(x)-f(x)\big]^2\,dx
$$

We can simplify this somewhat, following the convention of the text to define $R(K)=\int_{\mathbb R} K(x)^2\,dx$:

$$
\mathrm{MISE}(\hat f)=\frac{1}{nh}\,R(K) - \frac{1}{n}\,R(K_h*f) + R\big((K_h*f)-f\big)
$$



\newpage 

# Problem 2

## a) 

Use Hoeffding’s inequality to bound the probability that the kernel density estimator $\hat{f}_h$ deviates from its expectation at a fixed point $x$, i.e., find an upper bound for

$$
P\!\left( \big|\hat{f}_h(x) - \text{E}[\hat{f}_h(x)]\big| > \epsilon \right)
$$

for some $\epsilon$, and show how the bound depends on $n, h, \epsilon$ and $\lVert K \rVert_\infty = \sup_{u \in \text{R}} |K(u)| < \infty$.

**Hint:** Hoeffding’s inequality states that for i.i.d. random variables $Y_i$ such that $a \leq Y_i \leq b$,

$$
P\!\left( \left|\frac{1}{n} \sum_{i=1}^n Y_i - \text{E}\!\left[\frac{1}{n}\sum_{i=1}^n Y_i\right]\right| > \epsilon \right) 
\leq 2 \exp\!\left(- \frac{2n\epsilon^2}{(b-a)^2}\right).
$$

### Answer

Starting with our typical form of the kernel and kernel density estimator, let:

$$
Y_i = K_h(x - X_i) = \frac{1}{h}\,K\left(\frac{x - X_i}{h}\right) \qquad \text{where } i=1,\dots,n,
$$

Then, we may write the kernel density estimator as: 

$$
\hat f_h(x) = \frac{1}{n}\sum_{i=1}^n Y_i
$$

Since $|K|_\infty = \sup_{u\in\mathbb{R}} |K(u)| < \infty$, we have bounds given by: 

$$
-\frac{|K|_\infty}{h} \le Y_i \le \frac{|K|_\infty}{h}
$$

Thus we may take (noting the hint): 

$$
a = -\frac{|K|_\infty}{h},
\qquad
b = \frac{|K|_\infty}{h},
\qquad
(b-a)^2 = \frac{4|K|_\infty^2}{h^2}.
$$

Applying Hoeffding’s inequality:

$$
P\left( \big|\hat f_h(x) - \text{E}[\hat f_h(x)]\big| > \epsilon \right)
\le
2\exp\left( - \frac{2n\epsilon^2}{(b-a)^2} \right)
$$

Simplifying the right-hand side of the inequality:

$$
2\exp\left( - \frac{2n\epsilon^2}{4|K|_\infty^2/h^2} \right)
= 2\exp\left( - \frac{n h^2 \epsilon^2}{2|K|_\infty^2} \right)
$$

So

$$
P\left( \big|\hat f_h(x) - \text{E}[\hat f_h(x)]\big| > \epsilon \right)
\le
2\exp\left( - \frac{n h^2 \epsilon^2}{2|K|_\infty^2} \right)
$$

## b) 

Suppose you want to construct a uniform bound over a compact interval $[a, b]$. Show that

$$
P\!\left( \sup_{x \in [a, b]} \big|\hat{f}_{h}(x) - \text{E}[\hat{f}_h(x)]\big| > \epsilon \right) 
\leq \text{something small}.
$$

Write down all the assumptions you’re making in the process.

**Hint:** For a given $\delta > 0$, construct a finite set $N_\delta \subset [a, b]$ such that:

* For every $x \in [a, b]$, there exists $x' \in N_\delta$ with $|x - x'| \leq \delta$
* $|N_\delta| \leq \left\lceil \frac{b-a}{\delta} \right\rceil + 1$

### Answer

(1): Throughout, we assume $X_1,\dots,X_n$ are iid with valid density.

(2): The kernel $K$ is bounded ($|K|_{\infty} = \sup_{u\in\mathbb{R}} |K(u)| < \infty$).

(3): The kernel $K$ is differentiable and has bounded derivative ($|K'|_{\infty} = \sup_{u\in\mathbb{R}} |K'(u)| < \infty$).

(4): The kernel density estimator $f$ is bounded (a stronger assumption would be is integrable)

(5): As $h \rightarrow 0$, $n h^{2}\to\infty$

(6): (Perhaps not an assumption, but a given?) We have a compact interval $[a,b]$ (closed and bounded interval)

Given the setup and results from part a), we know that boundedness gives $|Y_i(x)| \le \frac{|K|_{\infty}}{h}$ for all $x$. 

We then also know that $|K'|_{\infty}<\infty$ (that is both exists and is bounded). Then, by the Mean-Value Theorem:

$$
|Y_i(x) - Y_i(x')|
\le \frac{|K'|_{\infty}}{h^2}\,|x - x'|\,
\Rightarrow
|\hat f_h(x) - \hat f_h(x')|
\le \frac{|K'|_{\infty}}{h^2}\,|x - x'|
$$

Taking expectations, 

$$
|\text{E}\hat f_h(x) - \text{E}\hat f_h(x')| \le \frac{|K'|_{\infty}}{h^2}\,|x - x'|
$$

(Noting the terms on the right-side of the inequality are non-random, i.e., fixed)

We then fix some (small) $\delta>0$, and define a $\delta$-net $N_\delta\subset[a,b]$ by: 

$$
|N_\delta|\le \left\lceil\frac{b-a}{\delta}\right\rceil + 1,
\qquad
\forall x\in[a,b]\ \quad \exists x'\in N_\delta \text{ such that } |x-x'|\le \delta
$$

Then for such $x$ and $x'$,

$$
\begin{aligned}
\big|\hat f_h(x) - \text{E}\hat f_h(x)\big|
&\le \big|\hat f_h(x) - \hat f_h(x')\big|
+ \big|\hat f_h(x') - \text{E}\hat f_h(x')\big|
+ \big|\text{E}\hat f_h(x') - \text{E}\hat f_h(x)\big| \
\le \frac{2|K'|_{\infty}}{h^2}\,\delta + \big|\hat f_h(x') - \text{E}\hat f_h(x')\big|
\end{aligned}
$$

(The additional terms come from "adding zeros" via $\pm \hat{f}_{h}(x') \pm \text{E}\hat{f}_{h}(x')$, followed by the Triangle Inequality)

Choose

$$
\delta = \frac{\epsilon h^{2}}{4|K'|_{\infty}}
\quad\Rightarrow\quad
\frac{2|K'|_{\infty}}{h^{2}}\,\delta = \frac{\epsilon}{2}
$$

Hence

$$
\Big\{ \sup_{x\in[a,b]} \big|\hat f_h(x)-\mathbb E\hat f_h(x)\big|>\epsilon \Big\}
\subseteq
\Big\{ \max_{x'\in N_\delta} \big|\hat f_h(x')-\mathbb E\hat f_h(x')\big|>\tfrac{\epsilon}{2} \Big\}
$$

Therefore, by the union bound,

$$
\text{P}\!\left( \sup_{x\in[a,b]} \big|\hat f_h(x)-\mathbb E\hat f_h(x)\big|>\epsilon \right)
\le
|N_\delta|\,
\max_{x'\in N_\delta} \text{P}\!\left(\big|\hat f_h(x')-\mathbb E\hat f_h(x')\big|>\tfrac{\epsilon}{2}\right).
$$

Applying results (the bound) from part a), for each fixed $x'$ we have

$$
\text{P}\left( \big|\hat f_h(x') - \text{E}\hat f_h(x')\big| > \tfrac{\epsilon}{2} \right)
\le
2\exp\left( - \frac{n h^{2} \epsilon^{2}}{8\,|K|_{\infty}^{2}} \right)
$$

Then, we have: 

$$
\text{P}\left( \sup_{x\in[a,b]} \big|\hat f_h(x) - \text{E}\hat f_h(x)\big| > \epsilon \right)
\le
\left( \left\lceil \frac{4(b-a)\,|K'|_{\infty}}{\epsilon h^{2}} \right\rceil + 1 \right)
\cdot 2\exp\left( - \frac{n h^{2} \epsilon^{2}}{8\,|K|_{\infty}^{2}} \right)
$$

We then need to determine whether this term is "something small". 

To that end note that from the bound

$$
\text{P}\!\left( \sup_{x\in[a,b]} \big|\hat f_h(x) - \mathbb{E}\hat f_h(x)\big| > \epsilon \right)
\le
\left( \left\lceil \frac{4(b-a)\,\|K'\|_{\infty}}{\epsilon h^{2}} \right\rceil + 1 \right)
\cdot 2\exp\!\left( - \frac{n h^{2} \epsilon^{2}}{8\,\|K\|_{\infty}^{2}} \right)
$$

Then, for any fixed $\epsilon>0$,

$$
\left\lceil \frac{4(b-a)\,\|K'\|_{\infty}}{\epsilon h^{2}} \right\rceil + 1
\;\le\;
\frac{4(b-a)\,\|K'\|_{\infty}}{\epsilon h^{2}} + 1
\le \frac{C_1}{\epsilon h^{2}}
$$

For some constant $C_1=4(b-a)\|K'\|_\infty+1$

Hence, for $c_1=\frac{1}{8\|K\|_\infty^2}$,

$$
\text{P}\!\left( \sup_{x\in[a,b]} \big|\hat f_h(x) - \mathbb{E}\hat f_h(x)\big| > \epsilon \right)
\;\le\;
\frac{2C_1}{\epsilon h^{2}}\,\exp\!\big(-c_1\,n h^{2}\epsilon^{2}\big)
$$

Since $h \equiv h_n$ satisfies $n h^{2}\to\infty$ 

$$
\frac{2C_1}{\epsilon h^{2}}\,\exp\!\big(-c_1\,n h^{2}\epsilon^{2}\big) \underset{n h^2 \to \infty} \to 0
$$

Such that: 

$$
\text{P}\!\left( \sup_{x\in[a,b]} \big|\hat f_h(x) - \mathbb{E}\hat f_h(x)\big| > \epsilon \right)
\underset{n h^{2}\to\infty} \to 0 
$$

And we have our desired outcome: 

$$
\text{P}\left( \sup_{x\in[a,b]} \big|\hat f_h(x) - \text{E}\hat f_h(x)\big| > \epsilon \right)
\le \text{something small}
$$

## c) 

From Question b), construct a nonparametric uniform $1 - \alpha$ confidence band for $\text{E}[\hat{f}_h(x)]$, i.e., find $L(x)$ and $U(x)$ such that

$$
P\!\big(L(x) \leq \text{E}[\hat{f}_h(x)] \leq U(x), \ \forall x\big) \geq 1 - \alpha.
$$

### Answer

For notational convenience, let $\Lambda=\|K'\|_\infty/h^2$. 

Then, from part b), for any $\delta>0$ and any $\delta$-net $N_\delta\subset[a,b]$,

$$
\Big\{\sup_{x\in[a,b]} \big|\hat f_h(x)-\text{E}\hat f_h(x)\big| > \varepsilon \Big\}
\subseteq
\Big\{ \max_{x'\in N_\delta} \big|\hat f_h(x')-\text{E}\hat f_h(x')\big|
> \varepsilon - 2\Lambda\,\delta \Big\}
$$

Applying Hoeffding's Inequality at each $x'\in N_\delta$ and the union bound, for any $t>0$,

$$
\text{P}\Big(\sup_{x\in[a,b]} \big|\hat f_h(x)-\text{E}\hat f_h(x)\big| > t+2\Lambda\delta\Big)
\le
2\,|N_\delta|\exp\!\Big(-\tfrac{n h^2 t^2}{8\,\|K\|_{\infty}^2}\Big)
$$

Let

$$
m_\delta=\Big\lceil\tfrac{b-a}{\delta}\Big\rceil+1,
\quad \text{and }
t_\alpha(\delta)=\sqrt{\frac{8\,\|K\|_{\infty}^2}{n\,h^2}\,
\log\!\Big(\frac{2\,m_\delta}{\alpha}\Big)}
$$

Then

$$
\text{P}\Big(\sup_{x\in[a,b]} \big|\hat f_h(x)-\text{E}\hat f_h(x)\big|
\le t_\alpha(\delta)+2\Lambda\delta\Big)\ge 1-\alpha
$$

Therefore, we may construct a nonparametric uniform $1 - \alpha$ confidence band for $\text{E}[\hat{f}_h(x)]$ a $(1-\alpha)$ (on a compact interval $[a,b]$) via $(L(x), U(x))$, where: 

$$
L(x)=\hat f_h(x)-\big(t_\alpha(\delta)+2\Lambda\delta\big)
$$

$$
U(x)=\hat f_h(x)+\big(t_\alpha(\delta)+2\Lambda\delta\big)
$$

(And again, using $\Lambda=\|K'\|_\infty/h^2$ and $t_\alpha(\delta)$ as defined previously.)
