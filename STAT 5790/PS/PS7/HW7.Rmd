---
title: "HW7"
output: pdf_document
date: "2024-10-27"
---

# 1. 

Consider the dataset txhousing which is in the ggplot2 R package. This dataset contains information about 46 Texas cities, recording the number of house sales (sales), the total volume of sales (volume), the average and median sale prices, the number of houses listed for sale (listing) and the number of months inventory (inventory). Data is recorded monthly from Jan 2000 to Apr 2015, 187 entries for each city.

## (a) 

Plot a time series of sales against date for each city on the same plot using ggplot2 package. Describe problems that make it hard to see the long-term trend in this plot. 

```{r}
library(ggplot2)
library(dplyr)
attach(txhousing)

# txhousing <- na.omit(txhousing, cols = c("sales", "city"))

ggplot(txhousing, aes(x = date, y = sales, color = city)) +
  geom_line() +
  labs(title = "Monthly House Sales of Texas Cities",
       x = "Date",
       y = "Sales",
       color = "City") +
  theme_minimal() +
  theme(legend.position = "none")

length(unique(txhousing$city))

lowCities <- txhousing |>
  group_by(city) |> 
  summarize(avgSales = mean(sales, na.rm = TRUE)) |>
  filter(avgSales < 2500) |> 
  nrow()

lowCities
```

There are a lot of cities in this dataset (46 to be exact)! The fact that so many cluster around 0 to 2500 in "Sales" (44 of 46 average below 2500) means there's a lot of difficulty distinguishing between trends for most of the cities in the above display. 

## (b) 

We can fix one problem described in (a) by plotting sales in the log-scale. Plot a time series of sales in the log-scale against date for each city on the same plot using ggplot2. 

```{r}
ggplot(txhousing, aes(x = date, y = log(sales), color = city)) +
  geom_line() +
  labs(title = "Monthly House Sales of Texas Cities",
       x = "Date",
       y = "log(Sales)",
       color = "City") +
  theme_minimal() +
  theme(legend.position = "none") + 
  annotate("text", x = 2005, y = 2.25, label = "Oh yeah, that helped a bunch...", size = 4, hjust = 0.5)
```

## (c) 

We can fix another problem described in (a) by removing the strong seasonal trend present in the dataset. Fit a linear model with log(sales) against month for each city, then create a new column in txhousing, call it "rel_sales", which contains the residuals of the linear fits. Then plot a time series of rel sales against date for each city (on the same plot) using ggplot2

```{r}
residualsList <- tapply(X = seq_along(txhousing$sales), 
                        INDEX = txhousing$city, 
                        FUN = function(idx) {
                          lm_fit <- lm(log(txhousing$sales[idx]) ~ txhousing$month[idx], na.action = na.exclude)
                          residuals(lm_fit)
                          }
                        )

txhousing$rel_sales <- unlist(residualsList)

ggplot(txhousing, aes(x = date, y = rel_sales, color = city)) +
  geom_line() +
  labs(title = "Residual Sales of Texas Cities",
       x = "Date",
       y = "Residuals of log(Sales)",
       color = "City") +
  theme_minimal() +
  theme(legend.position = "none") + 
  annotate("text", x = 2005, y = -1.5, label = "The lines Mason, what do they mean!?", size = 4, hjust = 0.5)
```

## (d) 

On the plot for (c), put in the mean curve using the function geom line() on the R code in (c).

```{r}
# Fighting the urge to use DB language and group_by...
# mean_rel_sales <- txhousing |>
#   group_by(date) |>
#   summarise(mean_rel_sales = mean(rel_sales, na.rm = TRUE))

# I'll give in to the underscore instead of Camel case...
# since it must be called rel_sales
mean_rel_sales <- data.frame(
  date = unique(txhousing$date),
  mean_rel_sales = tapply(txhousing$rel_sales, txhousing$date, mean, na.rm = TRUE)
)

# Plot the time series of 'rel_sales' for each city and add the mean curve
ggplot(txhousing, aes(x = date, y = rel_sales, color = city)) +
  geom_line() +
  geom_line(data = mean_rel_sales, aes(x = date, y = mean_rel_sales), color = "black", size = 1.2, linetype = "solid") +
  labs(title = "Residual Sales of Texas Cities",
       x = "Date",
       y = "Residuals of log(Sales)",
       color = "City") +
  theme_minimal() +
  theme(legend.position = "none") +
  annotate("text", x = 2005, y = -1.5, label = "Woah", size = 4, hjust = 0.5)
```

## (e) 

Provide a page of plots of rel sales against month by year using the facet option on ggplot2 package. On each plot, put in the mean curve using geom line() options. 

```{r}
# could also dplyr it, but I don't want to annoy the grading gods
# Oh Gautham, I lament: Have mercy! 
# mean_curve <- txhousing |>
#   group_by(year, month) |>
#   summarize(mean_rel_sales = mean(rel_sales, na.rm = TRUE), .groups = "drop")

mean_rel_sales <- data.frame(
  year = rep(unique(txhousing$year), each = 12),
  month = 1:12,
  mean_rel_sales = as.numeric(tapply(X = txhousing$rel_sales, 
                                     INDEX = list(txhousing$year, txhousing$month), 
                                     FUN = mean, 
                                     na.rm = TRUE))
)

mean_rel_sales <- na.omit(mean_rel_sales)

ggplot(txhousing, aes(x = month, y = rel_sales, color = city)) +
  geom_line() +
  geom_line(data = mean_rel_sales, aes(x = month, y = mean_rel_sales), 
            color = "black", size = 1.2, linetype = "solid") +
  labs(title = "Residual Sales of Texas Cities by Month, Year",
       x = "Month",
       y = "Residuals of log(Sales)",
       color = "City") +
  facet_wrap(~ year, scales = "free_y") +
  theme_minimal() +
  theme(legend.position = "none")
```



\newpage

# 2. 

When a cell phone relay tower is not working properly, wireless providers can lose great deal of money so it is important for them to be aware of and to be able to fix problems expeditiously. A first step toward understanding the problems involved is to collect data from designed experiments involving three factors, as was done in the experiment that gave rise to the dataset available in the file breakdown.dat. In this case, a reported problem was initially classified as low or high severity, simple or complex, and the engineer assigned was rated as relatively new (novice) or expert(guru). Two times were observed. The dataset has measurements on severity level of the problem (low/high, column 1), level of complexity of the problem (simple/complex, column 2), experience level of engineer (novice/guru, column 3), and time to assess problem (column 4), implement solution (column 5) and total resolution (column 6).

## (a) 

Read the data set and name each column, the variables corresponding names, problem, severity and engineer, i.e., problem for low and high, severity for simple and complex and engineer for new or expert. The names for the last three columns are given by ”Assessment”,”Implementation”,”Resolution”. Note that the resolution is the sum of the time assessment of the problem and the implementation. 

```{r}
breakdown <- read.table("breakdown.dat")
names(breakdown) <- c("Problem", "Severity", "Engineer", "Assessment","Implementation","Resolution")
```

## (b) 

For the engineer type as a categorical variable on the x axis, create a facet with boxplots for the variables. You may need to use the reshape2 package. Also rename the y-axis label. What difference can you tell for the engineer for each time?

```{r}
library(reshape2)
library(ggplot2)

breakdown_long <- melt(data = breakdown, 
                       id.vars = c("Problem", "Severity", "Engineer"),
                       measure.vars = c("Assessment", "Implementation", "Resolution"),
                       variable.name = "Thing", 
                       value.name = "Time")

ggplot(breakdown_long, aes(x = Engineer, y = Time)) +
  geom_boxplot() +
  facet_wrap(~ Thing, scales = "free_y") +
  labs(y = "Time", x = "Engineer Type", title = "Time Across Problems") +
  theme_minimal()
```

## (c) 

The above boxplot may be hard to interpret for some. For the above, separate the levels severity and create side-by-side boxplots for each level of engineer. What conclusions can you draw from each of these plots? 

```{r}
ggplot(breakdown_long, aes(x = Engineer, y = Time, fill = Severity)) +
  geom_boxplot(position = position_dodge(width = 0.8)) +
  facet_wrap(~ Thing, scales = "free_y") +
  labs(y = "Time", x = "Engineer Type", title = "Time Across Problems By Severity") +
  scale_fill_brewer(palette = "Set1", name = "Severity") +
  theme_minimal()
```

## (d) 

Now create a new faceting plot, by faceting with “Problem” and the three variables. Discuss the plot and draw appropriate conclusions, if any. 

```{r}
ggplot(breakdown_long, aes(x = Engineer, y = Time, fill = Severity)) +
  geom_boxplot(position = position_dodge(width = 0.8)) +
  facet_grid(Problem ~ Thing, scales = "free_y") +
  scale_y_continuous(
    name = "Time",
    sec.axis = sec_axis(~ ., name = "Problem",)  
  ) + 
  labs(x = "Engineer Type", title = "Time Across Problems By Severity and Problem") +
  scale_fill_brewer(palette = "Set1", name = "Severity") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

In general, I find the above difficult to see (so tiny!) That point notwithstanding there tend to be three comparisons I find myself making: Guru vs. Novice comparisons, "High" vs. "Low" Severity comparisons, and "Complex" vs. "Simple" comparisons, all across the (secret fourth comparison) of Assessment, Implementation, and Resolution. As Resolution is a composite of Assessment and Implementation, results tended to be most consistent with observations for Assessment or Implementation individually. 

Guru v. Novice: The Guru tends to have lower variability (greater precision) in time taken across all stages, and tends to take less time than the Novice regardless of task or complexity. By contrast the "Novice" shows more variability (less precision) particularly during the Implementation and Resolution. Though a bit difficult to see, Novice has significant outliers and longer times for resolving "Complex" problems, particularly in Implementation. 

Complex v. Simple: For "Complex" problems, the Resolution stage takes longer for the Novice compared to the Guru (largest disparity, visually, from the two plots), which seems in large part due to differences in Implementation (this point isn't especially poignant since Resolution is a composite of the two components). 

High v. Low: Assessment times do not vary much between "High" and "Low" severity (top vs. bottom graphs) though there are slight differences between Guru and Novice for Assessments. 

Let's get down to Business: I need Gurus, more Gurus. But short of that, maybe there is something to be said about Novices difficulty in Implementation (since Resolution is a composite of Implementation and Assessment). 