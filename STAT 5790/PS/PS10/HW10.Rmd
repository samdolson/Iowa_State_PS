---
title: "HW10"
output:
  pdf_document: default
  html_document:
    df_print: paged
date: "2024-11-24"
---
  
# 1. 

Magnetic Resonance Imaging (MRI) is a noninvasive radiologic technique used to image tissue structure and physiology. The imaging modality works on the principle of characterizing tissues based on the basis of their physical and chemical properties which can be adequately summarized in terms of three physical quantities: the longitudinal or spin-lattice relaxation time ($T_1$), spin-spin relaxation time ($T_2$), and the proton density ($\rho$). These physical quantities are themselves not directly observable but together determine image intensity with respect to photon density. Determination of image contrast is achieved by changing user-defined pulse sequence parameters which modulate the influence of the $T_1$, $T_2$, and $\rho$ values at a voxel, and hence the photon intensity. 

In spin-echo imaging, there are two design parameters – these are the repetition time (TR) and the echo time (TE). The relationship between $T_1$, $T_2$, $\rho$, and the true MR signal, in the spin-echo sequence of MR imaging, denoted by $\nu$, can be expressed in terms of a solution to the Bloch equation, an empirical expression describing nuclear magnetic resonance phenomena. Thus, the true MR signal ($\nu_{i,j,k}$) at the image pixel (or matrix element) $(i,j)$ for the $j$th set of design parameters (TE$_k$, TR$_k$) is given by:

$$
\nu_{i,j,k} = F(\rho_{ij}, T_{1ij}, T_{2ij}; \text{TE}_k, \text{TR}_k) = \rho_{ij} \exp\left(-\frac{\text{TE}_k}{T_{2ij}}\right) \left\{1 - \exp\left(-\frac{\text{TR}_k}{T_{1ij}}\right)\right\}.
$$

Different pairs of (TE, TR) values can be used to highlight contrasts between different tissue types. In a clinical context, this means that tuning the design parameters at different settings can be used to highlight various tissue types, providing a method for tissue classification in individuals. Therefore, in principle at least, a radiologist can acquire images for a range of values and then select from these images to optimize the contrasts between different tissue types. However, the optimal control parameters are patient- and/or application-specific and generally not known in advance. A systematic exploration of plausible (TE, TR) pairs is impractical because of constraints in time and expense and also because of patient motion and image registration issues. So, therefore a few (typically no more than three) are acquired. Once obtained, the radiologist could estimate the ($\rho$, $T_1$, $T_2$) at the $(i,j)$ voxel and use these estimates to synthetically predict images (Digital Object Identifier [10.1109/TMI.2009.2039487](https://doi.org/10.1109/TMI.2009.2039487)) at other (unobserved) (TE, TR) values.

The file 2dMRI-SE-phantom.dat contains intensity values of MRI images of a physical phantom (man-made object) acquired at 18 settings (in seconds) of (TE, TR) = $\{0.03, 0.06, 0.04, 0.08, 0.05, 0.10\} \times \{1, 2, 3\}$. For your convenience, these settings are also provided in the file te-tr.dat. The file 2dMRI-SE-phantom.dat should be read in as a vector and further the sequence of images is as follows: the first 256\(^2\) values are for the MR image acquired at the first (TE, TR) setting (i.e., at (0.03, 1.0)), the next 256\(^2\) values are for the MR image acquired at the second (TE, TR) setting (i.e., at (0.06, 1.0)), and so on.

## (a) 

Read in the dataset and store as a three-dimensional array. [5 points]  

```{r}
nx <- 256
ny <- 256
nz <- 18

data <- scan("2dMRI-SE-phantom.dat")
phantomArr <- array(data, dim = c(nx, ny, nz))
# Check dimensions of phantom 
dim(phantomArr)
```

## (b) 

Display the data as 18 images on one page. Make sure that all the 18 images have the same scale and use a scale of light to dark (in grays) to display the images. (Consider using the "GE" logo that exists on the phantom to make sure that your images are correctly displayed.) [10 points]  

```{r}
teTrSettings <- read.table("te-tr.dat", header = FALSE)
colnames(teTrSettings) <- c("TE", "TR")

par(mfrow = c(3, 6), mar = c(2, 2, 2, 2)) 
# need min and max for dimensions
# "pad" empty spaces if they don't meet min, max
globalMin <- min(phantomArr)
globalMax <- max(phantomArr)

# yucky for loop version
# for (i in 1:nz) {
#   image(
#     matrix(phantomArr[, , i], nrow = nx, byrow = TRUE),
#     col = gray.colors(256, start = 1, end = 0), 
#     zlim = c(globalMin, globalMax), 
#     axes = FALSE,
#     main = paste0("TE=", teTrSettings$TE[i], ", TR=", teTrSettings$TR[i])
#   )
# }

mapply(
  # UDF! 
  FUN = function(slice, te, tr) {
    image(
      t(matrix(slice, nrow = nx, byrow = TRUE)[nx:1, ]), # Flip vertically
      col = gray.colors(256, start = 1, end = 0), # Light to dark grayscale
      zlim = c(globalMin, globalMax), # Consistent scaling
      axes = FALSE,
      main = paste0("TE=", te, ", TR=", tr)
    )
  },
  # MoreArgs
  # argumnts to "vectorize" over
  slice = split(phantomArr, rep(1:nz, each = nx * ny)), # Extract slices as list
  te = teTrSettings$TE,
  tr = teTrSettings$TR
)
```

We see some "GE" logos, so this is a good sign that things are going right. 

## (c) 

Write a function in R that takes in values of ($\rho$, $T_1$, $T_2$) and (TE, TR) and returns the mean observed intensity value for that setting as per Equation 1. [10 points]  

```{r}
# what do you...mean? ;) 
meanIntensity <- function(rho, T1, T2, TE, TR) {
  # I'm trying to add more failsafes to my functions 
  # and this was a problem I encountered
  if (!all(dim(rho) == dim(T1), dim(T1) == dim(T2))) {
    stop("Inputs are not all the same dimension! Check dim of rho, T1, T2!")
  }
  # if not problematic
  # just a quick and dirty mean 
  # signal equation provided, bless 
  signal <- rho * exp(-TE / T2) * (1 - exp(-TR / T1))
  mean(signal)
}
```

## (d) 

Obtain the least-squares (LS) estimates of ($\rho$, $T_1$, $T_2$) given the observed image intensities at the set (TE, TR)s. Our goal therefore is to find ($\rho_{ij}$, $T_{1ij}$, $T_{2ij}$) at the $(i,j)$th image coordinate (matrix entry) that minimizes

$$
\psi(\rho_{ij}, T_{1ij}, T_{2ij}; (\text{TE}_k, \text{TR}_k), Y_{i,j,k}, k = 1, 2, \dots, 18) = \sum_{k=1}^{18} 1_{\left[Y_{i,j,k} > 0\right]} \left[Y_{i,j,k} - f(\rho_{ij}, T_{1ij}, T_{2ij}; \text{TE}_k, \text{TR}_k)\right]^2.
$$

Where $Y_{ij,k}$ is the intensity of the image at the $(i, j)$-th coordinate (matrix entry) acquired at $(\text{TE}_k, \text{TR}_k)$, and the indicator function $1_{[Y_{ij,k} > 0]}$ is simply to avoid inconsistencies in the estimation. Write a function in R that evaluates Equation (2). 

```{r}
objectiveFun <- function(params, Y, TE, TR) {
  rho <- params[1]
  T1 <- params[2]
  T2 <- params[3]
  
  # prediction formula
  predictions <- sapply(X = 1:length(TE), 
                        FUN = function(k) {
    meanIntensity(rho, T1, T2, TE[k], TR[k])
  })
  
  # valid index
  valid_indices <- Y > 0
  residuals <- (Y[valid_indices] - predictions[valid_indices])
  # sum of squared residuals
  sum(residuals^2)
}

estimateParams <- function(Y, TE, TR, init = c(1, 1000, 100)) {
  result <- optim(
    par = init,
    fn = objectiveFun,
    Y = Y,
    TE = TE,
    TR = TR,
    method = "L-BFGS-B",  
    lower = c(0, 0, 0)    
  )
  
  # return output as a list
  list(rho = result$par[1], 
       T1 = result$par[2], 
       T2 = result$par[3], 
       value = result$value)
}
```

So I didn't quite know what all to include in this part versus the next part. So more is included in part (e), the function(s) are actually being called, and I get the data for viz. Names and functions have also been altered slightly. 

## (e) 

We will now estimate ($\rho_{ij}, T_{1ij}, T_{2ij}$) for each imaging coordinate given the 18 values.  

In so doing, note that the value is indeterminate if there are no more than two $Y_{i,j,k}$s that are non-zero for any $(i,j)$. For these cases, we will set $\rho_{ij} = 0$, $T_{1ij} = 4.0$, and $T_{2ij} = 0.001$. For the other $(i,j)$s, use the R function `optim` (note: not `optimize`) to estimate ($\rho_{ij}, T_{1ij}, T_{2ij}$). Restrict $\rho$ to be in $[0,3000]$, $T_1$ to be in $[0.01, 4.0]$, and $T_2$ to be in $[0.001,2.0]$ by writing your function such that if these values are breached in any case, then the value returned is $\infty$. The function `optim` also needs initial values: use (1500, 1.5, 0.1) as starting values. Store the estimated $\rho, T_1, T_2$ values in three 256 $\times$ 256 matrices. [25 points]  

```{r}
objectiveFun <- function(params, yIjk, te, tr) {
  rho <- params[1]
  t1 <- params[2]
  t2 <- params[3]
  
  # handle when things go off the rails
  if (rho < 0 || rho > 3000 || t1 < 0.01 || t1 > 4.0 || t2 < 0.001 || t2 > 0.2) {
    return(Inf)
  }
  
  # based on formula given
  modelValues <- rho * exp(-te / t2) * (1 - exp(-tr / t1))
  residuals <- (yIjk - modelValues)[yIjk > 0]
  # return thing to minimize 
  return(sum(residuals^2))
}
```

```{r}
# old
# for loop version 
# estimateParameters <- function(yIj, te, tr) {
#   nrow <- dim(yIj)[1]
#   ncol <- dim(yIj)[2]
#   
#   rhoMatrix <- matrix(0, nrow, ncol)
#   t1Matrix <- matrix(4.0, nrow, ncol) 
#   t2Matrix <- matrix(0.001, nrow, ncol) 
# 
#   for (i in 1:nrow) {
#     for (j in 1:ncol) {
#       yIjk <- yIj[i, j, ]
# 
#       if (sum(yIjk > 0) <= 2) {
#         rhoMatrix[i, j] <- 0
#         t1Matrix[i, j] <- 4.0
#         t2Matrix[i, j] <- 0.001
#       } else {
#         
#         result <- tryCatch({
#           optim(
#             par = c(1500, 1.5, 0.1), 
#             fn = objectiveFun,
#             yIjk = yIjk,
#             te = te,
#             tr = tr,
#             method = "L-BFGS-B",
#             lower = c(0, 0.01, 0.001),
#             upper = c(3000, 4.0, 0.2)
#           )
#         }, error = function(e) {
#           list(par = c(0, 4.0, 0.001)) 
#         })
#         
#         rhoMatrix[i, j] <- result$par[1]
#         t1Matrix[i, j] <- result$par[2]
#         t2Matrix[i, j] <- result$par[3]
#       }
#     }
#   }
#   
#   return(list(rho = rhoMatrix, 
#               t1 = t1Matrix, 
#               t2 = t2Matrix))
# }
```

```{r}
estimateParameters <- function(yIj, te, tr) {
  nrow <- dim(yIj)[1]
  ncol <- dim(yIj)[2]
  
  # function within a function
  processPixel <- function(pixelIdx) {
    i <- pixelIdx[1]
    j <- pixelIdx[2]
    yIjk <- yIj[i, j, ]
    
    # default to use
    if (sum(yIjk > 0) <= 2) {
      return(c(0, 4.0, 0.001)) 
    }
    
    # tryCatch used for error handling
    # within a function
    result <- tryCatch({
      optim(
        par = c(1500, 1.5, 0.1), 
        fn = objectiveFun,
        yIjk = yIjk,
        te = te,
        tr = tr,
        method = "L-BFGS-B",
        lower = c(0, 0.01, 0.001),
        upper = c(3000, 4.0, 0.2)
      )
    }, 
    # return valid values when we don't achieve optimization (optimization failure)
    error = function(e) {
      list(par = c(0, 4.0, 0.001)) 
    })

    
    return(result$par)
  }
  
  # create dataframe of all results 
  pixelIndices <- expand.grid(1:nrow, 1:ncol)
  results <- apply(X = pixelIndices, 
                   MARGIN = 1, 
                   FUN = function(idx) {
                     processPixel(idx)
                     })
  
  rhoMatrix <- matrix(results[1, ], nrow = nrow, ncol = ncol)
  t1Matrix <- matrix(results[2, ], nrow = nrow, ncol = ncol)
  t2Matrix <- matrix(results[3, ], nrow = nrow, ncol = ncol)
  
  return(list(rho = rhoMatrix, 
              t1 = t1Matrix, 
              t2 = t2Matrix))
}
```

```{r}
te <- teTrSettings$TE
tr <- teTrSettings$TR

# actual call of function
# results <- estimateParameters(phantomArr, te, tr)
# making bailouts
# not super intensive, but still a time save just to load and not call
# saveRDS(results, file = "q1v2.rds")

# loading bailout
results <- readRDS("q1v2.rds")

rhoMat <- results$rho
t1Mat <- results$t1
t2Mat <- results$t2
```

## (f) 

Display the $\rho, T_1$, and $T_2$ images estimated as per part (e) above. [5 points] 

3 images 

```{r}
image(rhoMat,
      main = "Estimated Rho", 
      # colors courtesy of Craig
      col = gray.colors(32))
```

```{r}
image(t1Mat, 
      main = "Estimated T1", 
      # colors courtesy of Craig
      col = gray.colors(32))
```

```{r}
image(t2Mat, 
      main = "Estimated T2", 
      # colors courtesy of Craig
      col = gray.colors(32))
```

## (g) 

Our final objective here is to plug in the 18 (TE, TR) values to the $\rho, T_1$, and $T_2$ images estimated as per part (e) and to obtain fitted images at the 18 (TE, TR) settings. Use the function you wrote in part (c) along with the estimates obtained in part (e) to obtain the fitted images. Display these images using the same scale as in part 1b above. [15 points]  

18 Images

```{r}
computeFittedImages <- function(rhoMat, t1Mat, t2Mat, te, tr) {
  numSettings <- length(te)
  
  computeForTEandTR <- function(k) {
    rhoMat * exp(-te[k] / t2Mat) * (1 - exp(-tr[k] / t1Mat))
  }
  
  fittedImages <- array(unlist(lapply(X = seq_len(numSettings), FUN = computeForTEandTR)), 
                        dim = c(nrow(rhoMat), ncol(rhoMat), numSettings))
  
  return(fittedImages)
}
```

```{r}
# call function just defined
fittedImages <- computeFittedImages(rhoMat = rhoMat, 
                                    t1Mat = t1Mat, 
                                    t2Mat = t2Mat, 
                                    te = te, 
                                    tr = tr)
```

```{r}
lapply(X = seq_along(te), FUN = function(k) {
  image(fittedImages[,,k], 
        main = paste("Fitted Image at TE =", te[k], "TR =", tr[k]), 
        col = gray.colors(32))
})
```

\newpage 

# 2 

The dataset MPRAGE0.nii.gz contains the anatomic MR image volume of a healthy female in NIFTI format. The R package oro.nifti is required to read this file, with the function `readNIfTI`. The component `.Data` in the stored object (accessed via the `@` operator) contains the three-dimensional array containing the image voxel values. Display the volume via a three-dimensional contour display such as using the `contour3d` function in the misc3d package. Note that slices do not have the same dimensions (in particular, the vertical axis in the volume has slices that are 1.5 mm apart while the slices on the other two axes are 0.94 mm apart). Account for these differences by scaling the axes appropriately by checking out the recordings from the last virtual asynchronously recorded lecture on 3D graphics. Also test out several viewpoints and display the view that portrays the image the best in your opinion. [30 points]

```{r}
library("oro.nifti")
library("misc3d")
library("rgl")

niftiDat <- readNIfTI("MPRAGEco.nii.gz", reorient = FALSE)
imageVolume <- niftiDat@.Data

dims <- dim(imageVolume)
spacing <- c(0.94, 0.94, 1.5) 

x <- seq(0, dims[1] - 1) * spacing[1]
y <- seq(0, dims[2] - 1) * spacing[2]
z <- seq(0, dims[3] - 1) * spacing[3]

contour3d(
  imageVolume, 
  x = x, y = y, z = z,
  level = mean(imageVolume), 
  alpha = 0.2,                
  draw = TRUE
)

# used this for messing around and getting the image I then knit 
# I crashed my computer a few times for this
# view3d(theta = 0, phi = 0, zoom = 0.5)
# rglwidget()
# rgl.snapshot("3d_plot_snapshot.png")
```

```{r, eval = T, echo = T, fig.cap="CocoMelon", out.width = '100%'}
knitr::include_graphics("3d_plot_snapshot.png")
```

My computer is very close. I ended up with a "top down" image as the others just seemed a bit "off", with weird borders around the edge (non brain/head parts), which you still see, though not to a large extent. 

\newpage 

# 3 

The K-means algorithm iteratively partitions a dataset $\{X_1, X_2, \dots, X_n\}$ into groups $G_k$ for $k = 1, 2, \dots, K$ and group mean vectors for the observations in that partition by minimizing the objective function:

$$
SSW = \min \sum_{i=1}^n \sum_{k=1}^K I[X_i \in G_k] \|X_i - \mu_k\|^2. \tag{3}
$$

Both $I[X_i \in G_k]$ and $\mu_k$ are parameters that are estimated by the algorithm which finds locally optimizing solutions in the vicinity of its initialization, therefore it is recommended that the algorithm be run to completion from several starting points. However, this approach is time-consuming so Maitra (2009) ([Digital Object Identifier no. 10.1109/TCBB.2007.70244](https://doi.org/10.1109/TCBB.2007.70244)) suggested running the K-means algorithm for one iteration from multiple ($M$) starting points and then choosing the one that has the lowest values of $SSW$ and then running that to convergence. A further modification would choose the starting points with the $m$ lowest values of $SSW$ and then running K-means to convergence for each of these $m$ values and then choosing the one with the lowest $SSW$ as the optimal partitioning. 

The thinking behind this approach is that algorithms with poorer starting values can not hope to recover much and need not be labored over, but rather that the time can be more profitably used by searching over a larger set of initializing values. A further unpublished suggestion, made by former Iowa State University student Wei-Chien Chen (Ph.D., Statistics, 2013) can be modified to run the K-means algorithm not to convergence but for a small number ($I$) of iterations and then performing the evaluations suggested by Maitra (2009). We will use the R function `kmeans` to easily implement the smorgasbord of suggestions resulting from Maitra (2009) in an embarrassingly parallel setting. [30 points]

## (a) 

Write a function in which R’s `kmeans` function in parallel for $M$ randomly initialized starting points and runs each initialized K-means algorithm for $I$ iterations, chooses the $m$ best solutions and then runs each to convergence, finally choosing the solution with the lowest $SSW$. Note that the `kmeans` function in R has all the arguments (`iter.max`, `nstart`) that you need and in particular that the function returns the objective function (tot.withinss) as well as the centers (`centers`) and grouping (`cluster` at iteration). [30 points]

```{r}
library(parallel)

optimizedKMeans <- function(data, k, m, i, numBestSolutions) {
  # data, byeah 
  # k clusters
  # m random starts
  # i iterations
  # numBestSolutions, yeah

  # set up parallel processing 
  cl <- makeCluster(detectCores() - 1)  
  clusterExport(
    cl, 
    varlist = c("data", "k", "i"),  
    envir = environment()
  )
  
  # k means call, using parallel processing
  initialResults <- parLapply(cl, 1:m, function(x) {
    kmeans(
      x = data,         
      centers = k,      
      iter.max = i,     
      nstart = 1        
    )
  })
  
  # SAVE YOUR RESOURCES MY GOD WHAT HAVE I DONE 
  stopCluster(cl)

  # method SSW for determining "best" 
  initialSsw <- sapply(initialResults, function(res) res$tot.withinss)  

  # identify "best" result 
  bestIndices <- order(initialSsw)[1:numBestSolutions]  
  bestInitializations <- initialResults[bestIndices]  
  
  # call original kmeans method
  finalResults <- lapply(bestInitializations, function(initRes) {
    kmeans(
      x = data,                   
      centers = initRes$centers,  
      iter.max = 100,             
      nstart = 1                  
    )
  })

  # get "final" best
  finalSsw <- sapply(finalResults, function(res) res$tot.withinss)  
  # extract needed elements from "best" 
  bestSolutionIndex <- which.min(finalSsw)  
  bestSolution <- finalResults[[bestSolutionIndex]]  

  bestSolution
}
```

## (b) 

K-means color quantization is used in computer graphics to reduce the number of colors in an image without appreciably losing its visual quality. The importance of this process comes from a need to display images on devices that are not completely capable of dealing with multicolor images. It is also used for some image storing standards such as the Graphics Interchange Format (GIF).

Each pixel in an image is represented in terms of its primary components namely Red, Green, and Blue. Therefore, each pixel has a certain amount of Red, a certain amount of Green, and a certain amount of Blue. This way of representing color is known as RGB format. So, every picture can be presented as a three-dimensional dataset with the number of observations depending on its size: thus an image of size $P \times Q$ pixels is transformed into a dataset with $M = N$ observations. K-means color quantization ([Digital Object Identifier: 10.1080/01621459.2011.646935](https://doi.org/10.1080/01621459.2011.646935)) applies the K-means algorithm, suitably initialized, to such a dataset to yield a palette of $K$ colors for representing the image. We note that K-means represents one of several approaches to color quantization. We use the function written in part 3a to represent an image using 16 colors.

### i. 

Reading in a TIFF image.  
The file provided jubabrinda.tif in the usual places provides a digital file in the Tagged Image File Format (TIFF) of bamboo-handiwork decorations at a street-side exhibit during Kolkata’s (formerly, Calcutta) famed (Sharadiyutsab) fall festival. The currently archived R package rtiff can read in this file as follows:

`library(rtiff)`
`juba <- readTIFF(fn="jubabrinda.tif")`
`plot(juba)`

The amount of the primary colors at each pixel can be obtained from the read TIFF file by using: `juba@red`, `juba@green`, `juba@blue`. Note that these are all matrices of the same dimension as the image. (Make sure that you look at the help on `pixmap-class` to get an idea of what the components of `pixmap` are). Note also that these values are all between 0 and 1.

The alternative R package tiff may also be used. To read in the file, we use:

`library(tiff)`
`xx <- readTIFF(source="jubabrinda.tif")`
`img <- as.raster(xx)`
`plot(img)`

The object `xx` is a 3-dimensional array with RGB values in the last dimension. (If it is a 4-dimensional array, it has alpha, that is, transparency values, in the last dimension.)  

Read in the TIFF image and convert to a dataset of 375000 3-dimensional observations. [10 points]

```{r}
library("tiff")
xx <- readTIFF(source="jubabrinda.tif")
img <- as.raster(xx)
plot(img)
```

```{r}
imgArr <- readTIFF(source = "jubabrinda.tif")
dataMat <- matrix(imgArr, ncol = 3, byrow = FALSE)

dim(imgArr)
# head(imgArr)
dim(dataMat)
# head(dataMat) 
```

### ii. 

Use K-means with $K = 16$ and your function in part 3a with $M = 10000$, $I = 3$, and $m = 10$.  

(If your resources permit, you may try larger values of $M$.) Note that this exercise may take up to an hour. Pack the cluster output from your K-means function into a matrix of the size of the image and display using the image function and a categorical palette (e.g., Set2 of RColorBrewer). [20 points]

```{r, cache = T}
# optimizedKMeans <- function(data, k, m, i, numBestSolutions) {
# data might be image_matrix? 
# I MADE A BAILOUT 
# I LOVE MY COMPUTER 
# result <- optimizedKMeans(
#   data = dataMat,  
#   k = 16,          
#   m = 10000,       
#   i = 3,           
#   numBestSolutions = 10  
# )
# 
# saveRDS(result, file = "optimKMeansDat.rds")
```

The above was run and used for creating a "bailout" object. There were warnings when calling the function, but for brevity the bailout was used so this all knit together in under an hour. 

```{r}
result <- readRDS("optimKMeansDat.rds")
```

```{r}
imgX <- dim(imgArr)[1]
imgY <- dim(imgArr)[2]
clusterImg <- matrix(result$cluster, 
                     nrow = imgX, 
                     ncol = imgY)
```

```{r}
library(RColorBrewer)

tImg <- t(clusterImg)
flipImg <- tImg[, ncol(tImg):1]

image(
  flipImg,                       
  col = brewer.pal(16, "Set2"), 
  main = "K-means Clustering with K=16",
  useRaster = TRUE,                   
  xlab = "", ylab = ""
)
```

### iii. 

Replace the cluster indicator for each pixel with the means obtained from your function.  

Thus we get a matrix for the red (consisting of 16 distinct values), another matrix for the green (consisting of 16 distinct values), and a third matrix for the blues (again having 16 distinct values). Use `pixmap` to combine these matrices and the function `writeTIFF` to write to a TIFF file your quantized image. Display using R as in part 3(b)i. Comment on how well your quantized image reproduced the original. [30 points]

```{r, cache = T}
library("pixmap")
library("tiff")

clusterCenters <- result$centers
quantDat <- clusterCenters[result$cluster, ] 

imgX <- dim(imgArr)[1]
imgY <- dim(imgArr)[2]

quantImgArr <- array(quantDat, 
                     dim = c(imgX, imgY, 3)
                     )

quantImg <- pixmapRGB(data = quantImgArr)
# second bailout
# saveRDS(quantImg, file = "quantImg.rds")
```

```{r}
quantImg <- readRDS("quantImg.rds")
```

```{r}
# Display original image
plot(as.raster(imgArr))  
title(main = "Original (JUBA) Image")  
```

```{r}
# K Means
tImg <- t(clusterImg)
flipImg <- tImg[, ncol(tImg):1]

image(
  flipImg,
  col = brewer.pal(16, "Set2"),
  main = "K-means Clustering with K=16",
  useRaster = TRUE,
  xlab = "", ylab = ""
)
```

```{r}
library(pixmap)
# Display quantized image
# quantized_raster <- as.raster(quantized_image)
plot(quantImg, main = "K-means Image")
```
