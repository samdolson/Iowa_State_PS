---
title: "HW8"
output: pdf_document
date: "2024-11-06"
---

# Q1 

Write a function which takes 2 arguments n and k which are positive integers. It should return the nXn matrix:

$$
\begin{pmatrix}
k & 1 & 0 & 0 & \dots & 0 & 0 \\
1 & k & 1 & 0 & \dots & 0 & 0 \\
0 & 1 & k & 1 & \dots & 0 & 0 \\
0 & 0 & 1 & k & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \dots & k & 1 \\
0 & 0 & 0 & 0 & \dots & 1 & k
\end{pmatrix}
$$

Call the function defined above for n = 6 and k = 5, and provide the matrix you obtain.

One with a for loop, one without. 

```{r}
# nkMatFor <- function(n, k) {
#   mat <- matrix(0, n, n)
#   
#   diag(mat) <- k
#   
#   for (i in 1:(n-1)) {
#     mat[i, i+1] <- 1
#     mat[i+1, i] <- 1
#   }
#   
#   return(mat)
# }
# 
# n6k5 <- nkMatFor(n = 6,
#               k = 5)
# n6k5
```

```{r}
nkMat <- function(n, k) {
  mat <- matrix(0, n, n)
  
  diag(mat) <- k
  
  mat[row(mat) == col(mat) + 1] <- 1
  mat[row(mat) == col(mat) - 1] <- 1
  
  return(mat)
}

n6k5 <- nkMat(n = 6,
              k = 5)
n6k5
```

\newpage

# Q2 

Consider the continuous function

$$
f(x) = 
\begin{cases} 
x^2 + 2x + 3 & \text{if } x < 0 \\
x + 3 & \text{if } 0 \leq x < 2 \\
x^2 + 4x - 7 & \text{if } 2 \leq x 
\end{cases}
$$

Write a function tmpFn which takes a single argument xVec. The function should return the vector of values of the function f(x) evaluated at the values xVec. Plot the function f(x) for -3 < x < 3.

```{r}
tmpFn <- function(xVec) {
  result <- numeric(length(xVec))
  
  result[xVec < 0] <- xVec[xVec < 0]^2 + 2 * xVec[xVec < 0] + 3
  result[xVec >= 0 & xVec < 2] <- xVec[xVec >= 0 & xVec < 2] + 3
  result[xVec >= 2] <- xVec[xVec >= 2]^2 + 4 * xVec[xVec >= 2] - 7
  
  result
}

```

```{r}
xValues <- seq(from = -3, 
               to = 3, 
               length.out = 1000)

yValues <- tmpFn(xVec = xValues)

length(xValues) == length(yValues)

plot(x = xValues, y = yValues, 
     type = "l",
     col = "black", 
     lwd = 1.5,
     xlab = "x", 
     ylab = "f(x)", 
     main = "f(x) for -3 < x < 3")
```

\newpage

# Q3 

Greatest common divisor of two integers The greatest common divisor (gcd) of two integers m and n can be calculated using Euclidâ€™s Algorithm: Divide m by n. If the remainder is zero, the gcd is n. If not, divide n by the remainder. If the remainder is zero, then the previous remainder is the gcd. If not, continue dividing the remainder into previous remainder until a remainder of zero is obtained. The gcd is the value of the last nonzero remainder. Write a function gcd(m,n) using a while loop to find the gcd of two integers m and n. 

```{r}
gcd <- function(m, n) {
  m <- abs(m)
  n <- abs(n)
  
  while (n != 0) {
    remainder <- m %% n
    m <- n
    n <- remainder
  }
  
  m
}
```

```{r}
# Testing examples 
gcd(m = 4*7, 
    n = 4*4*4*7)
gcd(m = 47, 
    n = 93)
```

\newpage

# Q4

eQTL mapping. (The following problem was suggested by Professor Dan Nettleton.) Write a function order.matrix which takes in a matrix x and returns a matrix containing the row and column indices of the sorted values of x. Test this function on a 4X3 matrix of independent $\chi_{1}^2$ pseudo-random deviates. 

```{r}
order.matrix <- function(mat) {
  sorted.indices <- order(mat)
  result <- arrayInd(sorted.indices, dim(mat))
  colnames(result) <- c("row", "col")
  result
}
```

Had a previous version of this before discovering arrayInd: 

```{r}
# order.matrix <- function(mat) {
#   sorted.indices <- order(mat)
#   
#   row.indices <- (sorted.indices - 1) %% nrow(mat) + 1
#   col.indices <- (sorted.indices - 1) %/% nrow(mat) + 1
#   
#   result <- cbind(row = row.indices, 
#                   col = col.indices)
#   result
# }
```

```{r}
set.seed(42)  
testMat <- matrix(data = rchisq(4 * 3, df = 1), 
                      nrow = 4, 
                      ncol = 3)
dim(testMat)
testMat

testResults <- order.matrix(mat = testMat)
testResults
dim(testResults)
```

\newpage

# Q5 

Polar representation of a number. Let $x \in \mathbb{R}^p$. The polar respresentation of $\textbf{x} = (x_1, x_2, ..., x_p)$ is given by ($R, \theta_1, \theta_2, ..., \theta_{p-1}$), where: 

$$
\begin{aligned}
x_1 &= R \cos \theta_1 \\
x_2 &= R \sin \theta_1 \cos \theta_2 \\
x_3 &= R \sin \theta_1 \sin \theta_2 \cos \theta_3 \\
&\;\;\dots \quad = \quad \dots \\
x_{p-1} &= R \prod_{i=1}^{p-2} \sin \theta_i \cos \theta_{p-1} \\
x_p &= R \prod_{i=1}^{p-1} \sin \theta_i,
\end{aligned}
$$

where $0 \leq R < \infty, 0 \leq \theta_1 < 2\pi \text{ and } 0 \leq \theta_i < \pi$ i = 2, 3, . . . , p - 1.

## (a)

Write a function polaroid which takes in an arbitrary p-dimensional vector $\mathbf{x}$ and provides its polar representation as a vector, with the first element as $R$ and the remainder being $\theta_1, \theta_2, \dots, \theta_{p-1}$.

```{r}
polaroid <- function(x) {
  R <- sqrt(sum(x^2))
  
  # if radius is length 0, then no angles
  if (R == 0) return(c(R))
  
  # for an n length input, return length n, but n-1 are thetas
  theta <- numeric(length(x) - 1)
  
  # atan for arctan 
  theta[1] <- atan2(x[2], x[1])
  if (theta[1] < 0) {
    theta[1] <- theta[1] + 2 * pi 
  }
  
  p <- length(x)
  # only need to do function once if input is 2
  # else need to start iterating 
  if (p > 2) {
    for (i in 2:(p - 1)) {
      numerator <- sqrt(sum(x[i:p]^2))
      denominator <- sqrt(sum(x[(i-1):p]^2))
      theta[i] <- acos(numerator / denominator)
    }
  }
  
  dat <- c(R, theta)
  dat 
}
```

```{r}
x <- c(1, 2, 3)
polarRep <- polaroid(x)
polarRep
```

## (b)

Write a function normalize which takes in a matrix and returns its normalized form: i.e., the matrix with rows scaled such that the sum of squares of each row is equal to 1. 

```{r}
normalize <- function(x) {
  rowNorm <- sqrt(rowSums(x^2))
  normMat <- x / rowNorm
  
  normMat[is.nan(normMat)] <- 0
  normMat
}

# here's one using sweep 
# normalize <- function(x) {
#   rowNorm <- sqrt(rowSums(x^2))
#   normMat <- sweep(x, 1, row_norms, FUN = "/")
#   normMat[is.nan(normMat)] <- 0
#   normMat
# }
```

```{r}
set.seed(42)  
testMat <- matrix(data = rnorm(12), 
                  nrow = 4, 
                  ncol = 3)
testMat
rowSums(testMat^2)

normMatEx <- normalize(x = testMat)
normMatEx
rowSums(normMatEx^2)
```

## (c)

Obtain a 1000X5 matrix $\mathbf{y}$ of $N(0, 1)$ pseudo-random deviates. Use apply and normalize to obtain the normalized values. Call this matrix $\mathbf{z}$. We test whether the columns of $\mathbf{z}$ are uniform on $U(-1, 1)$. One may test whether a sample $x \sim U(-1, 1)$ using ks.test(x, "punif", min=-1, max=1) where punif represents the cumulative distribution function of the uniform over range $(-1, 1)$. Summarize your results. 

```{r, warning = F}
set.seed(42)  

y <- matrix(data = rnorm(1000 * 5), 
            nrow = 1000, 
            ncol = 5)

z <- apply(X = y, 
           MARGIN = 2,
           FUN = function(col) {
             normalize(x = matrix(col, ncol = 1))
             }
           )

ksTests <- apply(X = z, 
                  MARGIN = 2, 
                  FUN = function(col) {
                    ks.test(col, "punif", min = -1, max = 1)
                    }
                  )

ksTests
```

The KS tests provide small p-values, which is evidence to support rejecting the null hypothesis that the column vectors are distributed by $U(-1, 1)$, each of the five vectors each column. As we have evidence to suggest the vectors are not distributed by $U(-1, 1)$, then we have evidence that our efforts to normalize the vectors did not effectively transform the data into a uniform distribution over this range (at least when normalizing the sum of squares of each row).

## (d)

Obtain polar representations of $\mathbf{y}$ using your function polaroid and test whether $R^2 \sim \chi^2_5$ distribution.

Provide a page of histograms or boxplots of $\theta_1, \theta_2, \theta_3, \theta_4$. Test whether these are from the uniform distributions on their respective ranges, i.e., $[0, 2\pi)$ for $\theta_1$, and $[0, \pi)$ for $\theta_2, \theta_3, \theta_4$. 

```{r, warning = F}
polarRep <- t(apply(X = y,
                    MARGIN = 1,
                    FUN = polaroid))

rVal <- polarRep[, 1]
thetaVal <- polarRep[, -1]
# thetaVal <- if (ncol(polarRep) > 1) polarRep[, -1] else NA
```

```{r}
rSq <- rVal^2
csTest <- ks.test(rSq, "pchisq", df = 5)
csTest
```

The large p-value provides evidence in support of not rejecting the null hypothesis that $R^2 \sim \chi^2_5$, such that we have evidence that $R^2 \sim \chi^2_5$. 

```{r}
par(mfrow = c(2, 2))

hist(thetaVal[, 1], 
     breaks = 30, 
     main = expression(theta[1]), 
     xlab = expression(theta[1]), 
     xlim = c(0, 2 * pi))

hist(thetaVal[, 2], 
     breaks = 30, 
     main = expression(theta[2]), 
     xlab = expression(theta[2]), 
     xlim = c(0, pi))

hist(thetaVal[, 3], 
     breaks = 30, 
     main = expression(theta[3]), 
     xlab = expression(theta[3]), 
     xlim = c(0, pi))

hist(thetaVal[, 4], 
     breaks = 30, 
     main = expression(theta[4]), 
     xlab = expression(theta[4]), 
     xlim = c(0, pi))

```

```{r, warning = F}
theta1Test <- ks.test(x = thetaVal[, 1] / (2 * pi), 
                      y = "punif", 
                      min = 0, 
                      max = 1)
theta2Test <- ks.test(x = thetaVal[, 2] / pi, 
                      y = "punif", 
                      min = 0, 
                      max = 1)
theta3Test <- ks.test(x = thetaVal[, 3] / pi, 
                      y = "punif", 
                      min = 0, 
                      max = 1)
theta4Test <- ks.test(x = thetaVal[, 4] / pi, 
                      y = "punif", 
                      min = 0, 
                      max = 1)

list(theta1 = theta1Test, 
     theta2 = theta2Test,
     theta3 = theta3Test, 
     theta4 = theta4Test)
```

The above statistical tests may be summarized as follows: 

$H_0: \theta_1 \sim U[0, 2\pi)$

$H_0: \text{individually and respectively } \theta_2, \theta_3, \theta_4 \sim U[0, \pi)$

We do not have evidence in support or rejecting the null hypothesis that $\theta_1 \sim U[0, 2\pi)$ such that we have evidence in support of $\theta_1 \sim U[0, 2\pi)$. 

However, we have small p-values in evidence of rejecting the null hypotheses that $\text{individually and respectively } \theta_2, \theta_3, \theta_4 \sim U[0, \pi)$, which is evidence in support of $\theta_2, \theta_3, \text{ and } \theta_4$ not having distributions of $U[0, \pi)$. 
