---
title: "Assignment 8 "
author: "Sam Olson"
output:
  pdf_document:
    toc: false
    number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem Description

Consider a problem of conducting a Bayesian analysis with a one-sample gamma model.
Assume that random variables $Y_1, \ldots, Y_n$ are independent and identically distributed with common probability density function (for $\alpha > 0$ and $\beta > 0$):

$$
f(y \mid \alpha, \beta) =
\frac{\beta^{\alpha}}{\Gamma(\alpha)} y^{\alpha - 1} \exp(-\beta y),
\quad y > 0.
$$

Suppose that we will assign a joint prior to $\alpha$ and $\beta$ in product form, with particular values of $A > 0$, $\gamma_0 > 0$, and $\lambda_0 > 0$:

$$
\pi_{\alpha}(\alpha) = \frac{1}{A} I(0 < \alpha < A), \quad
\pi_{\beta}(\beta) =
\frac{\lambda_0^{\gamma_0}}{\Gamma(\gamma_0)} \beta^{\gamma_0 - 1} e^{-\lambda_0 \beta},
\quad \beta > 0.
$$

Recall that in the analysis of an actual data set, $A, \gamma_0$ and $\lambda_0$ will be given specific numerical values. Since this is a simulated example and we have no actual prior information, use the following hyperparameters:

$$
A = 20, \quad \gamma_0 = 0.5, \quad \lambda_0 = 0.1.
$$

This gives prior expectation of 5.0 and prior variance of 50. The prior does focus probability on smaller values, but still has $Pr(\beta > 10) = 0.16$.

```{r}
gammaDat <- read.table("C:/Users/samue/OneDrive/Desktop/Iowa_State_PS/STAT 5200/PS/PS8/gammadat_bayes.txt", header = T)
source("C:/Users/samue/OneDrive/Desktop/Iowa_State_PS/STAT 5200/PS/PS8/sourceHW.R")
```

\newpage

## 1. 

Consider using a Metropolis–Hastings algorithm with independent random-walk proposals for $\alpha$ and $\beta$.

Suppose that our current values are ($\alpha_m, \beta_m$), and that the proposal ($\alpha^*, \beta^*$) has been generated from

$$
q(\alpha, \beta \mid \alpha_m, \beta_m)
$$

which is the product of independent random walks.

Identify the appropriate acceptance probability for the jump proposal ($\alpha^*, \beta^*$).

### Answer

I believe this question is being asked in the abstract, i.e., not for the specific dataset in question. Under that pretense: 

The target distribution for the Metropolis–Hastings algorithm is the joint posterior

$$
\pi(\alpha, \beta \mid y) \propto L(y \mid \alpha, \beta)\,\pi_\alpha(\alpha)\,\pi_\beta(\beta)
$$

where the likelihood for independent $Y_i \sim \text{Gamma}(\alpha, \beta)$ is

$$
L(y \mid \alpha, \beta)
= \prod_{i=1}^n \frac{\beta^\alpha}{\Gamma(\alpha)} y_i^{\alpha-1} e^{-\beta y_i}
$$

If the proposal distribution is a product of independent random walks,

$$
q(\alpha^*, \beta^* \mid \alpha_m, \beta_m)
= q_\alpha(\alpha^* \mid \alpha_m) q_\beta(\beta^* \mid \beta_m)
$$

and each random walk is symmetric, then the proposal densities cancel in the Hastings ratio.

The acceptance probability is therefore

$$
a\big((\alpha_m,\beta_m)\to(\alpha^*,\beta^*)\big)
=\min\left\{1,\,
\frac{\pi(\alpha^*,\beta^*\mid y)}{\pi(\alpha_m,\beta_m\mid y)}\right\}
=\min\left\{1,\,
\frac{L(y\mid \alpha^*,\beta^*)\,\pi_\alpha(\alpha^*)\,\pi_\beta(\beta^*)}
     {L(y\mid \alpha_m,\beta_m)\,\pi_\alpha(\alpha_m)\,\pi_\beta(\beta_m)}
\right\}
$$

Because $\pi_\alpha(\alpha)$ is uniform on $(0, A)$, this implies that any proposal with $\alpha^* \notin (0, A)$ or $\beta^* \le 0$ is automatically rejected ($a = 0$).

Using sufficient statistics $S_1 = \sum_{i=1}^n \log y_i$ and $S_2 = \sum_{i=1}^n y_i$, we can write

$$
\log r
= n\left[\alpha^* \log \beta^* - \log \Gamma(\alpha^*) - \alpha_m \log \beta_m + \log \Gamma(\alpha_m)\right]
+ (\alpha^* - \alpha_m) S_1 - (\beta^* - \beta_m) S_2
+ (\gamma_0 - 1)\left[\log \beta^* - \log \beta_m\right]
- \lambda_0(\beta^* - \beta_m)
$$

and

$$
a = \min\left\{ 1,\; \exp(\log r) \right\}
$$

This is the appropriate acceptance probability for the Metropolis–Hastings update of $(\alpha, \beta)$.

\newpage

## 2. 

On the course web page is a data set called `gammadat_bayes.txt`.

Program a Metropolis–Hastings algorithm and simulate 50,000 values from the joint posterior of $\alpha$, $\beta$, and $\mu = \alpha / \beta$.

Provide (with supporting evidence if appropriate):

* Information on how you selected a burn-in period. *NOTE: I do not expect you to compute Gelman-Rubin scale reduction factors for this assignment.*

* Information on how you tuned the algorithm for acceptance rate, including the random-walk variances and the final acceptance rate.

* Summaries of the marginal posterior distributions of $\alpha$ and $\beta$ and $\mu = \alpha / \beta$, including histograms and five-number summaries, 95% central credible intervals, and correlation between $\alpha$ and $\beta$ in the Markov chain.

### Answer

```{r, eval = T, echo = F, warning = F, message = F}
set.seed(43)
y <- as.numeric(gammaDat$y)

m <- mean(y); v <- var(y)
alpha0 <- if (v > 0) (m^2 / v) else 1.0
alpha0 <- max(0.10, min(alpha0, 19.9))
beta0  <- max(1e-6, alpha0 / max(m, 1e-6))
start1 <- c(alpha0, beta0)

valpha <- (0.20 * alpha0)^2
vbeta  <- (0.20 * beta0)^2
jumpvars1 <- c(valpha, vbeta)

## Pilot / diagnostics with B=0 to *see* transients
j_diag <- metropforgamma(
  dat = y,
  start = start1,
  priorpars = c(gamma0 = 0.5, lambda0 = 0.1, A = 20),
  jumpvars = jumpvars1,
  B = 0,
  M = 50000
)

runmean <- function(x) cumsum(x) / seq_along(x)

## Visually pick burn-in (no z-scores): set B_line to what you’ll justify in text
B_line <- 5000L   # <- conservative choice you’ll justify from the plots

par(mfrow = c(2,2))
plot(j_diag$alpha, type="l", main="alpha trace (B=0)", xlab="iter")
abline(v = B_line, col=2, lty=2, lwd=2)
lines(runmean(j_diag$alpha), col=4)

plot(j_diag$beta,  type="l", main="beta trace (B=0)",  xlab="iter")
abline(v = B_line, col=2, lty=2, lwd=2)
lines(runmean(j_diag$beta), col=4)

acf(j_diag$alpha, lag.max = 400, main="ACF alpha (full)")
acf(j_diag$beta,  lag.max = 400, main="ACF beta (full)")
par(mfrow = c(1,1))
```

We start with our initial values given in the problem statement. We run a sampling procedure without any tuning, first to determine a suitable Burn-In period. 

To that end, we use a "running mean" (blue) to get a sense of when variability in the parameter "stabilizes". We see that by iteration 2,000-3,000, the ACFs decay rapidly; to stay on the conservative side, we double this and set that equal to our Burn-In period, settling on a Burn-In period of 5,000. 

We then have some additional tuning. 

```{r, cache = T, eval = T, echo = F, warning = F, message = F}
scales <- c(0.5, 0.8, 1.0, 1.2, 1.5)
tune_tab <- do.call(rbind, lapply(scales, function(s){
  j <- metropforgamma(
    dat = y, start = start1,
    priorpars = c(0.5, 0.1, 20),
    jumpvars = jumpvars1 * s^2,
    B = 2000, M = 8000
  )
  data.frame(scale = s,
             valpha = (jumpvars1[1] * s^2),
             vbeta  = (jumpvars1[2] * s^2),
             accept = attr(j, "acceptprob"))
}))
tune_tab
```

We "tune" by considering a range of parameter values; this involves tuning the random-walk variances to reach a Metropolis-Hastings acceptance rate somewhere in the range of 20-60%, which is a heuristic noted in Chapter 7 notes on Simulation. Ultimately, we decided on using a "scale" of 1.2, corresponding to `jumpvars = jumpvars * 1.2^2`. 

This then leads us to run the whole simulation procedure again, this time with the suitable Burn-In Period and tuned parameters. 

```{r, cache = T, eval = T, echo = F, warning = F, message = F}
final <- metropforgamma(
  dat = y,
  start = start1,
  priorpars = c(0.5, 0.1, 20),
  jumpvars = jumpvars1 * 1.2^2,   # from tuning above
  B = 5000,                       # chosen visually from traces/ACFs
  M = 50000
)
acc_final <- attr(final, "acceptprob")

alpha <- final$alpha
beta  <- final$beta
mu    <- final$mu

fivenum_alpha <- fivenum(alpha);  ci_alpha <- quantile(alpha, c(0.025, 0.975))
fivenum_beta  <- fivenum(beta);   ci_beta  <- quantile(beta,  c(0.025, 0.975))
fivenum_mu    <- fivenum(mu);     ci_mu    <- quantile(mu,    c(0.025, 0.975))
corr_ab <- cor(alpha, beta)

list(
  accept = acc_final,
  five_num = list(alpha = fivenum_alpha, beta = fivenum_beta, mu = fivenum_mu),
  ci_95   = list(alpha = ci_alpha, beta = ci_beta, mu = ci_mu),
  corr_ab = corr_ab
)

par(mfrow = c(1,3))
hist(alpha, breaks="FD", main=expression(paste("Posterior of ", alpha)), xlab=expression(alpha)); abline(v=ci_alpha, lty=2)
hist(beta,  breaks="FD", main=expression(paste("Posterior of ", beta)),  xlab=expression(beta));  abline(v=ci_beta,  lty=2)
hist(mu,    breaks="FD", main=expression(paste("Posterior of ", mu==alpha/beta)), xlab=expression(mu)); abline(v=ci_mu, lty=2)
par(mfrow = c(1,1))
```

\newpage

## 3.

Using both the 75th percentile and the range as data characteristics of potential interest, compute posterior predictive p-values from 10,000 posterior predictive datasets.

### Answer

We then do additional posterior predictive checks to validate the results of our simulation. 

```{r, cache = T, eval = T, echo = F, warning = F, message = F}
## Posterior predictive p-values (inclusive)
set.seed(43)

Q1 <- function(z) as.numeric(quantile(z, 0.75))
Q2 <- function(z) diff(range(z))

Q_obs1 <- Q1(y); Q_obs2 <- Q2(y)

S <- length(alpha)   # kept draws
n <- length(y)

## Preallocate 2 x S (rows = stats, cols = draws)
yrep_stats <- matrix(NA_real_, nrow = 2, ncol = S)

for (s in seq_len(S)) {
  yrep <- rgamma(n, shape = alpha[s], rate = beta[s])
  yrep_stats[1, s] <- Q1(yrep)
  yrep_stats[2, s] <- Q2(yrep)
}

p_up_Q1  <- mean(yrep_stats[1, ] >= Q_obs1)
p_low_Q1 <- mean(yrep_stats[1, ] <= Q_obs1)
p_up_Q2  <- mean(yrep_stats[2, ] >= Q_obs2)
p_low_Q2 <- mean(yrep_stats[2, ] <= Q_obs2)

list(
  ppp_75th  = c(p_low = p_low_Q1, p_up = p_up_Q1),
  ppp_range = c(p_low = p_low_Q2, p_up = p_up_Q2)
)

# yrep_stats <- vapply(
#   seq_len(S),
#   function(s) {
#     yrep <- rgamma(n, shape = alpha[s], rate = beta[s])
#     c(Q1(yrep), Q2(yrep))
#   },
#   FUN.VALUE = numeric(2L)
# )
# yrep_stats now has 2 rows (stats) x S columns (draws)
```

Generally, we want posterior predictive p-values that are not too large and not too small (so somewhere in the range of 0.2 to 0.7, or so); the values we observe for the statitics of interest seem suitable. 

\newpage

## 4. 

Now consider the use of a Gibbs Sampling algorithm to simulate from the joint posterior of $\alpha$ and $\beta$ and $\mu$. Derive full conditional posterior densities for $\alpha$ and $\beta$ Using these distributions, program a Gibbs Sampling algorithm and simulate 50,000 values from the joint posterior. Provide (with supporting evidence if appropriate),

* information on how you selected a burn-in period. Again, there is no need to compute Gelman-Rubin scale reduction factors for this assignment.

* summaries of the marginal posterior distributions of $\alpha$ and $\beta$, including histograms and five-number summaries, 95% central credible intervals, and correlation between $\alpha$ and $\beta$ in the Markov chain.

### Answer

Let $Y_i \stackrel{\text{iid}}{\sim}\text{Gamma}(\alpha,\beta)$ with density

$f(y\mid \alpha,\beta)=\dfrac{\beta^{\alpha}}{\Gamma(\alpha)},y^{\alpha-1}e^{-\beta y}$ for $y>0$.

Priors: $\alpha\sim\text{Uniform}(0,A)$ and $\beta\sim\text{Gamma}(\gamma_0,\lambda_0)$ in using the rate parametrization.

Denote $S_1=\sum_{i=1}^n \log y_i$ and $S_2=\sum_{i=1}^n y_i$.

The joint posterior (up to a constant) is

$$
\pi(\alpha,\beta \mid y)
\propto
\beta^{n\alpha} e^{-\beta S_2}
\frac{e^{(\alpha-1)S_1}}{[\Gamma(\alpha)]^{n}}
\beta^{\gamma_0-1} e^{-\lambda_0\beta}
\mathbb{1}_{(0,A)}(\alpha)
$$

Collecting terms in $\beta$ gives the kernel

$$
\beta^{n\alpha+\gamma_0-1}\exp\big(-(\lambda_0+S_2)\beta\big)
$$

so

$$
\beta\mid \alpha,y\ \sim\ \mathrm{Gamma}\left(\ \gamma_0+n\alpha,\ \lambda_0+S_2\ \right)\
$$

Collecting terms in $\alpha$ yields

$$
\pi(\alpha\mid \beta,y)\ \propto
\exp\Big(n\alpha\log\beta - n\log\Gamma(\alpha) + (\alpha-1)S_1\Big)\mathbf 1_{(0,A)}(\alpha)
$$

which is not a standard family (because of $\log\Gamma(\alpha)$). Therefore $\alpha$ is updated by a random-walk Metropolis step inside the Gibbs sampler (Metropolis-within-Gibbs), with proposals constrained to $(0,A)$.

```{r, cache = T, eval = T, echo = F, warning = F, message = F}
## ---- Gibbs: diagnostics first (no burn-in) ------------------------------
set.seed(43)

y <- as.numeric(gammaDat$y)
A <- 20
m <- mean(y); v <- var(y)

alpha0 <- max(0.10, min(ifelse(v > 0, m^2 / v, 1.0), A - 0.1))
beta0  <- max(1e-6, alpha0 / max(m, 1e-6))
start  <- c(alpha0, beta0)
priorpars <- c(gamma0 = 0.5, lambda0 = 0.1, A = A)

## Metropolis-within-Gibbs: only alpha uses MH; tune its variance
valpha <- (0.15 * alpha0)^2

## Keep everything for visual diagnostics
diag_all <- gibbsforgamma(
  dat = y, start = start, priorpars = priorpars,
  B = 0, M = 60000, valpha = valpha
)

alpha_all <- diag_all$alpha
beta_all  <- diag_all$beta
mu_all    <- diag_all$mu

runmean <- function(x) cumsum(x) / seq_along(x)

## Visually chosen burn-in (set after inspecting plots)
B_line <- 5000L   # <- state this choice in your text, justified by the plots

par(mfrow = c(2,2))
plot(alpha_all, type="l", main="alpha trace (B=0)", xlab="iter", ylab=expression(alpha))
abline(v=B_line, col=2, lty=2, lwd=2); lines(runmean(alpha_all), col=4)

plot(beta_all,  type="l", main="beta trace (B=0)",  xlab="iter", ylab=expression(beta))
abline(v=B_line, col=2, lty=2, lwd=2); lines(runmean(beta_all),  col=4)

acf(alpha_all, lag.max=400, main="ACF alpha (full)")
acf(beta_all,  lag.max=400, main="ACF beta (full)")
par(mfrow = c(1,1))
```

We start with our initial values given in the problem statement. We run a sampling procedure without any tuning, first to determine a suitable Burn-In period. 

Similar to Question 2, we use a "running mean" (blue) to get a sense of when variability in the parameter "stabilizes". We see that by iteration 2,000-3,000, the ACFs decay rapidly; to stay on the conservative side, we double this and set that equal to our Burn-In period, settling on a Burn-In period of 5,000. 

Then, and again, similarly to question 2, we have some additional tuning to do: 

```{r, cache = T, eval = T, echo = F, warning = F, message = F}
## helper: compute MH acceptance for alpha from the stored chain
alpha_accept_from_chain <- function(alpha_chain) {
  if (length(alpha_chain) < 2) return(NA_real_)
  mean(alpha_chain[-1] != alpha_chain[-length(alpha_chain)])
}

scales <- c(0.7, 1.0, 1.2, 1.5)
tune_tab <- do.call(rbind, lapply(scales, function(s){
  j <- gibbsforgamma(
    dat = y, start = start, priorpars = priorpars,
    B = 2000, M = 8000,
    valpha = valpha * s^2
  )
  data.frame(
    scale   = s,
    valpha  = valpha * s^2,
    acc_alpha = if (!is.null(j$alpha_accept_overall))
                  j$alpha_accept_overall
                else
                  alpha_accept_from_chain(j$alpha)
  )
}))
tune_tab
```

And now with the suitable Burn-In period and tuned parameters, we run our full simulation using the Gibbs method: 

```{r, cache = T, eval = T, echo = F, warning = F, message = F}
## ---- Final production run with tuned valpha and chosen burn-in ---------- 
final <- gibbsforgamma(
  dat = y, start = start, priorpars = priorpars,
  B = B_line, M = 50000,
  valpha = valpha * 1.2^2   # pick from tune_tab row with good acc (~0.3–0.5)
)
acc_alpha_final <- if (!is.null(final$alpha_accept_overall)) final$alpha_accept_overall else NA_real_

alpha <- final$alpha
beta  <- final$beta
mu    <- final$mu

## Summaries
fivenum_alpha <- fivenum(alpha);  ci_alpha <- quantile(alpha, c(0.025, 0.975))
fivenum_beta  <- fivenum(beta);   ci_beta  <- quantile(beta,  c(0.025, 0.975))
fivenum_mu    <- fivenum(mu);     ci_mu    <- quantile(mu,    c(0.025, 0.975))
corr_ab <- cor(alpha, beta)

list(
  valpha_final = valpha * 1.2^2,
  alpha_accept_final = acc_alpha_final,
  five_num = list(alpha = fivenum_alpha, beta = fivenum_beta, mu = fivenum_mu),
  ci_95   = list(alpha = ci_alpha, beta = ci_beta, mu = ci_mu),
  corr_ab = corr_ab
)
```

```{r, eval = T, echo = F, warning = F, message = F}
par(mfrow = c(1,3))
hist(alpha, breaks="FD", main=expression(paste("Posterior of ", alpha)), xlab=expression(alpha)); abline(v=ci_alpha, lty=2)
hist(beta,  breaks="FD", main=expression(paste("Posterior of ", beta)),  xlab=expression(beta));  abline(v=ci_beta,  lty=2)
hist(mu,    breaks="FD", main=expression(paste("Posterior of ", mu==alpha/beta)), xlab=expression(mu)); abline(v=ci_mu, lty=2)
par(mfrow = c(1,1))
```

\newpage

## 5. 

Using both the 75th percentile and the range as data characteristics of potential interest, compute posterior predictive p-values from 10,000 simulated posterior predictive data sets.

### Answer

```{r, eval = T, echo = F, warning = F, message = F}
## ---- Q5: Posterior predictive checks (75th percentile & range) ----------
set.seed(43)

gout <- final
# 0) inputs (post–burn-in Gibbs draws)
stopifnot(exists("gout"), is.data.frame(gout),
          all(c("alpha","beta") %in% names(gout)))
stopifnot(exists("y"), is.numeric(y), all(y > 0))

n <- length(y)

# 1) observed statistics
stat_q75   <- function(x) as.numeric(quantile(x, 0.75, type = 7))
stat_range <- function(x) diff(range(x))

Tobs_q75   <- stat_q75(y)
Tobs_range <- stat_range(y)

# 2) choose posterior draws for PPC
ndraws <- 10000L
K <- nrow(gout)
idx <- if (K >= ndraws) sample.int(K, ndraws, replace = FALSE) else sample.int(K, ndraws, replace = TRUE)
a_draws <- gout$alpha[idx]
b_draws <- gout$beta[idx]

# 3) generate posterior predictive datasets and compute replicated stats
Trep_q75   <- numeric(ndraws)
Trep_range <- numeric(ndraws)

for(k in seq_len(ndraws)){
  yrep <- rgamma(n, shape = a_draws[k], rate = b_draws[k])
  Trep_q75[k]   <- stat_q75(yrep)
  Trep_range[k] <- stat_range(yrep)
}

# 4) inclusive one-sided p-values and (optional) report both tails
p_up_q75   <- mean(Trep_q75   >= Tobs_q75)
p_low_q75  <- mean(Trep_q75   <= Tobs_q75)
p_up_range <- mean(Trep_range >= Tobs_range)
p_low_range<- mean(Trep_range <= Tobs_range)

# 5) (optional) two-sided tail-area about the replicated median
twosided <- function(Trep, Tobs){
  med <- median(Trep)
  mean(abs(Trep - med) >= abs(Tobs - med))
}
p_2s_q75   <- twosided(Trep_q75,   Tobs_q75)
p_2s_range <- twosided(Trep_range, Tobs_range)

# 6) report
list(
  observed_statistics = c(q75 = Tobs_q75, range = Tobs_range),
  ppp_upper_inclusive = c(q75 = p_up_q75,  range = p_up_range),
  ppp_lower_inclusive = c(q75 = p_low_q75, range = p_low_range),
  ppp_two_sided_median = c(q75 = p_2s_q75, range = p_2s_range),
  notes = "yrep_i ~ Gamma(alpha_k, rate=beta_k) for i=1..n; inclusive tail areas reported."
)
```

Generally, we want posterior predictive p-values that are not too large and not too small (so somewhere in the range of 0.2 to 0.7, or so); the values we observe for the statitics of interest do not seem suitable. Further comparison and analysis is discussed in Question 6. 

\newpage

## 6. 

Compare your results from the use of Metropolis-Hastings and Gibbs Sampling.

### Answer



\newpage 

## 7.

On this particular assignment, attach your R code for functions you programmed to do the necessary computations as an APPENDIX – not part of the body of your answer.

### Metropolis 

```{r appendix Metropolis, echo = T, eval = F}
metropforgamma <- function(dat, start, priorpars, jumpvars, B, M){
# Metropolis for a one-sample Gamma(shape = alpha, rate = beta) model
# with product prior: alpha ~ Uniform(0, A), beta ~ Gamma(gamma0, lambda0)
#
# dat        : vector of observed positive data (y_i > 0)
# start      : c(alpha0, beta0)  -- starting values for (alpha, beta)
# priorpars  : c(gamma0, lambda0, A)
#              - gamma0, lambda0 are shape/rate of prior on beta
#              - A is the upper bound for alpha's Uniform(0, A) prior
# jumpvars   : c(valpha, vbeta)  -- proposal variances for random-walk jumps
# B          : burn-in iterations
# M          : number of kept Monte Carlo draws
#
# Notes on parameterization/statistics:
# - Likelihood: Y_i ~ Gamma(alpha, beta) with density
#       f(y | alpha, beta) = beta^alpha / Gamma(alpha) * y^(alpha-1) * exp(-beta*y)
#   The log-likelihood is computed in a numerically stable way via sums.
# - Prior on alpha: Uniform(0, A). Inside (0, A) its log prior is constant (0),
#   outside the interval, log prior is -Inf.
# - Prior on beta: Gamma(gamma0, lambda0) with 'rate' parameterization.
# - Proposals: independent Gaussian random walks on alpha and beta, consistent
#   with the reference style. We clip invalid proposals by reverting to current
#   values, mirroring the reference behavior for sig2.
#
  calpha <- start[1]; cbeta <- start[2]
  gamma0 <- priorpars[1]; lambda0 <- priorpars[2]; A <- priorpars[3]
  valpha <- jumpvars[1]; vbeta <- jumpvars[2]

  alphas <- NULL; betas <- NULL; mus <- NULL
  acceptind <- 0
  cnt <- 0

  # Precompute sufficient statistics for the Gamma likelihood
  n <- length(dat)
  sumlogy <- sum(log(dat))
  sumy <- sum(dat)

  repeat{
    cnt <- cnt + 1
    alphastar <- proposealpha(calpha, valpha, A)
    betastar  <- proposebeta(cbeta, vbeta)

    # log-likelihood (current and proposed)
    #   log f(alpha, beta | y) = n * (alpha * log(beta) - log(Gamma(alpha))) +
    #                            (alpha - 1) * sum(log(y)) - beta * sum(y)
    lfcur  <- n * (calpha * log(cbeta) - lgamma(calpha)) + (calpha - 1) * sumlogy - cbeta * sumy
    lfstar <- n * (alphastar * log(betastar) - lgamma(alphastar)) + (alphastar - 1) * sumlogy - betastar * sumy

    # log-prior for alpha: Uniform(0, A)
    #   log pi(alpha) = 0 for alpha in (0, A), -Inf otherwise
    lpi_alpha_cur  <- if(calpha > 0 && calpha < A) 0 else -Inf
    lpi_alpha_star <- if(alphastar > 0 && alphastar < A) 0 else -Inf

    # log-prior for beta: Gamma(gamma0, lambda0), rate parameterization
    #   log pi(beta) = gamma0 * log(lambda0) - log(Gamma(gamma0))
    #                  + (gamma0 - 1) * log(beta) - lambda0 * beta
    lpi_beta_cur  <- gamma0 * log(lambda0) - lgamma(gamma0) + (gamma0 - 1) * log(cbeta)  - lambda0 * cbeta
    lpi_beta_star <- gamma0 * log(lambda0) - lgamma(gamma0) + (gamma0 - 1) * log(betastar) - lambda0 * betastar

    lpicur  <- lpi_alpha_cur + lpi_beta_cur
    lpistar <- lpi_alpha_star + lpi_beta_star

    # Metropolis acceptance (symmetric random-walk proposals)
    astar <- min(exp((lfstar + lpistar) - (lfcur + lpicur)), 1)
    ustar <- runif(1, 0, 1)

    newalpha <- calpha; newbeta <- cbeta
    if(ustar <= astar){
      newalpha <- alphastar; newbeta <- betastar
      acceptind <- acceptind + 1
    }

    if(cnt > B){
      alphas <- c(alphas, newalpha)
      betas  <- c(betas,  newbeta)
      mus    <- c(mus,    newalpha / newbeta)  # Posterior samples of mu = alpha / beta
    }

    calpha <- newalpha; cbeta <- newbeta
    if(cnt == (B + M)) break
  }

  cat("acceptprob:", acceptind / M, fill = TRUE)
  res <- data.frame(alpha = alphas, beta = betas, mu = mus)
  attr(res, "acceptprob") <- acceptind / M
  return(res)
}
#----------------------------------------------------------------
proposealpha <- function(calpha, valpha, A){
# propose jump from random walk for alpha (shape), enforce support (0, A)
# Reference-style: if invalid, revert to current (like proposesig2 in the ref)
  z <- rnorm(1, 0, sqrt(valpha))
  alphastar <- calpha + z
  if(alphastar <= 0 || alphastar >= A) alphastar <- calpha
  return(alphastar)
}
#----------------------------------------------------------------
proposebeta <- function(cbeta, vbeta){
# propose jump from random walk for beta (rate), enforce positivity
# Reference-style: if invalid, revert to current
  z <- rnorm(1, 0, sqrt(vbeta))
  betastar <- cbeta + z
  if(betastar <= 0) betastar <- cbeta
  return(betastar)
}
#----------------------------------------------------------------
```

### Gibbs 

```{r appendix Gibbs, echo = T, eval = F}
gibbsforgamma <- function(dat, start, priorpars, B, M, valpha){
# Gibbs sampler for one-sample Gamma(shape = alpha, rate = beta) model
# with alpha ~ Uniform(0, A), beta ~ Gamma(gamma0, lambda0)
#
# dat        : vector of observed positive data (y_i > 0)
# start      : c(alpha0, beta0)  -- starting values
# priorpars  : c(gamma0, lambda0, A)
# B          : burn-in iterations
# M          : number of kept Monte Carlo draws
# valpha     : proposal variance for MH step on alpha (Metropolis-within-Gibbs)
#
# Notes on full conditionals and conjugacy:
# - Conditional for beta | alpha, y is Gamma(gamma0 + n*alpha, lambda0 + sum(y))
#   (shape/rate parametrization) -- this is conjugate, so we can sample beta directly.
# - Conditional for alpha | beta, y is NOT standard:
#       p(alpha | beta, y) proportional to [beta^(n*alpha) / Gamma(alpha)^n] *
#       (prod(y_i))^(alpha - 1) * I(0 < alpha < A)
#   We use a random-walk MH step for alpha inside the Gibbs loop
#   (Metropolis-within-Gibbs), mirroring the reference Gibbs code structure.
#
  calpha <- start[1]; cbeta <- start[2]
  gamma0 <- priorpars[1]; lambda0 <- priorpars[2]; A <- priorpars[3]

  alphas <- NULL; betas <- NULL; mus <- NULL
  cnt <- 0
  accept_alpha <- 0

  # Precompute sufficient statistics
  n <- length(dat)
  sumlogy <- sum(log(dat))
  sumy <- sum(dat)

  repeat{
    cnt <- cnt + 1

    # 1) Sample beta | alpha, y  (conjugate Gamma)
    #    shape = gamma0 + n*alpha ; rate = lambda0 + sum(y)
    newbeta <- rgamma(1, shape = gamma0 + n * calpha, rate = lambda0 + sumy)

    # 2) Sample alpha | beta, y  (Metropolis step within Gibbs)
    #    target log-density up to constant:
    #       log p(alpha | beta, y) = n*alpha*log(beta) - n*log(Gamma(alpha))
    #                                + (alpha - 1)*sum(log(y)), for 0<alpha<A
    astep <- sampalpha_mh(calpha, newbeta, valpha, sumlogy, n, A)
    newalpha <- astep$alpha
    accept_alpha <- accept_alpha + astep$acc

    if(cnt > B){
      alphas <- c(alphas, newalpha)
      betas  <- c(betas,  newbeta)
      mus    <- c(mus,    newalpha / newbeta)
    }
    calpha <- newalpha; cbeta <- newbeta

    if(cnt == (B + M)) break
  }

  cat("alpha_acceptprob (within Gibbs):", accept_alpha / M, fill = TRUE)
  res <- data.frame(alpha = alphas, beta = betas, mu = mus)
  return(res)
}
#----------------------------------------------------------------
sampalpha_mh <- function(calpha, beta, valpha, sumlogy, n, A){
# One-step random-walk MH update for alpha (shape) given beta and y.
# Returns a list(alpha = ..., acc = 0/1)
#
# target log-density (up to constant in alpha):
#   log f(alpha | beta, y) = n*alpha*log(beta) - n*log(Gamma(alpha))
#                            + (alpha - 1)*sumlogy
# with support 0 < alpha < A; outside support, log-density = -Inf.
#
  z <- rnorm(1, 0, sqrt(valpha))
  alphastar <- calpha + z
  if(alphastar <= 0 || alphastar >= A){
    # As in reference style, invalid proposal -> revert (equivalent to reject)
    return(list(alpha = calpha, acc = 0))
  }

  # log target at current and proposed
  lfcur  <- n * calpha   * log(beta) - n * lgamma(calpha)   + (calpha   - 1) * sumlogy
  lfstar <- n * alphastar * log(beta) - n * lgamma(alphastar) + (alphastar - 1) * sumlogy

  a <- min(exp(lfstar - lfcur), 1)
  u <- runif(1, 0, 1)
  if(u <= a) return(list(alpha = alphastar, acc = 1))
  return(list(alpha = calpha, acc = 0))
}
#----------------------------------------------------------------
```
