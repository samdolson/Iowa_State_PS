---
title: "Assignment 8 "
author: "Sam Olson"
output:
  pdf_document:
    toc: false
    number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem Description

Consider a problem of conducting a Bayesian analysis with a one-sample gamma model.
Assume that random variables $Y_1, \ldots, Y_n$ are independent and identically distributed with common probability density function (for $\alpha > 0$ and $\beta > 0$):

$$
f(y \mid \alpha, \beta) =
\frac{\beta^{\alpha}}{\Gamma(\alpha)} y^{\alpha - 1} \exp(-\beta y),
\quad y > 0.
$$

Suppose that we will assign a joint prior to $\alpha$ and $\beta$ in product form, with particular values of $A > 0$, $\gamma_0 > 0$, and $\lambda_0 > 0$:

$$
\pi_{\alpha}(\alpha) = \frac{1}{A} I(0 < \alpha < A), \quad
\pi_{\beta}(\beta) =
\frac{\lambda_0^{\gamma_0}}{\Gamma(\gamma_0)} \beta^{\gamma_0 - 1} e^{-\lambda_0 \beta},
\quad \beta > 0.
$$

Recall that in the analysis of an actual data set, $A, \gamma_0$ and $\lambda_0$ will be given specific numerical values. Since this is a simulated example and we have no actual prior information, use the following hyperparameters:

$$
A = 20, \quad \gamma_0 = 0.5, \quad \lambda_0 = 0.1.
$$

This gives prior expectation of 5.0 and prior variance of 50. The prior does focus probability on smaller values, but still has $Pr(\beta > 10) = 0.16$.

```{r, echo = F, warning = F, message = F}
gammaDat <- read.table("C:/Users/samue/OneDrive/Desktop/Iowa_State_PS/STAT 5200/PS/PS8/gammadat_bayes.txt", header = T)
source("C:/Users/samue/OneDrive/Desktop/Iowa_State_PS/STAT 5200/PS/PS8/sourceHW.R")
library(kableExtra)
```

\newpage

## 1. 

Consider using a Metropolis–Hastings algorithm with independent random-walk proposals for $\alpha$ and $\beta$.

Suppose that our current values are ($\alpha_m, \beta_m$), and that the proposal ($\alpha^*, \beta^*$) has been generated from

$$
q(\alpha, \beta \mid \alpha_m, \beta_m)
$$

which is the product of independent random walks.

Identify the appropriate acceptance probability for the jump proposal ($\alpha^*, \beta^*$).

### Answer

I believe this question is being asked in the abstract, i.e., not for the specific dataset in question. Under that pretense: 

The target distribution for the Metropolis–Hastings algorithm is the joint posterior:

$$
\pi(\alpha, \beta \mid y) \propto L(y \mid \alpha, \beta)\,\pi_\alpha(\alpha)\,\pi_\beta(\beta)
$$

where $L$ denotes the likelihood (not log-transformed), and with the likelihood for independent $Y_i \sim \text{Gamma}(\alpha, \beta)$ given by: 

$$
L(y \mid \alpha, \beta)
= \prod_{i=1}^n \frac{\beta^\alpha}{\Gamma(\alpha)} y_i^{\alpha-1} e^{-\beta y_i}
$$

If the proposal distribution is a product of independent random walks, then it must satisfy the relation: 

$$
q(\alpha^*, \beta^* \mid \alpha_m, \beta_m)
= q_\alpha(\alpha^* \mid \alpha_m) q_\beta(\beta^* \mid \beta_m)
$$

The acceptance probability is therefore

$$
a\big((\alpha_m,\beta_m)\to(\alpha^*,\beta^*)\big)
=\min\left\{1,\,
\frac{\pi(\alpha^*,\beta^*\mid y)}{\pi(\alpha_m,\beta_m\mid y)}\right\}
=\min\left\{1,\,
\frac{L(y\mid \alpha^*,\beta^*)\,\pi_\alpha(\alpha^*)\,\pi_\beta(\beta^*)}
     {L(y\mid \alpha_m,\beta_m)\,\pi_\alpha(\alpha_m)\,\pi_\beta(\beta_m)}
\right\}
$$

Because $\pi_\alpha(\alpha)$ is uniform on $(0, A)$, this implies that any proposal with $\alpha^* \notin (0, A)$ or $\beta^* \le 0$ is automatically rejected ($a = 0$). However, for our purposes we would want to use the log-likelihoods $\ell$, as it will enforce positive values for $\alpha, \beta$ respectively (and generally seems to "stabilize" the sampling method). 

Knowing the data model comes from a one-sample Gamma, we know it comes from the exponential dispersion family. This is used primarily to motivate knowing the form of the sufficient statistics, $S_1 = \sum_{i=1}^n \log y_i$ and $S_2 = \sum_{i=1}^n y_i$. With this, we can write: 

$$
\log r
= n\left[\alpha^* \log \beta^* - \log \Gamma(\alpha^*) - \alpha_m \log \beta_m + \log \Gamma(\alpha_m)\right]
+ (\alpha^* - \alpha_m) S_1 - (\beta^* - \beta_m) S_2
+ (\gamma_0 - 1)\left[\log \beta^* - \log \beta_m\right]
- \lambda_0(\beta^* - \beta_m)
$$

Such that we have the acceptance probability for the Metropolis–Hastings update of $(\alpha, \beta)$ (exactly) as: 

$$
a = \min\left\{ 1,\; \exp(\log r) \right\}
$$

\newpage

## 2. 

On the course web page is a data set called `gammadat_bayes.txt`.

Program a Metropolis–Hastings algorithm and simulate 50,000 values from the joint posterior of $\alpha$, $\beta$, and $\mu = \alpha / \beta$.

Provide (with supporting evidence if appropriate):

* Information on how you selected a burn-in period. *NOTE: I do not expect you to compute Gelman-Rubin scale reduction factors for this assignment.*

* Information on how you tuned the algorithm for acceptance rate, including the random-walk variances and the final acceptance rate.

* Summaries of the marginal posterior distributions of $\alpha$ and $\beta$ and $\mu = \alpha / \beta$, including histograms and five-number summaries, 95% central credible intervals, and correlation between $\alpha$ and $\beta$ in the Markov chain.

### Answer

To start, while not affecting the priors (in distribution or parameter value), we do want to look at the data to get our "initial" values. 

Based on method of moments estimators, we have: 

$\alpha_0 = \frac{\bar{y}}{s_{y}^2}\,, \beta_0 = \frac{\alpha_0}{\bar{y}}$

Where $\bar{y}$ and $s_{y}^2$ are the sample mean and variance, respectively. 

For both $\alpha_0$ and $\beta_0$, we will "impose" that any estimates or updates retain $\alpha_0, \beta_0 > 0$. 

Additionally, given the hyperparameters to this problem, we also "impose" that $\alpha_0 < 20$. 

We run a sampling procedure without any tuning, first to determine a suitable Burn-In period. 

Note: Throughout the sampling procedure, we simulate $\alpha$ and $\beta$ simultaneously, which is important to keep later estimates of $\mu$ more comparable ("apples-to-apples"). Furthermore, the actual sampling is done using log likelihood, again, for the purpose of stability. 

Our sample values provide a basis for determining our Burn-In Period. We use our Metropolis-Hastings algorithm for a rather large number (50,000) without *any* Burn-In, and then look at both the trace plots and autocorrelation plots (ACFs). With that, we have: 

```{r, include = F, cache = T, eval = T, echo = F, warning = F, message = F}
set.seed(43)
y <- as.numeric(gammaDat$y)

m <- mean(y)
v <- var(y)

alpha0 <- if (v > 0) (m^2 / v) else 1.0
alpha0 <- max(0.10, min(alpha0, 19.9))
beta0  <- max(1e-6, alpha0 / max(m, 1e-6))
start1 <- c(alpha0, beta0)

valpha <- (0.20 * alpha0)^2
vbeta  <- (0.20 * beta0)^2
jumpvars1 <- c(valpha, vbeta)

j_diag <- metropforgamma(
  dat = y,
  start = start1,
  priorpars = c(gamma0 = 0.5, lambda0 = 0.1, A = 20),
  jumpvars = jumpvars1,
  B = 0,
  M = 50000
)
```

```{r, cache = T, eval = T, echo = F, warning = F, message = F}
runmean <- function(x) cumsum(x) / seq_along(x)

# Burn-In 
B_line <- 5000L   

par(mfrow = c(1,2))
plot(j_diag$alpha, type="l", main="alpha trace (B=0)", xlab="iter")
abline(v = B_line, col=2, lty=2, lwd=2)
abline(v = B_line*2, col=2, lty=2, lwd=2)
lines(runmean(j_diag$alpha), col=4)

plot(j_diag$beta,  type="l", main="beta trace (B=0)",  xlab="iter")
abline(v = B_line, col=2, lty=2, lwd=2)
abline(v = B_line*2, col=2, lty=2, lwd=2)

lines(runmean(j_diag$beta), col=4)
par(mfrow = c(1,2))
acf(j_diag$alpha, lag.max = 5000, main="ACF alpha (full)")
acf(j_diag$beta,  lag.max = 5000, main="ACF beta (full)")
par(mfrow = c(1,1))
```

To that end, we use a "running mean" (blue) to get a sense of when variability in the parameter "stabilizes" in the Trace Plot graphs. We see that by iteration 5,000, $\alpha$ and $\beta$ samples tend to stabilize. Additionally, the ACFs decay rapidly (well before the 5,000); to stay on the conservative side, we double this and set that equal to our Burn-In period. 

Ultimately, we settle on a Burn-In period of 10,000. 

We then have some additional tuning. The general procedure is to "scale" the jump variance by some proportion of the original sample variance. To begin with, we settle on multipling the variance by some proportion, initially around 0.25 to 2, and then after iterating deciding on 0.5, 0.75, 1.0, 1.25, and 1.5. The acceptance probabilities for each of these scales are reported, using the Burn-In of 10,000. 

```{r, include = F, cache = T, eval = T, echo = F, warning = F, message = F}
scales <- c(0.5, 0.75, 1.0, 1.25, 1.5)
tune_tab <- do.call(rbind, lapply(scales, function(s){
  j <- metropforgamma(
    dat = y, start = start1,
    priorpars = c(0.5, 0.1, 20),
    jumpvars = jumpvars1 * s^2,
    B = 10000, M = 50000
  )
  data.frame(scale = s,
             valpha = (jumpvars1[1] * s^2),
             vbeta  = (jumpvars1[2] * s^2),
             accept = attr(j, "acceptprob"))
}))

```

```{r, cache = T, eval = T, echo = F, warning = F, message = F}
kable(round(tune_tab, 4))
```

Our goal in tuning is to reach a Metropolis-Hastings acceptance rate somewhere in the range of 20-60%, which is a heuristic noted in the Chapter 7 notes on Simulation. Ultimately, we decided on using a "scale" of 1.25, which given the seed and method noted, gives us an acceptance probability around 16.5%. Note: While this is on the lower end, I ultimately wanted to scale the jump variance somewhat higher than the sample to allow for greater exploration of a wider range of possible values for the "jumps".  

This then leads us to run the whole simulation procedure again, now using both the Burn-In period of 5,000 and the tuned parameters. 

```{r, include = F, cache = T, eval = T, echo = F, warning = F, message = F}
final <- metropforgamma(
  dat = y,
  start = start1,
  priorpars = c(0.5, 0.1, 20),
  # from tuning
  jumpvars = jumpvars1 * 1.25^2,   
  B = 10000,                       
  M = 50000
)

```

```{r, cache = T, eval = T, echo = F, warning = F, message = F}
library(knitr)
library(kableExtra)

acc_final <- attr(final, "acceptprob")
alpha <- final$alpha; beta <- final$beta; mu <- final$mu
fivenum_alpha <- fivenum(alpha);  ci_alpha <- quantile(alpha, c(0.025, 0.975))
fivenum_beta  <- fivenum(beta);   ci_beta  <- quantile(beta,  c(0.025, 0.975))
fivenum_mu    <- fivenum(mu);     ci_mu    <- quantile(mu,    c(0.025, 0.975))
corr_ab <- cor(alpha, beta, use = "complete.obs")

alphaMH <- alpha; betaMH <- beta; muMH <- mu
ci_alphaMH <- ci_alpha; ci_betaMH <- ci_beta; ci_muMH <- ci_mu

# Don't think I need this anymore
esc <- function(x) gsub("%", "\\\\%", x, fixed = TRUE)

one_method_tbl <- function(vec_list, method = NULL) {
  out <- do.call(
    rbind,
    lapply(vec_list, function(x) {
      fn <- fivenum(x)
      ci <- quantile(x, c(0.025, 0.975), names = FALSE)
      c(Min = fn[1], Q1 = fn[2], Median = fn[3], Q3 = fn[4], Max = fn[5],
        "CI 2.5" = ci[1], "CI 97.5" = ci[2])
    })
  )
  df <- data.frame(Parameter = names(vec_list), round(out, 4),
                   row.names = NULL)
  if (!is.null(method)) df <- cbind(Method = method, df)
  df
}

mh_tbl <- one_method_tbl(list(alpha = alphaMH, beta = betaMH, mu = muMH), method = "MH")

if (exists("alphaG") && exists("betaG") && exists("muG")) {
  gibbs_tbl <- one_method_tbl(list(alpha = alphaG, beta = betaG, mu = muG), method = "Gibbs")
  param_tbl <- rbind(mh_tbl, gibbs_tbl)
} else {
  param_tbl <- mh_tbl
}

kable(
  param_tbl, booktabs = TRUE, escape = TRUE,
  caption = esc("Posterior summaries: five-number statistics and 95 central credible intervals")
) |>
  kable_styling(full_width = FALSE, position = "center",
                latex_options = c("hold_position","striped")) |>
  column_spec(1, bold = TRUE) |>
  add_header_above(c(" " = if ("Method" %in% names(param_tbl)) 2 else 1,
                     "Posterior Summary" = ncol(param_tbl) -
                       if ("Method" %in% names(param_tbl)) 2 else 1))


nm <- names(param_tbl)

ci_low  <- nm[grepl("^CI(\\.| )2\\.5",  nm)]
ci_high <- nm[grepl("^CI(\\.| )97\\.5", nm)]

cols_ci <- c(if ("Method" %in% nm) "Method", "Parameter", ci_low, ci_high)
ci_only <- param_tbl[, cols_ci, drop = FALSE]

names(ci_only)[names(ci_only) == ci_low]  <- "Lower 2.5"
names(ci_only)[names(ci_only) == ci_high] <- "Upper 97.5"

kable(
  ci_only, booktabs = TRUE, escape = TRUE,
  caption = esc("95 central credible intervals")
) |>
  kable_styling(full_width = FALSE, position = "center",
                latex_options = c("hold_position","striped")) |>
  column_spec(1, bold = TRUE)

acc_vec <- as.numeric(acc_final)
acc_names <- names(acc_final)
if (is.null(acc_names)) {
  acc_names <- if (length(acc_vec) == 1) "Overall" else paste0("Component_", seq_along(acc_vec))
}
acc_tbl <- data.frame(Component = acc_names,
                      `Acceptance Rate` = paste0(round(100 * acc_vec, 1), "%"),
                      row.names = NULL)

kable(
  acc_tbl, booktabs = TRUE, escape = TRUE,
  caption = "Acceptance rates"
) |>
  kable_styling(full_width = FALSE, position = "center",
                latex_options = c("hold_position","striped")) |>
  column_spec(1, bold = TRUE)

corr_mat <- matrix(c(1, corr_ab, corr_ab, 1), nrow = 2,
                   dimnames = list(c("alpha","beta"), c("alpha","beta")))
corr_df <- data.frame(Parameter = rownames(corr_mat), round(corr_mat, 4), row.names = NULL)

kable(
  corr_df, booktabs = TRUE, escape = TRUE,
  caption = "Correlation matrix for alpha and beta"
) |>
  kable_styling(full_width = FALSE, position = "center",
                latex_options = c("hold_position","striped")) |>
  column_spec(1, bold = TRUE)
```

```{r, cache = T, eval = T, echo = F, warning = F, message = F}
par(mfrow = c(1,3))
hist(alphaMH, breaks="FD", main=expression(paste("Posterior of ", alpha)), xlab=expression(alpha)); abline(v=ci_alpha, lty=2)
hist(betaMH,  breaks="FD", main=expression(paste("Posterior of ", beta)),  xlab=expression(beta));  abline(v=ci_beta,  lty=2)
hist(muMH,    breaks="FD", main=expression(paste("Posterior of ", mu==alpha/beta)), xlab=expression(mu)); abline(v=ci_mu, lty=2)
par(mfrow = c(1,1))
```

\newpage

## 3.

Using both the 75th percentile and the range as data characteristics of potential interest, compute posterior predictive p-values from 10,000 posterior predictive datasets.

### Answer

We then do additional posterior predictive checks to validate the results of our simulation. Using the kept MH draws from the prior Question, we simulated 10,000 posterior predictive datasets. 

```{r, cache = T, eval = T, echo = F, warning = F, message = F}
set.seed(43)

Q1 <- function(z) as.numeric(quantile(z, 0.75))
Q2 <- function(z) diff(range(z))

Q_obs1 <- Q1(y); Q_obs2 <- Q2(y)

S <- length(alpha)   
n <- length(y)

yrep_stats <- matrix(NA_real_, nrow = 2, ncol = S)

for (s in seq_len(S)) {
  yrep <- rgamma(n, shape = alpha[s], rate = beta[s])
  yrep_stats[1, s] <- Q1(yrep)
  yrep_stats[2, s] <- Q2(yrep)
}

# upper-tail PPP for 75th percentile
ppp_Q75   <- mean(yrep_stats[1, ] >= Q_obs1)  
# upper-tail PPP for range
ppp_range <- mean(yrep_stats[2, ] >= Q_obs2)  

library(knitr); library(kableExtra)
ppp_tbl <- data.frame(
  Statistic = c("75th percentile", "Range"),
  Observed  = round(c(Q_obs1, Q_obs2), 4),
  PPP       = round(c(ppp_Q75, ppp_range), 4),
  check.names = FALSE
)

kable(
  ppp_tbl, booktabs = TRUE,
  caption = "Posterior predictive p-values (upper tail) from 10,000 replicates"
) |>
  kable_styling(full_width = FALSE, position = "center",
                latex_options = c("hold_position","striped")) |>
  column_spec(1, bold = TRUE)
```

Generally, we want posterior predictive p-values that are not too large and not too small (so somewhere in the range of 0.2 to 0.7, or so); the values we observe for the statistics of interest seem suitable. 

\newpage

## 4. 

Now consider the use of a Gibbs Sampling algorithm to simulate from the joint posterior of $\alpha$ and $\beta$ and $\mu$. Derive full conditional posterior densities for $\alpha$ and $\beta$ Using these distributions, program a Gibbs Sampling algorithm and simulate 50,000 values from the joint posterior. Provide (with supporting evidence if appropriate),

* information on how you selected a burn-in period. Again, there is no need to compute Gelman-Rubin scale reduction factors for this assignment.

* summaries of the marginal posterior distributions of $\alpha$ and $\beta$, including histograms and five-number summaries, 95% central credible intervals, and correlation between $\alpha$ and $\beta$ in the Markov chain.

### Answer

Let $Y_i \stackrel{\text{iid}}{\sim}\text{Gamma}(\alpha,\beta)$ with density

$f(y\mid \alpha,\beta)=\dfrac{\beta^{\alpha}}{\Gamma(\alpha)},y^{\alpha-1}e^{-\beta y}$ for $y>0$.

Priors: $\alpha\sim\text{Uniform}(0,A)$ and $\beta\sim\text{Gamma}(\gamma_0,\lambda_0)$ in using the rate parametrization.

Denote $S_1=\sum_{i=1}^n \log y_i$ and $S_2=\sum_{i=1}^n y_i$.

The joint posterior (up to a constant) is

$$
\pi(\alpha,\beta \mid y)
\propto
\beta^{n\alpha} e^{-\beta S_2}
\frac{e^{(\alpha-1)S_1}}{[\Gamma(\alpha)]^{n}}
\beta^{\gamma_0-1} e^{-\lambda_0\beta}
\mathbb{1}_{(0,A)}(\alpha)
$$

Collecting terms in $\beta$ gives the kernel

$$
\beta^{n\alpha+\gamma_0-1}\exp\big(-(\lambda_0+S_2)\beta\big)
$$

so

$$
\beta\mid \alpha,y\ \sim\ \mathrm{Gamma}\left(\ \gamma_0+n\alpha,\ \lambda_0+S_2\ \right)\
$$

Collecting terms in $\alpha$ yields

$$
\pi(\alpha\mid \beta,y)\ \propto
\exp\Big(n\alpha\log\beta - n\log\Gamma(\alpha) + (\alpha-1)S_1\Big)\mathbf 1_{(0,A)}(\alpha)
$$

which is not a standard family (because of $\log\Gamma(\alpha)$). Therefore $\alpha$ is updated by a random-walk Metropolis step inside the Gibbs sampler (Metropolis-within-Gibbs), with proposals constrained to $(0,A)$.

Now, with that proof out the way, we can begin assessing a suitable Burn-In and tuning (which, given the priors, will be done solely on $\alpha$ given conditional conjugacy). 

Similar to the MH method, we start by looking at trace plots and ACF plots with zero burn-in. Again, the fine details of the method are noted in the Appendix, but primarily focus on simulating $\alpha$ and $\beta$ simmultaneously, and working with the log-likelihood. 

```{r, include = F, cache = T, eval = T, echo = F, warning = F, message = F}
set.seed(43)

y <- as.numeric(gammaDat$y)
A <- 20
m <- mean(y); v <- var(y)

alpha0 <- max(0.10, min(ifelse(v > 0, m^2 / v, 1.0), A - 0.1))
beta0  <- max(1e-6, alpha0 / max(m, 1e-6))
start  <- c(alpha0, beta0)
priorpars <- c(gamma0 = 0.5, lambda0 = 0.1, A = A)

# Metropolis-within-Gibbs
# only alpha uses MH
valpha <- (0.15 * alpha0)^2

diag_all <- gibbsforgamma(
  dat = y, start = start, priorpars = priorpars,
  B = 0, M = 50000, valpha = valpha
)
```

```{r, cache = T, eval = T, echo = F, warning = F, message = F}
alpha_all <- diag_all$alpha
beta_all  <- diag_all$beta
mu_all    <- diag_all$mu

runmean <- function(x) cumsum(x) / seq_along(x)

# Burn-In
B_line <- 10000L

par(mfrow = c(1,2))
plot(alpha_all, type="l", main="alpha trace (B=0)", xlab="iter", ylab=expression(alpha))
abline(v=B_line, col=2, lty=2, lwd=2); lines(runmean(alpha_all), col=4)
abline(v=B_line*2, col=2, lty=2, lwd=2)

plot(beta_all,  type="l", main="beta trace (B=0)",  xlab="iter", ylab=expression(beta))
abline(v=B_line, col=2, lty=2, lwd=2); lines(runmean(beta_all),  col=4)
abline(v=B_line*2, col=2, lty=2, lwd=2)

par(mfrow = c(1,2))
acf(alpha_all, lag.max=10000, main="ACF alpha (full)")
acf(beta_all,  lag.max=10000, main="ACF beta (full)")
par(mfrow = c(1,1))
```

Similar to Question 2, we use a "running mean" (blue) to get a sense of when variability in the parameter "stabilizes". And again, similar (but different!) to Question 2, by iteration 10,000, the ACFs has decayed rapidly, and the trace plot has generally stabilized; to stay on the conservative side, we double this and set that equal to our Burn-In period, settling on a Burn-In period of 20,000. 

For tuning then, we again scale our $\alpha$ variance in increments of 0.25, considering the same range of scales, and targeting an acceptance probability around 20-60%. However, given this is using the Gibbs sampling method, I am cautious about not properly "mixing", so apriori I will prioritize lower acceptance proabilities (we will accept more sampling that don't make a "jump", where by comparisons the MH algorithm will actively discard unsuitable "jump" candidates). 

```{r, include = F, cache = T, eval = T, echo = F, warning = F, message = F}
# compute MH acceptance for alpha from the stored chain
alpha_accept_from_chain <- function(alpha_chain) {
  if (length(alpha_chain) < 2) return(NA_real_)
  mean(alpha_chain[-1] != alpha_chain[-length(alpha_chain)])
}

scales <- c(0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0)
tune_tab <- do.call(rbind, lapply(scales, function(s){
  j <- gibbsforgamma(
    dat = y, start = start, priorpars = priorpars,
    B = 20000, M = 50000,
    valpha = valpha * s^2
  )
  data.frame(
    scale   = s,
    valpha  = valpha * s^2,
    acc_alpha = if (!is.null(j$alpha_accept_overall))
                  j$alpha_accept_overall
                else
                  alpha_accept_from_chain(j$alpha)
  )
}))
```

```{r, cache = T, eval = T, echo = F, warning = F, message = F}
kable(round(tune_tab, 4))
```

Given the scaling of 2.0, with acceptance probability around 19.1%, we now have both a suitable Burn-In Period and tuned parameters for our "full simulation" via Gibbs. 

```{r, include = F, cache = T, eval = T, echo = F, warning = F, message = F}
B_line <- 20000L 

final <- gibbsforgamma(
  dat = y, start = start, priorpars = priorpars,
  B = B_line, M = 50000,
  # tuning parameter
  valpha = valpha * 2^2   
)
```

```{r, cache = T, eval = T, echo = F, warning = F, message = F}
acc_alpha_final <- if (!is.null(final$alpha_accept_overall)) final$alpha_accept_overall else NA_real_

alpha <- final$alpha
beta  <- final$beta
mu    <- final$mu 

fivenum_alpha <- fivenum(alpha)
fivenum_beta  <- fivenum(beta)
ci_alpha <- quantile(alpha, c(0.025, 0.975), names = FALSE)
ci_beta  <- quantile(beta,  c(0.025, 0.975), names = FALSE)

corr_ab <- cor(alpha, beta, use = "complete.obs")

library(knitr)
library(kableExtra)

esc <- function(x) gsub("%", "\\%", x, fixed = TRUE)

summ_tbl <- data.frame(
  Parameter = c("alpha","beta"),
  Min    = c(fivenum_alpha[1], fivenum_beta[1]),
  Q1     = c(fivenum_alpha[2], fivenum_beta[2]),
  Median = c(fivenum_alpha[3], fivenum_beta[3]),
  Q3     = c(fivenum_alpha[4], fivenum_beta[4]),
  Max    = c(fivenum_alpha[5], fivenum_beta[5]),
  `CI 2.5%`  = c(ci_alpha[1], ci_beta[1]),
  `CI 97.5%` = c(ci_alpha[2], ci_beta[2]),
  check.names = FALSE
)
summ_tbl[,-1] <- lapply(summ_tbl[,-1], function(z) round(as.numeric(z), 4))
names(summ_tbl) <- esc(names(summ_tbl))

kable(
  summ_tbl, booktabs = TRUE, escape = TRUE,
  caption = esc("Posterior summaries for alpha and beta: five-number statistics and 95 central credible intervals")
) |>
  kable_styling(full_width = FALSE, position = "center",
                latex_options = c("hold_position","striped")) |>
  column_spec(1, bold = TRUE) |>
  add_header_above(c(" " = 1, "Posterior Summary" = ncol(summ_tbl) - 1))

corr_mat <- matrix(c(1, corr_ab, corr_ab, 1), nrow = 2,
                   dimnames = list(c("alpha","beta"), c("alpha","beta")))
corr_df <- data.frame(Parameter = rownames(corr_mat), round(corr_mat, 4), row.names = NULL)
names(corr_df) <- esc(names(corr_df))

kable(
  corr_df, booktabs = TRUE, escape = TRUE,
  caption = "Correlation matrix for alpha and beta (from Markov chain)"
) |>
  kable_styling(full_width = FALSE, position = "center",
                latex_options = c("hold_position","striped")) |>
  column_spec(1, bold = TRUE)
```

```{r, eval = T, echo = F, warning = F, message = F}
alphaG <- alpha
ci_alphaG <- ci_alpha
betaG <- beta 
ci_betaG <- ci_beta
muG <- mu 
ci_muG <- ci_mu

par(mfrow = c(1,3))
hist(alphaG, breaks="FD", main=expression(paste("Posterior of ", alpha)), xlab=expression(alpha)); abline(v=ci_alpha, lty=2)
hist(betaG,  breaks="FD", main=expression(paste("Posterior of ", beta)),  xlab=expression(beta));  abline(v=ci_beta,  lty=2)
hist(muG,    breaks="FD", main=expression(paste("Posterior of ", mu==alpha/beta)), xlab=expression(mu)); abline(v=ci_mu, lty=2)
par(mfrow = c(1,1))
```

\newpage

## 5. 

Using both the 75th percentile and the range as data characteristics of potential interest, compute posterior predictive p-values from 10,000 simulated posterior predictive data sets.

### Answer

We then do additional posterior predictive checks to validate the results of our simulation. Using the Gibbs draws from the prior Question, we simulated 10,000 posterior predictive datasets. 

```{r, eval = T, echo = F, warning = F, message = F}
set.seed(43)

gout <- final
stopifnot(is.data.frame(gout), all(c("alpha","beta") %in% names(gout)))
stopifnot(exists("y"), is.numeric(y), all(y > 0))
n <- length(y)

# observed
stat_q75   <- function(x) as.numeric(quantile(x, 0.75, type = 7))
stat_range <- function(x) diff(range(x))
Tobs_q75   <- stat_q75(y)
Tobs_range <- stat_range(y)

# posterior draws
ndraws <- 10000L
K <- nrow(gout)
idx <- if (K >= ndraws) sample.int(K, ndraws) else sample.int(K, ndraws, replace = TRUE)
a_draws <- gout$alpha[idx]
b_draws <- gout$beta[idx]

# pposterior predictive datasets
Trep_q75   <- numeric(ndraws)
Trep_range <- numeric(ndraws)
for (k in seq_len(ndraws)) {
  yrep <- rgamma(n, shape = a_draws[k], rate = b_draws[k])
  Trep_q75[k]   <- stat_q75(yrep)
  Trep_range[k] <- stat_range(yrep)
}

ppp_q75   <- mean(Trep_q75   >= Tobs_q75)
ppp_range <- mean(Trep_range >= Tobs_range)

library(knitr)
library(kableExtra)

ppp_tbl <- data.frame(
  Statistic = c("75th percentile", "Range"),
  Observed  = round(c(Tobs_q75, Tobs_range), 4),
  PPP       = round(c(ppp_q75, ppp_range), 4),
  check.names = FALSE
)

kable(
  ppp_tbl, booktabs = TRUE,
  caption = "Posterior predictive p-values (upper tail) from 10,000 replicated datasets"
) |>
  kable_styling(full_width = FALSE, position = "center",
                latex_options = c("hold_position","striped")) |>
  column_spec(1, bold = TRUE) |>
  add_header_above(c(" " = 1, "Posterior Predictive Check" = 2))
```

Generally, we want posterior predictive p-values that are not too large and not too small (so somewhere in the range of 0.2 to 0.7, or so); the values we observe for the statitics of interest seem suitable, though potentially a bit large. Further comparison and analysis is discussed in Question 6. 

\newpage

## 6. 

Compare your results from the use of Metropolis-Hastings and Gibbs Sampling.

### Answer

```{r, eval = T, echo = F, warning = F, message = F}
# alpha_range <- range(c(alphaMH, alphaG))
# beta_range  <- range(c(betaMH, betaG))
# mu_range    <- range(c(muMH, muG))
# 
# alpha_breaks <- hist(c(alphaMH, alphaG), breaks = "FD", plot = FALSE)$breaks
# beta_breaks  <- hist(c(betaMH, betaG),  breaks = "FD", plot = FALSE)$breaks
# mu_breaks    <- hist(c(muMH, muG),    breaks = "FD", plot = FALSE)$breaks
# 
# alpha_ylim <- range(
#   hist(alphaMH, breaks = alpha_breaks, plot = FALSE)$counts,
#   hist(alphaG,  breaks = alpha_breaks, plot = FALSE)$counts
# )
# 
# beta_ylim <- range(
#   hist(betaMH, breaks = beta_breaks, plot = FALSE)$counts,
#   hist(betaG,  breaks = beta_breaks, plot = FALSE)$counts
# )
# 
# mu_ylim <- range(
#   hist(muMH, breaks = mu_breaks, plot = FALSE)$counts,
#   hist(muG,  breaks = mu_breaks, plot = FALSE)$counts
# )
make_breaks <- function(x1, x2, width = 0.1) seq(from = floor(x1), to = ceiling(x2), by = width)

alpha_range <- range(c(alphaMH, alphaG), na.rm = TRUE)
beta_range  <- range(c(betaMH,  betaG),  na.rm = TRUE)
mu_range    <- range(c(muMH,    muG),    na.rm = TRUE)

alpha_breaks <- make_breaks(min(alpha_range), max(alpha_range), width = 0.2)
beta_breaks  <- make_breaks(min(beta_range),  max(beta_range),  width = 0.05)
mu_breaks    <- make_breaks(min(mu_range),    max(mu_range),    width = 0.1)

alpha_ylim <- c(0, max(
  hist(alphaMH, breaks = alpha_breaks, plot = FALSE)$counts,
  hist(alphaG,  breaks = alpha_breaks, plot = FALSE)$counts
))
beta_ylim <- c(0, max(
  hist(betaMH, breaks = beta_breaks, plot = FALSE)$counts,
  hist(betaG,  breaks = beta_breaks, plot = FALSE)$counts
))
mu_ylim <- c(0, max(
  hist(muMH, breaks = mu_breaks, plot = FALSE)$counts,
  hist(muG,  breaks = mu_breaks, plot = FALSE)$counts
))
```

```{r, eval = T, echo = F, warning = F, message = F}
par(mfrow = c(1,2))

hist(alphaMH, breaks = alpha_breaks, xlim = alpha_range, ylim = alpha_ylim,
     main = expression(paste("MH - Posterior of ", alpha)),
     xlab = expression(alpha))
abline(v = ci_alphaMH, lty = 2)

hist(alphaG, breaks = alpha_breaks, xlim = alpha_range, ylim = alpha_ylim,
     main = expression(paste("Gibbs - Posterior of ", alpha)),
     xlab = expression(alpha))
abline(v = ci_alphaG, lty = 2)
```

```{r, eval = T, echo = F, warning = F, message = F}
par(mfrow = c(1,2))

hist(betaMH, breaks = beta_breaks, xlim = beta_range, ylim = beta_ylim,
     main = expression(paste("MH - Posterior of ", beta)),
     xlab = expression(beta))
abline(v = ci_betaMH, lty = 2)

hist(betaG, breaks = beta_breaks, xlim = beta_range, ylim = beta_ylim,
     main = expression(paste("Gibbs - Posterior of ", beta)),
     xlab = expression(beta))
abline(v = ci_betaG, lty = 2)
```

```{r, eval = T, echo = F, warning = F, message = F}
par(mfrow = c(1,2))

hist(muMH, breaks = mu_breaks, xlim = mu_range, ylim = mu_ylim,
     main = expression(paste("MH - Posterior of ", mu == alpha/beta)),
     xlab = expression(mu))
abline(v = ci_muMH, lty = 2)

hist(muG, breaks = mu_breaks, xlim = mu_range, ylim = mu_ylim,
     main = expression(paste("Gibbs - Posterior of ", mu == alpha/beta)),
     xlab = expression(mu))
abline(v = ci_muG, lty = 2)
```

\newpage 

## 7.

On this particular assignment, attach your R code for functions you programmed to do the necessary computations as an APPENDIX – not part of the body of your answer.

### Metropolis 

```{r appendix Metropolis, echo = T, eval = F}
metropforgamma <- function(dat, start, priorpars, jumpvars, B, M){
# Metropolis for a one-sample Gamma(shape = alpha, rate = beta) model
# with product prior: alpha ~ Uniform(0, A), beta ~ Gamma(gamma0, lambda0)
#
# dat        : vector of observed positive data (y_i > 0)
# start      : c(alpha0, beta0)  -- starting values for (alpha, beta)
# priorpars  : c(gamma0, lambda0, A)
#              - gamma0, lambda0 are shape/rate of prior on beta
#              - A is the upper bound for alpha's Uniform(0, A) prior
# jumpvars   : c(valpha, vbeta)  -- proposal variances for random-walk jumps
# B          : burn-in iterations
# M          : number of kept Monte Carlo draws
#
# Notes on parameterization/statistics:
# - Likelihood: Y_i ~ Gamma(alpha, beta) with density
#       f(y | alpha, beta) = beta^alpha / Gamma(alpha) * y^(alpha-1) * exp(-beta*y)
#   The log-likelihood is computed in a numerically stable way via sums.
# - Prior on alpha: Uniform(0, A). Inside (0, A) its log prior is constant (0),
#   outside the interval, log prior is -Inf.
# - Prior on beta: Gamma(gamma0, lambda0) with 'rate' parameterization.
# - Proposals: independent Gaussian random walks on alpha and beta, consistent
#   with the reference style. We clip invalid proposals by reverting to current
#   values, mirroring the reference behavior for sig2.
#
  calpha <- start[1]; cbeta <- start[2]
  gamma0 <- priorpars[1]; lambda0 <- priorpars[2]; A <- priorpars[3]
  valpha <- jumpvars[1]; vbeta <- jumpvars[2]

  alphas <- NULL; betas <- NULL; mus <- NULL
  acceptind <- 0
  cnt <- 0

  # Precompute sufficient statistics for the Gamma likelihood
  n <- length(dat)
  sumlogy <- sum(log(dat))
  sumy <- sum(dat)

  repeat{
    cnt <- cnt + 1
    alphastar <- proposealpha(calpha, valpha, A)
    betastar  <- proposebeta(cbeta, vbeta)

    # log-likelihood (current and proposed)
    #   log f(alpha, beta | y) = n * (alpha * log(beta) - log(Gamma(alpha))) +
    #                            (alpha - 1) * sum(log(y)) - beta * sum(y)
    lfcur  <- n * (calpha * log(cbeta) - lgamma(calpha)) + 
      (calpha - 1) * sumlogy - cbeta * sumy
    lfstar <- n * (alphastar * log(betastar) - lgamma(alphastar)) + 
      (alphastar - 1) * sumlogy - betastar * sumy

    # log-prior for alpha: Uniform(0, A)
    #   log pi(alpha) = 0 for alpha in (0, A), -Inf otherwise
    lpi_alpha_cur  <- if(calpha > 0 && calpha < A) 0 else -Inf
    lpi_alpha_star <- if(alphastar > 0 && alphastar < A) 0 else -Inf

    # log-prior for beta: Gamma(gamma0, lambda0), rate parameterization
    #   log pi(beta) = gamma0 * log(lambda0) - log(Gamma(gamma0))
    #                  + (gamma0 - 1) * log(beta) - lambda0 * beta
    lpi_beta_cur  <- gamma0 * log(lambda0) - lgamma(gamma0) + 
      (gamma0 - 1) * log(cbeta)  - lambda0 * cbeta
    lpi_beta_star <- gamma0 * log(lambda0) - lgamma(gamma0) + 
      (gamma0 - 1) * log(betastar) - lambda0 * betastar

    lpicur  <- lpi_alpha_cur + lpi_beta_cur
    lpistar <- lpi_alpha_star + lpi_beta_star

    # Metropolis acceptance (symmetric random-walk proposals)
    astar <- min(exp((lfstar + lpistar) - (lfcur + lpicur)), 1)
    ustar <- runif(1, 0, 1)

    newalpha <- calpha; newbeta <- cbeta
    if(ustar <= astar){
      newalpha <- alphastar; newbeta <- betastar
      acceptind <- acceptind + 1
    }

    if(cnt > B){
      alphas <- c(alphas, newalpha)
      betas  <- c(betas,  newbeta)
      # Posterior samples of mu = alpha / beta
      mus    <- c(mus,    newalpha / newbeta)  
    }

    calpha <- newalpha; cbeta <- newbeta
    if(cnt == (B + M)) break
  }

  cat("acceptprob:", acceptind / M, fill = TRUE)
  res <- data.frame(alpha = alphas, beta = betas, mu = mus)
  attr(res, "acceptprob") <- acceptind / M
  return(res)
}

proposealpha <- function(calpha, valpha, A){
# propose jump from random walk for alpha (shape), enforce support (0, A)
# Reference-style: if invalid, revert to current (like proposesig2 in the ref)
  z <- rnorm(1, 0, sqrt(valpha))
  alphastar <- calpha + z
  if(alphastar <= 0 || alphastar >= A) alphastar <- calpha
  return(alphastar)
}

proposebeta <- function(cbeta, vbeta){
# propose jump from random walk for beta (rate), enforce positivity
# Reference-style: if invalid, revert to current
  z <- rnorm(1, 0, sqrt(vbeta))
  betastar <- cbeta + z
  if(betastar <= 0) betastar <- cbeta
  return(betastar)
}
```

### Gibbs 

```{r appendix Gibbs, echo = T, eval = F}
gibbsforgamma <- function(dat, start, priorpars, B, M, valpha){
# Gibbs sampler for one-sample Gamma(shape = alpha, rate = beta) model
# with alpha ~ Uniform(0, A), beta ~ Gamma(gamma0, lambda0)
#
# dat        : vector of observed positive data (y_i > 0)
# start      : c(alpha0, beta0)  -- starting values
# priorpars  : c(gamma0, lambda0, A)
# B          : burn-in iterations
# M          : number of kept Monte Carlo draws
# valpha     : proposal variance for MH step on alpha (Metropolis-within-Gibbs)
#
# Notes on full conditionals and conjugacy:
# - Conditional for beta | alpha, y is Gamma(gamma0 + n*alpha, lambda0 + sum(y))
#   (shape/rate parametrization) -- this is conjugate, so we can sample beta directly.
# - Conditional for alpha | beta, y is NOT standard:
#       p(alpha | beta, y) proportional to [beta^(n*alpha) / Gamma(alpha)^n] *
#       (prod(y_i))^(alpha - 1) * I(0 < alpha < A)
#   We use a random-walk MH step for alpha inside the Gibbs loop
#   (Metropolis-within-Gibbs), mirroring the reference Gibbs code structure.
#
  calpha <- start[1]; cbeta <- start[2]
  gamma0 <- priorpars[1]; lambda0 <- priorpars[2]; A <- priorpars[3]

  alphas <- NULL; betas <- NULL; mus <- NULL
  cnt <- 0
  accept_alpha <- 0

  # Precompute sufficient statistics
  n <- length(dat)
  sumlogy <- sum(log(dat))
  sumy <- sum(dat)

  repeat{
    cnt <- cnt + 1

    # 1) Sample beta | alpha, y  (conjugate Gamma)
    #    shape = gamma0 + n*alpha ; rate = lambda0 + sum(y)
    newbeta <- rgamma(1, shape = gamma0 + n * calpha, rate = lambda0 + sumy)

    # 2) Sample alpha | beta, y  (Metropolis step within Gibbs)
    #    target log-density up to constant:
    #       log p(alpha | beta, y) = n*alpha*log(beta) - n*log(Gamma(alpha))
    #                                + (alpha - 1)*sum(log(y)), for 0<alpha<A
    astep <- sampalpha_mh(calpha, newbeta, valpha, sumlogy, n, A)
    newalpha <- astep$alpha
    accept_alpha <- accept_alpha + astep$acc

    if(cnt > B){
      alphas <- c(alphas, newalpha)
      betas  <- c(betas,  newbeta)
      mus    <- c(mus,    newalpha / newbeta)
    }
    calpha <- newalpha; cbeta <- newbeta

    if(cnt == (B + M)) break
  }

  cat("alpha_acceptprob (within Gibbs):", accept_alpha / M, fill = TRUE)
  res <- data.frame(alpha = alphas, beta = betas, mu = mus)
  return(res)
}

sampalpha_mh <- function(calpha, beta, valpha, sumlogy, n, A){
# One-step random-walk MH update for alpha (shape) given beta and y.
# Returns a list(alpha = ..., acc = 0/1)
#
# target log-density (up to constant in alpha):
#   log f(alpha | beta, y) = n*alpha*log(beta) - n*log(Gamma(alpha))
#                            + (alpha - 1)*sumlogy
# with support 0 < alpha < A; outside support, log-density = -Inf.
#
  z <- rnorm(1, 0, sqrt(valpha))
  alphastar <- calpha + z
  if(alphastar <= 0 || alphastar >= A){
    # As in reference style, invalid proposal -> revert (equivalent to reject)
    return(list(alpha = calpha, acc = 0))
  }

  # log target at current and proposed
  lfcur  <- n * calpha   * log(beta) - n * lgamma(calpha)   + 
    (calpha   - 1) * sumlogy
  lfstar <- n * alphastar * log(beta) - n * lgamma(alphastar) + 
    (alphastar - 1) * sumlogy

  a <- min(exp(lfstar - lfcur), 1)
  u <- runif(1, 0, 1)
  if(u <= a) return(list(alpha = alphastar, acc = 1))
  return(list(alpha = calpha, acc = 0))
}
```
