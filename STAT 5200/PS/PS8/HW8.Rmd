---
title: "Assignment 8 "
author: "Sam Olson"
output:
  pdf_document:
    toc: false
    number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem Description

Consider a problem of conducting a Bayesian analysis with a one-sample gamma model.

Assume that random variables $Y_1, \ldots, Y_n$ are independent and identically distributed with common probability density function (for $\alpha > 0$ and $\beta > 0$):

$$
f(y \mid \alpha, \beta) =
\frac{\beta^{\alpha}}{\Gamma(\alpha)} y^{\alpha - 1} \exp(-\beta y),
\quad y > 0.
$$

Suppose that we will assign a joint prior to $\alpha$ and $\beta$ in product form, with particular values of $A > 0$, $\gamma_0 > 0$, and $\lambda_0 > 0$:

$$
\pi_{\alpha}(\alpha) = \frac{1}{A} I(0 < \alpha < A), \quad
\pi_{\beta}(\beta) =
\frac{\lambda_0^{\gamma_0}}{\Gamma(\gamma_0)} \beta^{\gamma_0 - 1} e^{-\lambda_0 \beta},
\quad \beta > 0.
$$

Recall that in the analysis of an actual data set, $A, \gamma_0$ and $\lambda_0$ will be given specific numerical values. Since this is a simulated example and we have no actual prior information, use the following hyperparameters:

$$
A = 20, \quad \gamma_0 = 0.5, \quad \lambda_0 = 0.1.
$$

This gives prior expectation of 5.0 and prior variance of 50. The prior does focus probability on smaller values, but still has $Pr(\beta > 10) = 0.16$.

```{r, echo = F, warning = F, message = F}
gammaDat <- read.table("C:/Users/samue/OneDrive/Desktop/Iowa_State_PS/STAT 5200/PS/PS8/gammadat_bayes.txt", header = T)
source("C:/Users/samue/OneDrive/Desktop/Iowa_State_PS/STAT 5200/PS/PS8/sourceHW.R")
library(knitr)
library(kableExtra)
```

\newpage

## 1. 

Consider using a Metropolis–Hastings algorithm with independent random-walk proposals for $\alpha$ and $\beta$.

Suppose that our current values are ($\alpha_m, \beta_m$), and that the proposal ($\alpha^*, \beta^*$) has been generated from

$$
q(\alpha, \beta \mid \alpha_m, \beta_m)
$$

which is the product of independent random walks.

Identify the appropriate acceptance probability for the jump proposal ($\alpha^*, \beta^*$).

### Answer

I believe this question is being asked in the abstract, i.e., not for the specific dataset in question. Under that pretense: 

The target distribution for the Metropolis–Hastings algorithm is the joint posterior:

$$
\pi(\alpha, \beta \mid y) \propto L(y \mid \alpha, \beta)\,\pi_\alpha(\alpha)\,\pi_\beta(\beta)
$$

where $L$ denotes the likelihood (not log-transformed), and with the likelihood for independent $Y_i \sim \text{Gamma}(\alpha, \beta)$ given by: 

$$
L(y \mid \alpha, \beta)
= \prod_{i=1}^n \frac{\beta^\alpha}{\Gamma(\alpha)} y_i^{\alpha-1} e^{-\beta y_i}
$$

Suppose the proposal distribution is a product of independent symmetric random walks, such that: 

$$
q(\alpha^*, \beta^* \mid \alpha_m, \beta_m)
= q_\alpha(\alpha^* \mid \alpha_m) q_\beta(\beta^* \mid \beta_m)
$$

Because each random-walk is symmetric, the proposal satisfies: 

$$
q(\alpha^{*}, \beta^{*} | \alpha_m, \beta_m) = q_{\alpha}(\alpha^{*} | \alpha_m) q_{\beta}(\beta^{*} | \beta_m)
$$

And the ratio:

$$
\frac{q(\alpha_m,\beta_m \mid \alpha^*,\beta^*)}{q(\alpha^*,\beta^* \mid \alpha_m,\beta_m)} = 1
$$

Consequently, the Hastings term"cancels and the acceptance probability depends only on the posterior ratio, specifically:

$$
a\big((\alpha_m,\beta_m)\to(\alpha^*,\beta^*)\big)
=\min\left\{1,\,
\frac{\pi(\alpha^*,\beta^*\mid y)}{\pi(\alpha_m,\beta_m\mid y)}\right\}
=\min\left\{1,\,
\frac{L(y\mid \alpha^*,\beta^*)\,\pi_\alpha(\alpha^*)\,\pi_\beta(\beta^*)}
     {L(y\mid \alpha_m,\beta_m)\,\pi_\alpha(\alpha_m)\,\pi_\beta(\beta_m)}
\right\}
$$

Because $\pi_\alpha(\alpha)$ is uniform on $(0, A)$, this implies that any proposal with $\alpha^* \notin (0, A)$ or $\beta^* \le 0$ is automatically rejected ($a = 0$). 

Knowing the data model comes from a one-sample Gamma, we know it comes from the exponential dispersion family. This is used primarily to motivate knowing the form of the sufficient statistics, $S_1 = \sum_{i=1}^n \log y_i$ and $S_2 = \sum_{i=1}^n y_i$. With this, we can write: 

$$
\log r
= n\left[\alpha^* \log \beta^* - \log \Gamma(\alpha^*) - \alpha_m \log \beta_m + \log \Gamma(\alpha_m)\right]
+ (\alpha^* - \alpha_m) S_1 - (\beta^* - \beta_m) S_2
+ (\gamma_0 - 1)\left[\log \beta^* - \log \beta_m\right]
- \lambda_0(\beta^* - \beta_m)
$$

Such that we have the acceptance probability for the Metropolis–Hastings update of $(\alpha, \beta)$ (exact) as: 

$$
a = \min\left\{ 1,\; \exp(\log r) \right\}
$$

for a joint update of ($\alpha, \beta$) under independent symmetric random-walk proposals.

\newpage

## 2. 

On the course web page is a data set called `gammadat_bayes.txt`.

Program a Metropolis–Hastings algorithm and simulate 50,000 values from the joint posterior of $\alpha$, $\beta$, and $\mu = \alpha / \beta$.

Provide (with supporting evidence if appropriate):

* Information on how you selected a burn-in period. *NOTE: I do not expect you to compute Gelman-Rubin scale reduction factors for this assignment.*

* Information on how you tuned the algorithm for acceptance rate, including the random-walk variances and the final acceptance rate.

* Summaries of the marginal posterior distributions of $\alpha$ and $\beta$ and $\mu = \alpha / \beta$, including histograms and five-number summaries, 95% central credible intervals, and correlation between $\alpha$ and $\beta$ in the Markov chain.

### Answer

To start, while not affecting the priors (in distribution or parameter value), we do want to look at the data to get our "initial" values. 

Based on method of moments estimators, we have: 

$\alpha_0 = \frac{\bar{y}}{s_{y}^2}\,, \beta_0 = \frac{\alpha_0}{\bar{y}}$

Where $\bar{y}$ and $s_{y}^2$ are the sample mean and variance, respectively. 

For both $\alpha_0$ and $\beta_0$, we will "impose" that any estimates or updates retain $\alpha_0, \beta_0 > 0$. 

Additionally, given the hyperparameters to this problem, we also "impose" that $\alpha_0 < 20$. 

We run a sampling procedure without any tuning, first to determine a suitable Burn-In period. 

Note: Throughout the sampling procedure, we simulate $\alpha$ and $\beta$ simultaneously, which is important to keep later estimates of $\mu$ more comparable ("apples-to-apples"). Furthermore, the actual sampling is done using log likelihood, again, for the purpose of stability. However, we built-in a check for "invalid" parameter proposals (negative-valued); during the procedure, after identifying a "candidate", if the proposed parameter is negative-valued, it is "reverted" to the prior value. This ensures appropriateness of the method. 

We also consider a handful of starting values for $\alpha, \beta$ to ensure the trace plots achieve stability around the same number of iterations. 

Our sample values provide a basis for determining our Burn-In Period. We use our Metropolis-Hastings algorithm for a rather large number (50,000) without *any* Burn-In, and then look at both the trace plots and autocorrelation plots (ACFs). With that, we have: 

```{r, include = F, cache = T, eval = T, echo = F, warning = F, message = F}
set.seed(43)
y <- as.numeric(gammaDat$y)

m <- mean(y)
v <- var(y)

alpha0 <- if (v > 0) (m^2 / v) else 1.0
alpha0 <- max(0.10, min(alpha0, 19.9))
beta0  <- max(1e-6, alpha0 / max(m, 1e-6))
start1 <- c(alpha0, beta0)

valpha <- (0.20 * alpha0)^2
vbeta  <- (0.20 * beta0)^2
jumpvars1 <- c(valpha, vbeta)

j_diag <- metropforgamma(
  dat = y,
  start = start1,
  priorpars = c(gamma0 = 0.5, lambda0 = 0.1, A = 20),
  jumpvars = jumpvars1,
  B = 0,
  M = 50000
)

j_diag2 <- metropforgamma(
  dat = y,
  start = start1*1.5,
  priorpars = c(gamma0 = 0.5, lambda0 = 0.1, A = 20),
  jumpvars = jumpvars1,
  B = 0,
  M = 50000
)

j_diag3 <- metropforgamma(
  dat = y,
  start = start1*0.5,
  priorpars = c(gamma0 = 0.5, lambda0 = 0.1, A = 20),
  jumpvars = jumpvars1,
  B = 0,
  M = 50000
)
```

```{r, cache = T, eval = T, echo = F, warning = F, message = F}
runmean <- function(x) cumsum(x) / seq_along(x)

# Burn-In 
B_line <- 5000L   

par(mfrow = c(1,2))
plot(j_diag$alpha, type="l", main="alpha trace (B=0)", xlab="iter")
# lines(j_diag2$alpha, col = "grey50")
# lines(j_diag3$alpha, col = "steelblue4")
abline(v = B_line, col=2, lty=2, lwd=2)
abline(v = B_line*2, col=2, lty=2, lwd=2)
lines(runmean(j_diag$alpha), col = 4)
# lines(runmean(j_diag2$alpha), col = 5)
# lines(runmean(j_diag3$alpha), col = 6)

# plot(j_diag$alpha, type="l", col="#1B9E77",  # teal green
#      main="alpha trace (B=0)", xlab="iter",
#      ylim = range(c(j_diag$alpha, j_diag2$alpha, j_diag3$alpha)))
# lines(j_diag2$alpha, col="#D95F02")  # orange
# lines(j_diag3$alpha, col="#7570B3")  # purple
# 
# abline(v = B_line, col="#E7298A", lty=2, lwd=2)   # magenta for vertical markers
# abline(v = B_line*2, col="#E7298A", lty=2, lwd=2)
# 
# lines(runmean(j_diag$alpha), col="#005A46", lwd=2)   # darker teal
# lines(runmean(j_diag2$alpha), col="#A34900", lwd=2)  # darker orange
# lines(runmean(j_diag3$alpha), col="#4B3B8F", lwd=2)  # darker purple

# legend("topright",
#        legend = c("j_diag", "j_diag2", "j_diag3"),
#        col = c("#1B9E77", "#D95F02", "#7570B3"),
#        lty = 1, lwd = 2, bty = "n")
# 
plot(j_diag$beta,  type="l", main="beta trace (B=0)",  xlab="iter")
abline(v = B_line, col=2, lty=2, lwd=2)
abline(v = B_line*2, col=2, lty=2, lwd=2)
lines(runmean(j_diag$beta),  col = 4)
# lines(runmean(j_diag2$beta), col = 5)
# lines(runmean(j_diag3$beta), col = 6)

# plot(j_diag$beta, type = "l", col = "#1B9E77", 
#      main = "beta trace (B=0)", xlab = "iter",
#      ylim = range(c(j_diag$beta, j_diag2$beta, j_diag3$beta)))
# lines(j_diag2$beta, col = "#D95F02")
# lines(j_diag3$beta, col = "#7570B3")
# 
# abline(v = B_line,  col = "#E7298A", lty = 2, lwd = 2)
# abline(v = B_line*2, col = "#E7298A", lty = 2, lwd = 2)
# 
# lines(runmean(j_diag$beta),  col = "#005A46", lwd = 2)
# lines(runmean(j_diag2$beta), col = "#A34900", lwd = 2)
# lines(runmean(j_diag3$beta), col = "#4B3B8F", lwd = 2)
# 
# legend("topright",
#        legend = c("j_diag", "j_diag2", "j_diag3"),
#        col = c("#1B9E77", "#D95F02", "#7570B3"),
#        lty = 1, lwd = 2, bty = "n")
```

```{r, cache = T, eval = T, echo = F, warning = F, message = F}
par(mfrow = c(1,2))
acf(j_diag$alpha, lag.max = 10000, main="ACF alpha (full)")
acf(j_diag$beta,  lag.max = 10000, main="ACF beta (full)")
par(mfrow = c(1,1))
```

To that end, we use a "running mean" (blue) to get a sense of when variability in the parameter "stabilizes" in the Trace Plot graphs. We see that by iteration 5,000, $\alpha$ and $\beta$ samples tend to stabilize. Additionally, the ACFs decay rapidly (well before the 5,000); to stay on the conservative side, we double this and set that equal to our Burn-In period. 

Ultimately, we settle on a Burn-In period of 10,000. 

Note: Although the ACF plot displays values that remain slightly outside the “white-noise” bounds around the proposed Burn-In period, the autocorrelation clearly decays toward zero and alternates in sign, indicating that dependence weakens over time. Therefore, it is still reasonable to suppose the proposed Burn-In period is appropriate.

We then have some additional tuning. The general procedure is to "scale" the jump variance by some proportion of the original sample variance. To begin with, we settle on multipling the variance by some proportion, initially around 0.25 to 2, and then after iterating deciding on 0.5, 0.75, 1.0, 1.25, and 1.5. The acceptance probabilities for each of these scales are reported, using the Burn-In of 10,000. 

```{r, include = F, cache = T, eval = T, echo = F, warning = F, message = F}
scales <- c(0.5, 0.75, 1.0, 1.25, 1.5)
tune_tab <- do.call(rbind, lapply(scales, function(s){
  j <- metropforgamma(
    dat = y, start = start1,
    priorpars = c(0.5, 0.1, 20),
    jumpvars = jumpvars1 * s^2,
    B = 10000, M = 50000
  )
  data.frame(scale = s,
             valpha = (jumpvars1[1] * s^2),
             vbeta  = (jumpvars1[2] * s^2),
             accept = attr(j, "acceptprob"))
}))
```

```{r, cache = T, eval = T, echo = F, warning = F, message = F}
kable(round(tune_tab, 4))
```

Our goal in tuning is to reach a Metropolis-Hastings acceptance rate somewhere in the range of 20-60%, which is a heuristic noted in the Chapter 7 notes on Simulation. Ultimately, we decided on using a "scale" of 1.25, which given the seed and method noted, gives us an acceptance probability around 16.5%. Note: While this is on the lower end, I ultimately wanted to scale the jump variance somewhat higher than the sample to allow for greater exploration of a wider range of possible values for the "jumps".  

This then leads us to run the whole simulation procedure again, now using both the Burn-In period of 5,000 and the tuned parameters. 

```{r, include = F, cache = T, eval = T, echo = F, warning = F, message = F}
final <- metropforgamma(
  dat = y,
  start = start1,
  priorpars = c(0.5, 0.1, 20),
  # from tuning
  jumpvars = jumpvars1 * 1.25^2,   
  B = 10000,                       
  M = 50000
)
```

```{r, cache = T, eval = T, echo = F, warning = F, message = F}
acc_final <- attr(final, "acceptprob")
alpha <- final$alpha; beta <- final$beta; mu <- final$mu
fivenum_alpha <- fivenum(alpha);  ci_alpha <- quantile(alpha, c(0.025, 0.975))
fivenum_beta  <- fivenum(beta);   ci_beta  <- quantile(beta,  c(0.025, 0.975))
fivenum_mu    <- fivenum(mu);     ci_mu    <- quantile(mu,    c(0.025, 0.975))
corr_ab <- cor(alpha, beta, use = "complete.obs")

alphaMH <- alpha; betaMH <- beta; muMH <- mu
ci_alphaMH <- ci_alpha; ci_betaMH <- ci_beta; ci_muMH <- ci_mu

# Don't think I need this anymore
esc <- function(x) gsub("%", "\\\\%", x, fixed = TRUE)

one_method_tbl <- function(vec_list, method = NULL) {
  out <- do.call(
    rbind,
    lapply(vec_list, function(x) {
      fn <- fivenum(x)
      ci <- quantile(x, c(0.025, 0.975), names = FALSE)
      c(Min = fn[1], Q1 = fn[2], Median = fn[3], Q3 = fn[4], Max = fn[5],
        "CI 2.5" = ci[1], "CI 97.5" = ci[2])
    })
  )
  df <- data.frame(Parameter = names(vec_list), round(out, 4),
                   row.names = NULL)
  if (!is.null(method)) df <- cbind(Method = method, df)
  df
}

mh_tbl <- one_method_tbl(list(alpha = alphaMH, beta = betaMH, mu = muMH), method = "MH")

if (exists("alphaG") && exists("betaG") && exists("muG")) {
  gibbs_tbl <- one_method_tbl(list(alpha = alphaG, beta = betaG, mu = muG), method = "Gibbs")
  param_tbl <- rbind(mh_tbl, gibbs_tbl)
} else {
  param_tbl <- mh_tbl
}

kable(
  param_tbl, booktabs = TRUE, escape = TRUE,
  caption = esc("Posterior summaries: five-number statistics and 95 central credible intervals")
) |>
  kable_styling(full_width = FALSE, position = "center",
                latex_options = c("hold_position")) |>
  column_spec(1, bold = TRUE) |>
  add_header_above(c(" " = if ("Method" %in% names(param_tbl)) 2 else 1,
                     "Posterior Summary" = ncol(param_tbl) -
                       if ("Method" %in% names(param_tbl)) 2 else 1))


nm <- names(param_tbl)

ci_low  <- nm[grepl("^CI(\\.| )2\\.5",  nm)]
ci_high <- nm[grepl("^CI(\\.| )97\\.5", nm)]

cols_ci <- c(if ("Method" %in% nm) "Method", "Parameter", ci_low, ci_high)
ci_only <- param_tbl[, cols_ci, drop = FALSE]

names(ci_only)[names(ci_only) == ci_low]  <- "Lower 2.5"
names(ci_only)[names(ci_only) == ci_high] <- "Upper 97.5"

kable(
  ci_only, booktabs = TRUE, escape = TRUE,
  caption = esc("95 central credible intervals")
) |>
  kable_styling(full_width = FALSE, position = "center",
                latex_options = c("hold_position")) |>
  column_spec(1, bold = TRUE)

acc_vec <- as.numeric(acc_final)
acc_names <- names(acc_final)
if (is.null(acc_names)) {
  acc_names <- if (length(acc_vec) == 1) "Overall" else paste0("Component_", seq_along(acc_vec))
}
acc_tbl <- data.frame(Component = acc_names,
                      `Acceptance Rate` = paste0(round(100 * acc_vec, 1), "%"),
                      row.names = NULL)

kable(
  acc_tbl, booktabs = TRUE, escape = TRUE,
  caption = "Acceptance rates"
) |>
  kable_styling(full_width = FALSE, position = "center",
                latex_options = c("hold_position")) |>
  column_spec(1, bold = TRUE)

corr_mat <- matrix(c(1, corr_ab, corr_ab, 1), nrow = 2,
                   dimnames = list(c("alpha","beta"), c("alpha","beta")))
corr_df <- data.frame(Parameter = rownames(corr_mat), round(corr_mat, 4), row.names = NULL)

kable(
  corr_df, booktabs = TRUE, escape = TRUE,
  caption = "Correlation matrix for alpha and beta"
) |>
  kable_styling(full_width = FALSE, position = "center",
                latex_options = c("hold_position")) |>
  column_spec(1, bold = TRUE)
```

```{r, cache = T, eval = T, echo = F, warning = F, message = F}
par(mfrow = c(1,3))
hist(alphaMH, breaks="FD", main=expression(paste("Posterior of ", alpha)), xlab=expression(alpha)); abline(v=ci_alpha, lty=2)
hist(betaMH,  breaks="FD", main=expression(paste("Posterior of ", beta)),  xlab=expression(beta));  abline(v=ci_beta,  lty=2)
hist(muMH,    breaks="FD", main=expression(paste("Posterior of ", mu==alpha/beta)), xlab=expression(mu)); abline(v=ci_mu, lty=2)
par(mfrow = c(1,1))
```

\newpage

## 3.

Using both the 75th percentile and the range as data characteristics of potential interest, compute posterior predictive p-values from 10,000 posterior predictive datasets.

### Answer

We then do additional posterior predictive checks to validate the results of our simulation. Using the kept MH draws from the prior Question, we simulated 10,000 posterior predictive datasets. 

```{r, cache = T, eval = T, echo = F, warning = F, message = F}
set.seed(43)

Q1 <- function(z) as.numeric(quantile(z, 0.75))
Q2 <- function(z) diff(range(z))

# Q_obs1 <- Q1(y); Q_obs2 <- Q2(y)
# 
# S <- length(alpha)   
# n <- length(y)
# 
# yrep_stats <- matrix(NA_real_, nrow = 2, ncol = S)
# 
# for (s in seq_len(S)) {
#   yrep <- rgamma(n, shape = alpha[s], rate = beta[s])
#   yrep_stats[1, s] <- Q1(yrep)
#   yrep_stats[2, s] <- Q2(yrep)
# }

# Quartile obs
Q_obs1 <- Q1(y)
# Range obs
Q_obs2 <- Q2(y)

# 10,000 draws 
ndraws <- 10000L
n <- length(y)

set.seed(43)
# randomly take the 10,000
idx <- sample(seq_along(alpha), ndraws)

yrep_stats <- matrix(NA, nrow = 2, ncol = ndraws)

# for each of the 10,000, create datasets of size 50
for (s in seq_len(ndraws)) {
  yrep <- rgamma(n, shape = alpha[idx[s]], rate = beta[idx[s]])
  yrep_stats[1, s] <- Q1(yrep)  
  yrep_stats[2, s] <- Q2(yrep)  
}

# upper-tail PPP for 75th percentile
ppp_Q75   <- mean(yrep_stats[1, ] >= Q_obs1)  
# upper-tail PPP for range
ppp_range <- mean(yrep_stats[2, ] >= Q_obs2)  

ppp_tbl <- data.frame(
  Statistic = c("75th percentile", "Range"),
  Observed  = round(c(Q_obs1, Q_obs2), 4),
  PPP       = round(c(ppp_Q75, ppp_range), 4),
  check.names = FALSE
)

kable(
  ppp_tbl, booktabs = TRUE,
  caption = "Posterior predictive p-values (upper) 10,000 datasets"
) |>
  kable_styling(full_width = FALSE, position = "center",
                latex_options = c("hold_position")) |>
  column_spec(1, bold = TRUE)
```

Generally, we want posterior predictive p-values that are not too large and not too small (so somewhere in the range of 0.2 to 0.7, or so); the values we observe for the statistics of interest seem suitable. 

\newpage

## 4. 

Now consider the use of a Gibbs Sampling algorithm to simulate from the joint posterior of $\alpha$ and $\beta$ and $\mu$. Derive full conditional posterior densities for $\alpha$ and $\beta$ Using these distributions, program a Gibbs Sampling algorithm and simulate 50,000 values from the joint posterior. Provide (with supporting evidence if appropriate),

* information on how you selected a burn-in period. Again, there is no need to compute Gelman-Rubin scale reduction factors for this assignment.

* summaries of the marginal posterior distributions of $\alpha$ and $\beta$, including histograms and five-number summaries, 95% central credible intervals, and correlation between $\alpha$ and $\beta$ in the Markov chain.

### Answer

Let $Y_i \stackrel{\text{iid}}{\sim}\text{Gamma}(\alpha,\beta)$ with density

$$
f(y\mid \alpha,\beta)=\dfrac{\beta^{\alpha}}{\Gamma(\alpha)}\,y^{\alpha-1}e^{-\beta y}
$$ 

for $y>0$

Priors specified as $\alpha\sim\text{Uniform}(0,A)$ and $\beta\sim\text{Gamma}(\gamma_0,\lambda_0)$ using the rate parametrization.

Denote $S_1=\sum_{i=1}^n \log y_i$ and $S_2=\sum_{i=1}^n y_i$.

The joint posterior (up to a constant) is

$$
\pi(\alpha,\beta \mid y)
\propto
\beta^{n\alpha} e^{-\beta S_2}
\frac{e^{(\alpha-1)S_1}}{[\Gamma(\alpha)]^{n}}
\beta^{\gamma_0-1} e^{-\lambda_0\beta}
\mathbb{1}_{(0,A)}(\alpha)
$$

Collecting all terms in $\beta$ from the joint posterior gives the kernel for the conditional posterior:

$$
\beta^{n\alpha+\gamma_0-1}\exp\big(-(\lambda_0+S_2)\beta\big)
$$

so

$$
\beta\mid \alpha,y\ \sim\ \mathrm{Gamma}\left(\ \gamma_0+n\alpha,\ \lambda_0+S_2\ \right)\
$$

Collecting terms in $\alpha$ yields

$$
\pi(\alpha\mid \beta,y)\ \propto
\exp\Big(n\alpha\log\beta - n\log\Gamma(\alpha) + (\alpha-1)S_1\Big)\mathbf 1_{(0,A)}(\alpha)
$$

Where $S_1$ and $S_2$ are the sufficient statistics for the Gamma family, which simplifies the expression of the conditional posteriors.

Note: The conditional posterior for $\alpha$ does not belong to any standard family due to the presence of the $\log\Gamma(\alpha)$ term. Consequently, $\alpha$ must be updated using a random-walk Metropolis step within the Gibbs sampler (i.e., a Metropolis-within-Gibbs update), with proposals restricted to $(0, A)$.

With the full conditionals derived, we now proceed to simulation. As in Question 2, we assess an appropriate Burn-In period and tuning strategy. However, unlike in Question 2, since $\beta$ is conditionally conjugate, tuning is required only for the Metropolis step on $\alpha$.

Convergence diagnostics such as trace plots and autocorrelation functions are examined using zero Burn-In initially. The method primarily focuses on jointly simulating $(\alpha, \beta)$ and employs the log-likelihood for improved numerical stability, again, similar to Question 2.

```{r, include = F, cache = T, eval = T, echo = F, warning = F, message = F}
set.seed(43)

y <- as.numeric(gammaDat$y)
A <- 20
m <- mean(y); v <- var(y)

alpha0 <- max(0.10, min(ifelse(v > 0, m^2 / v, 1.0), A - 0.1))
beta0  <- max(1e-6, alpha0 / max(m, 1e-6))
start  <- c(alpha0, beta0)
priorpars <- c(gamma0 = 0.5, lambda0 = 0.1, A = A)

# Metropolis-within-Gibbs
# only alpha uses MH
valpha <- (0.15 * alpha0)^2

diag_all <- gibbsforgamma(
  dat = y, start = start, priorpars = priorpars,
  B = 0, M = 50000, valpha = valpha
)

diag_all2 <- gibbsforgamma(
  dat = y, start = start*1.5, priorpars = priorpars,
  B = 0, M = 50000, valpha = valpha
)

diag_all3 <- gibbsforgamma(
  dat = y, start = start*0.5, priorpars = priorpars,
  B = 0, M = 50000, valpha = valpha
)
```

```{r, cache = T, eval = T, echo = F, warning = F, message = F}
alpha_all <- diag_all$alpha
beta_all  <- diag_all$beta
mu_all    <- diag_all$mu

alpha_all2 <- diag_all2$alpha
beta_all2  <- diag_all2$beta
mu_all2    <- diag_all2$mu

alpha_all3 <- diag_all3$alpha
beta_all3  <- diag_all3$beta
mu_all3    <- diag_all3$mu

runmean <- function(x) cumsum(x) / seq_along(x)

# Burn-In
B_line <- 10000L

par(mfrow = c(1,2))
plot(alpha_all, type="l", main="alpha trace (B=0)", xlab="iter", ylab=expression(alpha))
abline(v=B_line, col=2, lty=2, lwd=2)
abline(v=B_line*2, col=2, lty=2, lwd=2)
lines(runmean(alpha_all), col=4)
# lines(runmean(alpha_all2), col=5)
# lines(runmean(alpha_all2), col=6)

plot(beta_all,  type="l", main="beta trace (B=0)",  xlab="iter", ylab=expression(beta))
abline(v=B_line, col=2, lty=2, lwd=2)
abline(v=B_line*2, col=2, lty=2, lwd=2)
lines(runmean(beta_all),  col=4)
# lines(runmean(beta_all2),  col=5)
# lines(runmean(beta_all3),  col=6)

par(mfrow = c(1,2))
acf(alpha_all, lag.max=20000, main="ACF alpha (full)")
acf(beta_all,  lag.max=20000, main="ACF beta (full)")
par(mfrow = c(1,1))
```

Similar to Question 2, we use a "running mean" (blue) to get a sense of when variability in the parameter "stabilizes". And again, similar (but different!) to Question 2, by iteration 10,000, the ACFs has decayed rapidly, and the trace plot has generally stabilized; to stay on the conservative side, we double this and set that equal to our Burn-In period, settling on a Burn-In period of 20,000. 

Note: Similar to Question 2, although the ACF plot displays values that remain slightly outside the “white-noise” bounds around the proposed Burn-In period, the autocorrelation clearly decays toward zero and alternates in sign, indicating that dependence weakens over time. Therefore, it is still reasonable to suppose the proposed Burn-In period is appropriate.

For tuning then, we again scale our $\alpha$ variance in increments of 0.25, considering the same range of scales, and targeting an acceptance probability around 20-60%. However, given this is using the Gibbs sampling method, I am cautious about not properly "mixing", so apriori I will prioritize lower acceptance proabilities (we will accept more sampling that don't make a "jump", where by comparisons the MH algorithm will actively discard unsuitable "jump" candidates). 

```{r, include = F, cache = T, eval = T, echo = F, warning = F, message = F}
# compute MH acceptance for alpha from the stored chain
alpha_accept_from_chain <- function(alpha_chain) {
  if (length(alpha_chain) < 2) return(NA_real_)
  mean(alpha_chain[-1] != alpha_chain[-length(alpha_chain)])
}

scales <- c(0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0)
tune_tab <- do.call(rbind, lapply(scales, function(s){
  j <- gibbsforgamma(
    dat = y, start = start, priorpars = priorpars,
    B = 20000, M = 50000,
    valpha = valpha * s^2
  )
  data.frame(
    scale   = s,
    valpha  = valpha * s^2,
    acc_alpha = if (!is.null(j$alpha_accept_overall))
                  j$alpha_accept_overall
                else
                  alpha_accept_from_chain(j$alpha)
  )
}))
```

```{r, cache = T, eval = T, echo = F, warning = F, message = F}
kable(round(tune_tab, 4))
```

Given the scaling of 2.0, with acceptance probability around 19.1%, we now have both a suitable Burn-In Period and tuned parameters for our "full simulation" via Gibbs. 

```{r, include = F, cache = T, eval = T, echo = F, warning = F, message = F}
B_line <- 20000L 

final <- gibbsforgamma(
  dat = y, start = start, priorpars = priorpars,
  B = B_line, M = 50000,
  # tuning parameter
  valpha = valpha * 2^2   
)
```

```{r, cache = T, eval = T, echo = F, warning = F, message = F}
acc_alpha_final <- if (!is.null(final$alpha_accept_overall)) final$alpha_accept_overall else NA_real_

alpha <- final$alpha
beta  <- final$beta
mu    <- final$mu 

fivenum_alpha <- fivenum(alpha)
fivenum_beta  <- fivenum(beta)
ci_alpha <- quantile(alpha, c(0.025, 0.975), names = FALSE)
ci_beta  <- quantile(beta,  c(0.025, 0.975), names = FALSE)

corr_ab <- cor(alpha, beta, use = "complete.obs")

esc <- function(x) gsub("%", "\\%", x, fixed = TRUE)

summ_tbl <- data.frame(
  Parameter = c("alpha","beta"),
  Min    = c(fivenum_alpha[1], fivenum_beta[1]),
  Q1     = c(fivenum_alpha[2], fivenum_beta[2]),
  Median = c(fivenum_alpha[3], fivenum_beta[3]),
  Q3     = c(fivenum_alpha[4], fivenum_beta[4]),
  Max    = c(fivenum_alpha[5], fivenum_beta[5]),
  `CI 2.5%`  = c(ci_alpha[1], ci_beta[1]),
  `CI 97.5%` = c(ci_alpha[2], ci_beta[2]),
  check.names = FALSE
)
summ_tbl[,-1] <- lapply(summ_tbl[,-1], function(z) round(as.numeric(z), 4))
names(summ_tbl) <- esc(names(summ_tbl))

kable(
  summ_tbl, booktabs = TRUE, escape = TRUE,
  caption = esc("Posterior summaries for alpha and beta: five-number statistics and 95 central credible intervals")
) |>
  kable_styling(full_width = FALSE, position = "center",
                latex_options = c("hold_position")) |>
  column_spec(1, bold = TRUE) |>
  add_header_above(c(" " = 1, "Posterior Summary" = ncol(summ_tbl) - 1))

corr_mat <- matrix(c(1, corr_ab, corr_ab, 1), nrow = 2,
                   dimnames = list(c("alpha","beta"), c("alpha","beta")))
corr_df <- data.frame(Parameter = rownames(corr_mat), round(corr_mat, 4), row.names = NULL)
names(corr_df) <- esc(names(corr_df))

kable(
  corr_df, booktabs = TRUE, escape = TRUE,
  caption = "Correlation matrix for alpha and beta (from Markov chain)"
) |>
  kable_styling(full_width = FALSE, position = "center",
                latex_options = c("hold_position")) |>
  column_spec(1, bold = TRUE)
```

```{r, eval = T, echo = F, warning = F, message = F}
alphaG <- alpha
ci_alphaG <- ci_alpha
betaG <- beta 
ci_betaG <- ci_beta
muG <- mu 
ci_muG <- ci_mu

par(mfrow = c(1,3))
hist(alphaG, breaks="FD", main=expression(paste("Posterior of ", alpha)), xlab=expression(alpha)); abline(v=ci_alpha, lty=2)
hist(betaG,  breaks="FD", main=expression(paste("Posterior of ", beta)),  xlab=expression(beta));  abline(v=ci_beta,  lty=2)
hist(muG,    breaks="FD", main=expression(paste("Posterior of ", mu==alpha/beta)), xlab=expression(mu)); abline(v=ci_mu, lty=2)
par(mfrow = c(1,1))
```

\newpage

## 5. 

Using both the 75th percentile and the range as data characteristics of potential interest, compute posterior predictive p-values from 10,000 simulated posterior predictive data sets.

### Answer

We then do additional posterior predictive checks to validate the results of our simulation. Using the Gibbs draws from the prior Question, we simulated 10,000 posterior predictive datasets. 

```{r, eval = T, echo = F, warning = F, message = F}
set.seed(43)

gout <- final
stopifnot(is.data.frame(gout), all(c("alpha","beta") %in% names(gout)))
stopifnot(exists("y"), is.numeric(y), all(y > 0))
n <- length(y)

# observed
stat_q75   <- function(x) as.numeric(quantile(x, 0.75, type = 7))
stat_range <- function(x) diff(range(x))
Tobs_q75   <- stat_q75(y)
Tobs_range <- stat_range(y)

# posterior draws
ndraws <- 10000L
K <- nrow(gout)
idx <- if (K >= ndraws) sample.int(K, ndraws) else sample.int(K, ndraws, replace = TRUE)
a_draws <- gout$alpha[idx]
b_draws <- gout$beta[idx]

# pposterior predictive datasets
Trep_q75   <- numeric(ndraws)
Trep_range <- numeric(ndraws)
for (k in seq_len(ndraws)) {
  yrep <- rgamma(n, shape = a_draws[k], rate = b_draws[k])
  Trep_q75[k]   <- stat_q75(yrep)
  Trep_range[k] <- stat_range(yrep)
}

ppp_q75   <- mean(Trep_q75   >= Tobs_q75)
ppp_range <- mean(Trep_range >= Tobs_range)

ppp_tbl <- data.frame(
  Statistic = c("75th percentile", "Range"),
  Observed  = round(c(Tobs_q75, Tobs_range), 4),
  PPP       = round(c(ppp_q75, ppp_range), 4),
  check.names = FALSE
)

kable(
  ppp_tbl, booktabs = TRUE,
  caption = "Posterior predictive p-values (upper) 10,000 datasets"
) |>
  kable_styling(full_width = FALSE, position = "center",
                latex_options = c("hold_position")) |>
  column_spec(1, bold = TRUE) |>
  add_header_above(c(" " = 1, "Posterior Predictive Check" = 2))
```

Generally, we want posterior predictive p-values that are not too large and not too small (so somewhere in the range of 0.2 to 0.7, or so); the values we observe for the statistics of interest seem suitable, though potentially a bit large. Further comparison and analysis is discussed in Question 6. 

\newpage

## 6. 

Compare your results from the use of Metropolis-Hastings and Gibbs Sampling.

### Answer

The two sampling approaches, Metropolis–Hastings (MH) and Gibbs (Metropolis-within-Gibbs), produce highly consistent (across methods and against the observed sample) posterior inferences for all parameters of interest, $\alpha, \beta$, and $\mu$. Both have nearly identical posterior locations and spreads for $\alpha$, $\beta$, and the derived parameter $\mu = \alpha/\beta$.

Specifically: For the MH method, the 95% central credible intervals were:

$$
\alpha \in [6.32,\ 13.72],\quad \beta \in [0.578,\ 1.294],\quad \mu \in [9.823,\ 11.788],
$$

While the Gibbs method had:

$$
\alpha \in [6.40,\ 13.41],\quad \beta \in [0.587,\ 1.259],\quad \mu \in [9.810,\ 11.792].
$$

The pairwise posterior correlation between $\alpha$ and $\beta$ is also very similar, and strongly positive, at approximately 0.97 for both methods. This is a rather unsurprising result, and while we did not take advantage of this correlation, we perhaps could adapt our sampling methods to be more efficient by incorporating this relationship. 

From a more descriptive perspective, the MH posterior histograms appear slightly more “jagged,” reflecting that its random-walk updates move both parameters jointly (possibly yielding higher autocorrelation). By comparison the Gibbs method, which draws $\beta$ directly from its conjugate full conditional and updates $\alpha$ by a Metropolis step, produces smoother histograms and traces.

Posterior predictive checks are also consistent across methods. Both yield one-sided posterior predictive p-values between approximately 0.4 and 0.6 for both the 75th percentile and range statistics, with:

$$
p_{\text{ppp}}^{(Q_{0.75})} \approx 0.43 \text{ or } 0.44, \quad p_{\text{ppp}}^{(\text{range})} \approx 0.57
$$

Since the ideal results are posterior predictive p-values around 0.4 to 0.6 (since these are not interpreted like traditional p-values), our results indicate that the simulations provide an adequate job at simulating these statistics of interest (statistics which are notably **not** the sufficient statistics). Another way to say this is: The simulated datasets from each posterior reproduce the empirical features of the data without obvious systematic bias.

Though unfortunately there are not contextual details to draw upon in concluding, there are a few remarks to summarize. To start, both algorithms appear to converge to very similar (if not the same) posterior distributions for $\alpha, \beta$, and $\mu$. In comparing the two methods, the MH method shows slightly rougher marginal behavior but comparable central estimates, while the Gibbs method achieves smoother mixing. In both cases, we do an adequate job of simulating the original data, such that, after identifying and applying an appropriate Burn-In and tuning, the two methods give nearly indistinguishable posterior summaries and predictive performance.

```{r, eval = T, echo = F, warning = F, message = F}
# alpha_range <- range(c(alphaMH, alphaG))
# beta_range  <- range(c(betaMH, betaG))
# mu_range    <- range(c(muMH, muG))
# 
# alpha_breaks <- hist(c(alphaMH, alphaG), breaks = "FD", plot = FALSE)$breaks
# beta_breaks  <- hist(c(betaMH, betaG),  breaks = "FD", plot = FALSE)$breaks
# mu_breaks    <- hist(c(muMH, muG),    breaks = "FD", plot = FALSE)$breaks
# 
# alpha_ylim <- range(
#   hist(alphaMH, breaks = alpha_breaks, plot = FALSE)$counts,
#   hist(alphaG,  breaks = alpha_breaks, plot = FALSE)$counts
# )
# 
# beta_ylim <- range(
#   hist(betaMH, breaks = beta_breaks, plot = FALSE)$counts,
#   hist(betaG,  breaks = beta_breaks, plot = FALSE)$counts
# )
# 
# mu_ylim <- range(
#   hist(muMH, breaks = mu_breaks, plot = FALSE)$counts,
#   hist(muG,  breaks = mu_breaks, plot = FALSE)$counts
# )
make_breaks <- function(x1, x2, width = 0.1) seq(from = floor(x1), to = ceiling(x2), by = width)

alpha_range <- range(c(alphaMH, alphaG), na.rm = TRUE)
beta_range  <- range(c(betaMH,  betaG),  na.rm = TRUE)
mu_range    <- range(c(muMH,    muG),    na.rm = TRUE)

alpha_breaks <- make_breaks(min(alpha_range), max(alpha_range), width = 0.2)
beta_breaks  <- make_breaks(min(beta_range),  max(beta_range),  width = 0.05)
mu_breaks    <- make_breaks(min(mu_range),    max(mu_range),    width = 0.1)

alpha_ylim <- c(0, max(
  hist(alphaMH, breaks = alpha_breaks, plot = FALSE)$counts,
  hist(alphaG,  breaks = alpha_breaks, plot = FALSE)$counts
))
beta_ylim <- c(0, max(
  hist(betaMH, breaks = beta_breaks, plot = FALSE)$counts,
  hist(betaG,  breaks = beta_breaks, plot = FALSE)$counts
))
mu_ylim <- c(0, max(
  hist(muMH, breaks = mu_breaks, plot = FALSE)$counts,
  hist(muG,  breaks = mu_breaks, plot = FALSE)$counts
))
```

```{r, eval = T, echo = F, warning = F, message = F}
par(mfrow = c(1,2))

hist(alphaMH, breaks = alpha_breaks, xlim = alpha_range, ylim = alpha_ylim,
     main = expression(paste("MH - Posterior of ", alpha)),
     xlab = expression(alpha))
abline(v = ci_alphaMH, lty = 2)

hist(alphaG, breaks = alpha_breaks, xlim = alpha_range, ylim = alpha_ylim,
     main = expression(paste("Gibbs - Posterior of ", alpha)),
     xlab = expression(alpha))
abline(v = ci_alphaG, lty = 2)
```

```{r, eval = T, echo = F, warning = F, message = F}
par(mfrow = c(1,2))

hist(betaMH, breaks = beta_breaks, xlim = beta_range, ylim = beta_ylim,
     main = expression(paste("MH - Posterior of ", beta)),
     xlab = expression(beta))
abline(v = ci_betaMH, lty = 2)

hist(betaG, breaks = beta_breaks, xlim = beta_range, ylim = beta_ylim,
     main = expression(paste("Gibbs - Posterior of ", beta)),
     xlab = expression(beta))
abline(v = ci_betaG, lty = 2)
```

```{r, eval = T, echo = F, warning = F, message = F}
par(mfrow = c(1,2))

hist(muMH, breaks = mu_breaks, xlim = mu_range, ylim = mu_ylim,
     main = expression(paste("MH - Posterior of ", mu == alpha/beta)),
     xlab = expression(mu))
abline(v = ci_muMH, lty = 2)

hist(muG, breaks = mu_breaks, xlim = mu_range, ylim = mu_ylim,
     main = expression(paste("Gibbs - Posterior of ", mu == alpha/beta)),
     xlab = expression(mu))
abline(v = ci_muG, lty = 2)
```

\newpage 

## 7.

On this particular assignment, attach your R code for functions you programmed to do the necessary computations as an APPENDIX – not part of the body of your answer.

### Answer 

R Code attached separately. Note: R code functions are shown in the attached, and then their respective call(s) are noted. 
