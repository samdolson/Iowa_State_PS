---
title: "Statistics 520: Assignment 5"
author: "Sam Olson"
output:
  pdf_document:
    keep_tex: true
header-includes:
  - \usepackage{amsmath,amssymb,mathtools,bm}
  - \usepackage[T1]{fontenc}
  - \usepackage{lmodern}
---

# Assignment 5

The objectives of this assignment are to (1) ensure that you have a grasp on using the tools of basic likelihood in a data analysis and (2) help you to continue to develop the precise use of notation in presenting descriptions of analyses. On the course web page is a file in the Data folder called `gammadat.txt`. This file contains two columns of values with a header having labels `group1` and `group2`. Each column should be considered to contain values corresponding to a set of independent and identical gamma random variables. That is, the two columns are values from two groups that we wish to compare using a two-sample model with gamma distributions. Consider the first column to contain values for Group 1 and the second column to contain values for Group 2.

A number of resources are available to you to help you complete this assignment. Chapter 5 of the course notes contains a summary of likelihood methods. In the Computing folder of the course web page is a file `newtraph.txt` that contains a generic Newton-Raphson algorithm that you may use for maximum likelihood estimation. There is also a file called `newtraphexplain.txt` that describes the inputs needed, the syntax, and the output. Alternatively you may choose to make use of the built-in R functions `optim` or `nlm`. Any of these options (or others you might know of if you prefer Matlab or something else) are fine as long as you know what you are doing and can produce the quantities needed to conduct the analysis.

Your answer should contain complete and consistent notation using no undefined symbols. You should always clearly explain what you computed and the formulas used. Your answer should not contain computer code or material from a “screen dump.” You will not be awarded any points for such material. If you want to report estimated values do so in the text, as a list, or construct a table.

Again, do not include copied computer function output. You will not get credit for anything presented in that way.

```{r, echo = F, warning = F, message = F}
source("C:/Users/samue/OneDrive/Desktop/Iowa_State_PS/STAT 5200/PS/PS5/newtraph.txt")
source("C:/Users/samue/OneDrive/Desktop/Iowa_State_PS/STAT 5200/PS/PS5/hw5source.R")
gammadat <- read.table("C:/Users/samue/OneDrive/Desktop/Iowa_State_PS/STAT 5200/PS/PS5/gammadat.txt", header = T)
dat <- gammadat
```

```{r, echo = F, warning = F, message = F}
# y1 <- dat$group1
# y2 <- dat$group2
# y_all <- c(y1, y2)
# 
# fit1 <- mle_gamma_sr(y1)
# fit2 <- mle_gamma_sr(y2)
# fitA <- mle_gamma_sr(y_all)
# t1 <- loglik_gamma_sr(y1, fit1$alpha, fit1$beta)
# t2 <- loglik_gamma_sr(y2, fit1$alpha, fit1$beta)
# 
# t1 + t2
# toto <- loglik_gamma_sr(y_all, fitA$alpha, fitA$beta)
# toto
# 
# toto1 <- (toto - (t1 + t2))
# toto2 <- 2 * toto1
# pchisq(toto2, df = 2)
```

\newpage 

## 1. 

Assume random variables $Y_{1,1}, \ldots, Y_{1,n_1}$ and $Y_{2,1}, \ldots, Y_{2,n_2}$ have been defined for the responses in this problem. These responses are strictly positive numbers, and an assumption of independence is reasonable. Formulate a two-sample model using gamma distributions. For one group, write the form of the log likelihood that will need to be computed to find estimates and other inferential quantities.

### Answer

Assuming the random variables defined as given, taking observed values that are strictly positive and iid. 

Each group is then modeled with a (potentially different) Gamma distribution parameterized by the ($\alpha, \beta$) parameters (shape and rate, repectively):

$$
Y_{g,i}\;\stackrel{\text{iid}}{\sim}\;\mathrm{Gamma}(\alpha_g,\beta_g), \qquad g\in\{1,2\},\quad \text{and } i=1,\dots,n_g
$$

Where, for our purposes $n_{1} = n_{2}$ (equal sample sizes between the two groups of Gamma-distributed random variables). 

With pdf of the form: 

$$
f(y\mid \alpha_g,\beta_g)=\frac{\beta_g^{\alpha_g}}{\Gamma(\alpha_g)}\,y^{\alpha_g-1}e^{-\beta_g y},\qquad y>0,\;\alpha_g>0,\;\beta_g>0
$$

Using the pdf, we then take the log to define a single group $g$'s log-likelihood function by: 

$$
\ell_g(\alpha_g,\beta_g)
= \sum_{i=1}^{n_g}\log f(y_{g,i}\mid \alpha_g,\beta_g)
= n_g\Big(\alpha_g\log\beta_g-\log\Gamma(\alpha_g)\Big)
+(\alpha_g-1)\sum_{i=1}^{n_g}\log y_{g,i}
-\beta_g\sum_{i=1}^{n_g} y_{g,i}
$$

Note: The log-likelihood function is defined by 2 sufficient statistics, of the form: 

$$
T_{g,1}=\sum_{i=1}^{n_g}\log y_{g,i}
\quad\text{and}\quad
T_{g,2}=\sum_{i=1}^{n_g} y_{g,i}
$$

So, for the score equations, computing MLEs (say, via Newton–Raphson or some other optimization methods) is done via: 

$$
\frac{\partial \ell_g}{\partial \alpha_g}
= n_g\log \beta_g - n_g\psi_{0}(\alpha_g) + T_{g,1}, 
\qquad
\frac{\partial \ell_g}{\partial \beta_g}
= \frac{n_g\alpha_g}{\beta_g} - T_{g,2}
$$

With the Hessian given by: 

$$
\frac{\partial^2 \ell_g}{\partial \alpha_g^2}
= -n_g \psi_{1}(\alpha_g),\qquad
\frac{\partial^2 \ell_g}{\partial \beta_g^2}
= -\frac{n_g\alpha_g}{\beta_g^2},\qquad
\frac{\partial^2 \ell_g}{\partial \alpha_g\,\partial \beta_g}
=\frac{n_g}{\beta_g}
$$

Giving the Hessian:

$$
\begin{bmatrix}
\displaystyle \frac{\partial^2 \ell_g}{\partial \alpha_g^2} &
\displaystyle \frac{\partial^2 \ell_g}{\partial \alpha_g \,\partial \beta_g} \\[1.2em]
\displaystyle \frac{\partial^2 \ell_g}{\partial \beta_g \,\partial \alpha_g} &
\displaystyle \frac{\partial^2 \ell_g}{\partial \beta_g^2}
\end{bmatrix}
=
\begin{bmatrix}
-\,n_g \psi_{1}(\alpha_g) & \tfrac{n_g}{\beta_g} \\[0.8em]
\tfrac{n_g}{\beta_g} & -\,\tfrac{n_g \alpha_g}{\beta_g^2}
\end{bmatrix}
$$

where $\psi_{0}(\cdot)$ and $\psi_{1}(\cdot)$ are the digamma and trigamma functions, respectively (following notation convention seen on Wikipedia).

Then, taking the individual group log-likelihoods, we calculate the full two-sample log-likelihood as the sum (again, noting iid assumption between and within groups):

$$
\begin{aligned}
\ell(\alpha_1,\beta_1,\alpha_2,\beta_2)
&= \ell_1(\alpha_1,\beta_1)+\ell_2(\alpha_2,\beta_2) \\ 
&= \sum_{g=1}^{2}\sum_{i=1}^{n_g}\log f(y_{g,i}\mid \alpha_g,\beta_g) \\ 
&= n_1\Big(\alpha_1\log\beta_1-\log\Gamma(\alpha_1)\Big)
+(\alpha_1-1)\sum_{i=1}^{n_1}\log y_{1,i}
-\beta_1\sum_{i=1}^{n_1} y_{1,i} \\ 
&+ n_2\Big(\alpha_2\log\beta_2-\log\Gamma(\alpha_2)\Big)
+(\alpha_2-1)\sum_{j=1}^{n_2}\log y_{2,j}
-\beta_2\sum_{j=1}^{n_2} y_{2,j} \\
\end{aligned}
$$

\newpage

## 2. 

Find maximum likelihood estimates and 95% Wald theory intervals for the parameters of each group. Recall that, in the data file, the first column of values is Group 1 and the second column of values is Group 2.

### Answer

```{r, echo = F, warning = F, message = F}
res1 <- gamma_mle_wald(dat$group1)
res2 <- gamma_mle_wald(dat$group2)

summary_tbl <- rbind(
  to_row("group1", res1),
  to_row("group2", res2)
)
# summary_tbl
```

As noted in part 1)., the log-likelihood is of the form:  

$$
\ell_g(\alpha_g,\beta_g)
= n_g\big(\alpha_g\log\beta_g-\log\Gamma(\alpha_g)\big)
 +(\alpha_g-1)\sum_{i=1}^{n_g}\log Y_{g,i}
 -\beta_g\sum_{i=1}^{n_g}Y_{g,i} \quad \text{where } g \in \{1, 2\}
$$

Setting the score functions to zero gives the standard MLE system of equations. 

The Fisher Information (negative of the Hessian of the log-likelihood evaluated at the MLE, $(\alpha_g,\beta_g)$) then is: 

$$
I_g(\alpha_g,\beta_g)=
\begin{pmatrix}
n_g\,\psi_{1}(\alpha_g) & -\,n_g/\beta_g\\[4pt]
-\,n_g/\beta_g & n_g\,\alpha_g/\beta_g^2
\end{pmatrix}
$$

Taking these quantities, the Wald covariance is then given by: 

$$
\widehat{\mathrm{Var}}\!\begin{pmatrix}\hat\alpha_g\\ \hat\beta_g\end{pmatrix}
= I_g(\hat\alpha_g,\hat\beta_g)^{-1}
$$

After numeric approximation, taking square roots where appropriate (square root of the variance is SE), and evaluating the typical expression for confidence intervals, we have (with each group having $n_1=n_2=50$ samples): 

| Group | $\hat\alpha$ | SE($\hat\alpha$) | 95% CI for $\alpha$ | $\hat\beta$ | SE($\hat\beta$) | 95% CI for $\beta$ |
| ----- | -----------: | ---------------: | :------------------ | ----------: | --------------: | :----------------- |
| 1     |        3.497 |            0.669 | (2.186, 4.808)      |       1.519 |           0.312 | (0.907, 2.131)     |
| 2     |        1.626 |            0.298 | (1.042, 2.210)      |       0.726 |           0.155 | (0.421, 1.031)     |

Note, to be explicit about the formula for confidence intervals: 

$$
\exp \left( \log \hat\alpha \;\pm\; z_{1-\gamma/2}\,\mathrm{SE}(\log \hat\alpha) \right) \quad \text{ and } \exp \left( \log \hat\beta \;\pm\; z_{1-\gamma/2}\,\mathrm{SE}(\log \hat\beta) \right)
$$

Where $\gamma = 0.05, 1- \frac{\gamma}{2} = 0.975$

And to be explicit about the optimization method used: Using R's `optim` function for maximization (minimize negative log-likelihood), which is a "quasi-Newton method", i.e., using the original log-likelihood, first derivative, and second derivative (also using `method = ‘BFGS’`). 

\newpage 

## 3. 

Using a likelihood ratio test, determine whether you would reject a model having a common gamma distribution for both groups in favor of a model having separate gamma distributions for each of the two groups. Produce a plot of the estimated densities for each group (both densities on the same plot).

### Answer

Define the (nested) hypotheses by: 

  - $H_0:$ $\alpha_1=\alpha_2$ and $\beta_1=\beta_2$ (one common Gamma for both groups, only 2 unique parameters between groups).
  - $H_1:$ $(\alpha_1 \neq \alpha_2)$ and $(\beta_1 \neq \beta_2)$ (two separate Gammas, 4 unique parameters between groups).

Let $\hat\theta_0=(\hat\alpha_0,\hat\beta_0)$ be the MLE under $H_0$ (fitted using pooled data), and $\hat\theta_1=((\hat\alpha_1,\hat\beta_1),(\hat\alpha_2,\hat\beta_2))$ the MLEs fitted to each group separately. 

The LRT statistic is of the form: 

$$
\Lambda = - 2\bigl\{\ell(\hat\theta_0) - \ell(\hat\theta_1)\bigr\}
\;\xrightarrow{d}\; \chi^2_{\,2}
$$

With degrees of freedom 2 from (full - reduced = 4 - 2), i.e., $H_1$ has two more "free" parameters than $H_0$.

Using the same optimization method using in part 2., we calculate: 

  - Separate-group MLEs: $\hat\alpha_1=3.497,\;\hat\beta_1=1.519$, and $\hat\alpha_2=1.626,\;\hat\beta_2=0.726$
  
  - Common (pooled) MLEs: $\hat\alpha_0=2.202,\;\hat\beta_0=0.970$

Where:

$$
\ell(\hat\theta_1)=-163.447 (= \ell_1(\alpha_1,\beta_1)+\ell_2(\alpha_2,\beta_2) = -76.22814 + -87.21866),\qquad \ell(\hat\theta_0)=-167.526
$$

Using the log-likelihood values above, the LRT statistic is:

$$
\Lambda = - 2 (- 167.526 - (-163.447)) = 8.158
$$

<!-- # ```{r, echo = F, message = F, warning = F} -->
<!-- # T <- 8.157 -->
<!-- # pchisq(T, df = 2, lower.tail = FALSE) -->
<!-- # ``` -->

With corresponding p-value (with reference distribution $\chi^2_2$: 

$$
p\text{-value} = 0.01693 
$$

Reducing the question to an "Accept"/"Reject" framework, we Reject $H_0$ at the $\alpha = 0.05$ level (or make a "strength of evidence" argument to say we have strong evidence in favor of rejecting the null hypothesis). Interpreting this, we'd say that modeling the two groups as separate (differently parametrized) Gamma distributions seems a better fit than pooling them together as a single Gamma distribution (with shared shape and rate parameters). 

And a graph! 

```{r, echo = F, warning = F, message = F}
y1 <- dat$group1
y2 <- dat$group2
y_all <- c(y1, y2)

# Separate MLEs
fit1 <- mle_gamma_sr(y1)
fit2 <- mle_gamma_sr(y2)
ll_alt <- loglik_gamma_sr(y1, fit1$alpha, fit1$beta) +
          loglik_gamma_sr(y2, fit2$alpha, fit2$beta)

# Common (pooled) MLEs
fit0 <- mle_gamma_sr(y_all)
ll_null <- loglik_gamma_sr(y_all, fit0$alpha, fit0$beta)

# LRT statistic, df = 2
LR <- - 2 * (ll_null - ll_alt)
pval <- pchisq(LR, df = 2, lower.tail = FALSE)
# cat(sprintf("LRT = %.3f, df=2, p-value = %.4f\n", LR, pval))

# Plot fitted densities with distinct colors
rng <- range(y_all)
x <- seq(0, max(rng[2], quantile(y_all, 0.999)), length.out = 400)
plot(x, dgamma(x, shape = fit1$alpha, rate = fit1$beta),
     type = "l", col = "blue", lwd = 2,
     xlab = "y", ylab = "Density", main = "MLE Gamma Densities by Group")
lines(x, dgamma(x, shape = fit2$alpha, rate = fit2$beta),
      col = "red", lwd = 2)
legend("topright", legend = c("Group 1", "Group 2"),
       col = c("blue","red"), lty = 1, lwd = 2, bty = "n")

```

\newpage 

## 4. 

Find maximum likelihood estimates and 95% Wald theory intervals for the expected value of each group. Also produce a 95% interval for the difference in expected values (Group 1 minus Group 2).

### Answer

For a Gamma distributed random variable Y, whose distribution is parametrized by the shape and rate parameters, the expected value is of the form:

$$
E[Y] = \frac{\alpha}{\beta}
$$

For our case, thus the expected values for each group are given by: 

$$
\mu_g = \frac{\alpha_g}{\beta_g},\qquad g=1,2
$$

Let $(\hat\alpha_g,\hat\beta_g)$ be the MLEs, then: 

$$
\hat{\mu_g} = \frac{\hat{\alpha_g}}{\hat{\beta_g}},\qquad g=1,2
$$

Note then that the covariance matrix of the MLEs (the inverse observed information) is given by: 

$$
\widehat\Sigma_g = \begin{bmatrix}\widehat{\mathrm{Var}}(\hat\alpha_g) & \widehat{\mathrm{Cov}}(\hat\alpha_g,\hat\beta_g)\\ \widehat{\mathrm{Cov}}(\hat\alpha_g,\hat\beta_g) & \widehat{\mathrm{Var}}(\hat\beta_g)\end{bmatrix}
$$

Then, using the delta method with gradient given by:

$$
\begin{aligned}
\nabla\mu_g(\alpha,\beta) 
&= (\frac{\partial}{\partial \alpha} (\hat{\mu_g}), \frac{\partial}{\partial \beta} (\hat{\mu_g})) \\ 
&= (\frac{\partial}{\partial \alpha} (\frac{\hat{\alpha_g}}{\hat{\beta_g}}), \frac{\partial}{\partial \beta} (\frac{\hat{\alpha_g}}{\hat{\beta_g}})) \\ 
&= \left(\tfrac{1}{\beta},\; -\tfrac{\alpha}{\beta^2}\right) \\ 
\end{aligned}
$$

Then

$$
\widehat{\mathrm{Var}}(\hat\mu_g)
= \nabla\mu_g^\top \,\widehat\Sigma_g \,\nabla\mu_g
$$

Taking the SE then is just taking the root of the above quantity. 

Note: When calculating the quantities of interest, we use the sample (observed) variances of $\hat{\alpha}$ and $\hat{\beta}$. 

Then, we may construct a 95% Wald interval for $\mu_g$ by: 

$$
\hat\mu_g \pm z_{1 - \frac{\gamma}{2}} \mathrm{SE}(\hat\mu_g)
= \hat\mu_g \pm 1.96\,\mathrm{SE}(\hat\mu_g)
$$

(Using 1.96 based on the Standard Normal distribution, where $\gamma = 0.05$, as Wald Theory uses asymptotic results.)

\newpage 

For the difference $\mu_1-\mu_2$, treat groups as independent, i.e., $\mathrm{Cov}(\hat{\mu_1}, \hat{\mu_2}) = 0$, so

$$
\begin{aligned}
\widehat{\mathrm{Var}}(\hat\mu_1-\hat\mu_2)
&= \widehat{\mathrm{Var}}(\hat\mu_1)+\widehat{\mathrm{Var}}(\hat\mu_2) + 2\widehat{\mathrm{Cov}}(\hat{\mu_1}, \hat{\mu_2})\\
&= \widehat{\mathrm{Var}}(\hat\mu_1)+\widehat{\mathrm{Var}}(\hat\mu_2) \\ 
\end{aligned}
$$

<!-- * Group 1 MLEs: $\hat\alpha_1=3.497,\;\hat\beta_1=1.519$. -->
<!--   Expected value: $\hat\mu_1 = 2.303.$ -->
<!--   SE($\hat\mu_1$) = 0.278. -->
<!--   95% Wald CI: (1.758, 2.848). -->

<!-- * Group 2 MLEs: $\hat\alpha_2=1.626,\;\hat\beta_2=0.726$. -->
<!--   Expected value: $\hat\mu_2 = 2.240.$ -->
<!--   SE($\hat\mu_2$) = 0.284. -->
<!--   95% Wald CI: (1.683, 2.797). -->

<!-- * Difference: $\hat\mu_1-\hat\mu_2=0.063.$ -->
<!--   SE(diff) = $\sqrt{0.278^2+0.284^2}=0.397.$ -->
<!--   95% Wald CI: $(-0.715,\;0.841).$ -->

<!-- * The expected values of the two groups are both around 2.3. -->
<!-- * Wald 95% CIs for each mean overlap heavily. -->
<!-- * The difference $\mu_1-\mu_2$ is small relative to its SE; the 95% CI includes 0 widely. -->
<!-- * Conclusion: there is no evidence of a meaningful difference in the **expected values** between groups, even though the likelihood ratio test (Q3) showed their **distributions** differ in shape and rate. -->

Taken together, the quantities of interest we estimate are: 

```{r, echo = F, warning = F, message = F}
library(knitr)
library(kableExtra)

fit1 <- fit_gamma_sr(dat$group1)
fit2 <- fit_gamma_sr(dat$group2)

m1 <- mean_from_fit(fit1)
m2 <- mean_from_fit(fit2)

## Difference (independent groups): Var(diff) = Var(m1) + Var(m2)
diff_hat <- m1$mu - m2$mu
se_diff  <- sqrt(m1$se^2 + m2$se^2)
z <- qnorm(0.975)
ci_diff  <- c(diff_hat - z*se_diff, diff_hat + z*se_diff)

summary_q4 <- data.frame(
  group = c("group1", "group2", "difference (1 - 2)"),
  mu_hat = c(m1$mu, m2$mu, diff_hat),
  se     = c(m1$se,  m2$se,  se_diff),
  ci_L   = c(m1$ci[1], m2$ci[1], ci_diff[1]),
  ci_U   = c(m1$ci[2], m2$ci[2], ci_diff[2])
)

summary_q4_out <- summary_q4
num_cols <- sapply(summary_q4_out, is.numeric)
summary_q4_out[num_cols] <- lapply(summary_q4_out[num_cols], function(x) round(x, 3))

# print(summary_q4_out)
# If using knitr:
num_cols <- sapply(summary_q4, is.numeric)
summary_q4[num_cols] <- lapply(summary_q4[num_cols], round, 3)
knitr::kable(summary_q4, caption = "Gamma means and 95% Wald intervals")
# knitr::kable(summary_q4, caption = "Gamma means and 95% Wald intervals")
```

\newpage

## 5. 

Test whether the two groups should be considered significantly different using a two-sample $t$-test. (Take square roots if you think it makes the data look more symmetric for each group, though this is optional.) Does your result agree with the likelihood ratio test? Does it agree with the interval for difference in expected values?

### Answer

The observed group sample means are given by:

  - Group 1: $\bar y_1 \approx 2.29$ (2.30 expected),

  - Group 2: $\bar y_2 \approx 2.24$ (2.24 expected).

```{r, echo = F, message = F, warning = F}
# y1 <- dat$group1
# y2 <- dat$group2
# 
# n1 <- sum(!is.na(y1)); n2 <- sum(!is.na(y2))
# s1 <- sd(y1, na.rm = TRUE); s2 <- sd(y2, na.rm = TRUE)
# 
# # 1) Pooled SD (for equal-variance t-test)
# s_pooled <- sqrt(((n1 - 1)*s1^2 + (n2 - 1)*s2^2) / (n1 + n2 - 2))
# 
# # 2) Overall SD after concatenating
# s_overall <- sd(c(y1, y2), na.rm = TRUE)
# 
# c(pooled_sd = s_pooled, overall_sd = s_overall)
```

Also, the observed sample standard deviation is given by. 

  - Group 1: $s_1 \approx 1.31$
  
  - Group 2: $s_2 \approx 1.75$
  
  - Pooled: $s_0 \approx 1.55$

And we have equal number of samples per group, $n_1=n_2=50$, which taken together could justify use of the two-sample t-test. 

The hypotheses we may then test are o the form: 

$$
H_0:\; \mu_1=\mu_2 \quad\text{vs.}\quad H_A:\; \mu_1\ne \mu_2
$$

Since there are differences in variance between Group 1 and Group 2, I used the Welch two-sample $t$-test (which does not assume equal variances between groups). Notably, while I cannot emphatically justify using Welch, I prefer to use it in this instance because it reduces to the (“pooled”) two-sample $t$-test when the sample variances are close (which I think they are, though still distinct between groups). 

Additionally: While the formulas below are for the Welch two-sample $t$-test, I also calculate and provide formulas for the (non-Welch, standard) two-sample $t$-test. 

That being said, the relevant formulas for the Welch are of the form: 

$$
t_{\nu} = \frac{\bar y_1-\bar y_2}{\sqrt{s_1^2/n_1 + s_2^2/n_2}} \approx 0.15
$$

Where the degrees of freedom are calculated using the Welch-Satterthwaite approximation: 

$$
\nu \;=\; 
\frac{\left(\dfrac{s_1^2}{n_1} + \dfrac{s_2^2}{n_2}\right)^{2}}
{\dfrac{\left(\dfrac{s_1^2}{n_1}\right)^{2}}{n_1 - 1} \;+\; \dfrac{\left(\dfrac{s_2^2}{n_2}\right)^{2}}{n_2 - 1}}
$$

```{r, echo = F, message = F, warning = F}
# Suppose you've already computed the pooled sd:
# s_pooled <- s_pooled
# y1 <- dat$group1
# y2 <- dat$group2
# n1 <- length(y1); n2 <- length(y2)
# 
# # Compute t statistic (equal-variance t test, two-sample)
# t_stat <- (mean(y1) - mean(y2)) / (s_pooled * sqrt(1/n1 + 1/n2))
# 
# # Degrees of freedom
# df <- n1 + n2 - 2
# 
# # Two-sided p-value
# p_val <- 2 * pt(-abs(t_stat), df = df)
# 
# c(t_statistic = t_stat, df = df, p_value = p_val)
```

```{r, echo = F, message = F, warning = F}
# y1 <- sqrt(dat$group1)
# y2 <- sqrt(dat$group2)
# 
# n1 <- sum(!is.na(y1)); n2 <- sum(!is.na(y2))
# s1 <- sd(y1, na.rm = TRUE); s2 <- sd(y2, na.rm = TRUE)
# 
# # 1) Pooled SD (for equal-variance t-test)
# s_pooled <- sqrt(((n1 - 1)*s1^2 + (n2 - 1)*s2^2) / (n1 + n2 - 2))
# 
# # 2) Overall SD after concatenating
# s_overall <- sd(c(y1, y2), na.rm = TRUE)
# 
# c(pooled_sd = s_pooled, overall_sd = s_overall)
# 
# # Suppose you've already computed the pooled sd:
# s_pooled <- s_pooled
# # y1 <- dat$group1
# # y2 <- dat$group2
# n1 <- length(y1); n2 <- length(y2)
# 
# # Compute t statistic (equal-variance t test, two-sample)
# t_stat <- (mean(y1) - mean(y2)) / (s_pooled * sqrt(1/n1 + 1/n2))
# 
# # Degrees of freedom
# df <- n1 + n2 - 2
# 
# # Two-sided p-value
# p_val <- 2 * pt(-abs(t_stat), df = df)
# 
# c(t_statistic = t_stat, df = df, p_value = p_val)
```

The corresponding p-value is then: $p\approx 0.84$ (using Welch, and with no transformation).

If we repeat this process *after* doing a `sqrt` transformation on the observed samples, the result is very similar: The test statistic is $t_{98} \approx 0.78$ with corresponding $p \approx 0.44$ (again, using Welch). So, reducing to a decision "Accept"/"Reject" or even a "strength of evidence" argument, the interpretation remains largely the same. 

These results may (at first) seem opposed to the Likelihood ratio test from part 3), but it isn't. Importantly, these two methods (LRT test vs. the two-sample t-test) correspond to different tests. The two-sample $t$-test does not provide evidence of a significant difference in means between the two groups. This is also consistent with the Wald interval for the difference in means $\mu_1-\mu_2$, containing 0. But LRT is not a direct comparison of means; instead, the LRT tests whether the two *distributions* (particularly, the parameters and distributional form, so encompassing distributional shape, spread, and other aspects of the distributional form) differ significantly between groups.

Also, the results of the two-sample test are consistent when we use the normal "pooled" two-sample t-test, and also when we transform the original data using the `sqrt` transformation. All the relevant statistics and summary information of the calculations considered are in the following table:  

```{r, echo = F, warning = F, message = F}
y1 <- dat$group1
y2 <- dat$group2

# Run tests
res_raw_welch   <- summarize_ttest(y1, y2, "Raw, Welch", var_equal = FALSE)
res_sqrt_welch  <- summarize_ttest(sqrt(y1), sqrt(y2), "Sqrt, Welch", var_equal = FALSE)
res_raw_eqvar   <- summarize_ttest(y1, y2, "Raw, pooled", var_equal = TRUE)
res_sqrt_eqvar  <- summarize_ttest(sqrt(y1), sqrt(y2), "Sqrt, pooled", var_equal = TRUE)

# Combine and round numeric columns for display
results <- rbind(res_raw_welch, res_sqrt_welch, res_raw_eqvar, res_sqrt_eqvar)
num_cols <- sapply(results, is.numeric)
results_out <- results
results_out[num_cols] <- lapply(results_out[num_cols], function(x) round(x, 4))

# print(results_out)

## Optional (if using knitr):
num_cols <- sapply(results_out, is.numeric)
results_out[num_cols] <- lapply(results_out[num_cols], round, 3)

results_out <- subset(results,
                      select = -c(equal_var, n1, n2, method))

# Round numeric columns
num_cols <- sapply(results_out, is.numeric)
results_out[num_cols] <- lapply(results_out[num_cols], function(x) round(x, 3))

knitr::kable(results_out, caption = "Table of Two-Sample Test Statistics")
```

Misc: There are two notes to the above. First is, the Welch seemed most appropriate given differences in variance (and differences in shape-rate parameters), hence why it was formally noted and used. However, since the two-sample t-test was also calculated, the formulas used are as follows: 

$$
s_p^2 \;=\; \frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{\,n_1+n_2-2\,} 
$$

$$
t_{df} \;=\; \frac{\bar{y}_1 - \bar{y}_2}{s_p \sqrt{\tfrac{1}{n_1} + \tfrac{1}{n_2}}},
\qquad \text{df} = n_1+n_2-2
$$

Where $\text{df}, s_p^2,  \text{ and } t_P$ as given in the above table.  

Also, since I included the `sqrt` evaluation as well in the table, below is visual evidence that the distributions appear more symmetric after `sqrt` transformation to the data (which also gives further visual evidence of the spread/variance being different between the two groups).

```{r, echo = F, warning = F, message = F}
y1 <- sqrt(dat$group1)
y2 <- sqrt(dat$group2)
y_all <- c(y1, y2)

# Separate MLEs
fit1 <- mle_gamma_sr(y1)
fit2 <- mle_gamma_sr(y2)
ll_alt <- loglik_gamma_sr(y1, fit1$alpha, fit1$beta) +
          loglik_gamma_sr(y2, fit2$alpha, fit2$beta)

# Common (pooled) MLEs
fit0 <- mle_gamma_sr(y_all)
ll_null <- loglik_gamma_sr(y_all, fit0$alpha, fit0$beta)

# LRT statistic, df = 2
LR <- 2 * (ll_alt - ll_null)
pval <- pchisq(LR, df = 2, lower.tail = FALSE)
# cat(sprintf("LRT = %.3f, df=2, p-value = %.4f\n", LR, pval))

# Plot fitted densities with distinct colors
rng <- range(y_all)
x <- seq(0, max(rng[2], quantile(y_all, 0.999)), length.out = 400)
plot(x, dgamma(x, shape = fit1$alpha, rate = fit1$beta),
     type = "l", col = "blue", lwd = 2,
     xlab = "y", ylab = "Density", main = "Sqrt -- MLE Gamma Densities by Group")
lines(x, dgamma(x, shape = fit2$alpha, rate = fit2$beta),
      col = "red", lwd = 2)
legend("topright", legend = c("Group 1", "Group 2"),
       col = c("blue","red"), lty = 1, lwd = 2, bty = "n")
```

\newpage 

## 6.

Find maximum likelihood estimates and 95% Wald theory intervals for the mode of each group. Also produce a 95% interval for the difference in modes (Group 1 minus Group 2).

### Answer

For Y a Gamma-distributed random variable, parametrized in shape–rate form, the mode is of the form: 

$$
\text{mode} =
\begin{cases}
\dfrac{\alpha-1}{\beta}, & \alpha>1,\\[6pt]
0, & 0 < \alpha\le 1\ 
\end{cases}
$$

As the MLEs of $\hat{\alpha_1}, \hat{\alpha_2} > 1$, we won't worry about the second condition of the mode, i.e., where $0 < \alpha \leq 1$. 

As defined previously, $(\hat\alpha_g,\hat\beta_g)$ are the MLEs for group $g$ with covariance given by $\widehat\Sigma_g$. 

For $\hat\alpha_g>1$, we use the delta method with the expressions: 

$$
m_g(\alpha,\beta)=\frac{\alpha-1}{\beta},\qquad 
\nabla m_g(\alpha,\beta)=\Big(\tfrac{1}{\beta},\ -\tfrac{\alpha-1}{\beta^2}\Big)
$$

Again, mirroring the formulae used in part 4). 

We then have the quantities: 

$$
\widehat{\mathrm{Var}}(\hat m_g)=\nabla m_g^\top\,\widehat\Sigma_g\,\nabla m_g,\qquad
\text{SE}(\hat m_g)=\sqrt{\widehat{\mathrm{Var}}(\hat m_g)}
$$

To construct a 95% Wald CI, given by: 

$$
\hat m_g \pm z_{1 - \frac{\gamma}{2}} \,\text{SE}(\hat m_g) =
\hat m_g \pm 1.96\,\text{SE}(\hat m_g)
$$

For the difference $m_1-m_2$, independence of groups gives

$$
\begin{aligned}
\widehat{\mathrm{Var}}(\hat{m_1}-\hat{m_2})
&= \widehat{\mathrm{Var}}(\hat{m_1})+\widehat{\mathrm{Var}}(\hat{m_2}) + 2\widehat{\mathrm{Cov}}(\hat{m_1}, \hat{m_2})\\
&= \widehat{\mathrm{Var}}(\hat{m_1})+\widehat{\mathrm{Var}}(\hat{m_2}) \\ 
\end{aligned}
$$

<!-- Using the MLEs from earlier: -->

<!-- * Group 1: $\hat\alpha_1=3.497$, $\hat\beta_1=1.519$ $\Rightarrow$ -->
<!--   $\hat m_1=(\hat\alpha_1-1)/\hat\beta_1=1.644$, -->
<!--   $\text{SE}(\hat m_1)=0.177$, -->
<!--   95% CI: $(1.297,\ 1.991)$. -->

<!-- * Group 2: $\hat\alpha_2=1.626$, $\hat\beta_2=0.726$ $\Rightarrow$ -->
<!--   $\hat m_2=0.862$, -->
<!--   $\text{SE}(\hat m_2)=0.270$, -->
<!--   95% CI: $(0.334,\ 1.391)$. -->

<!-- * Difference (Group 1 - Group 2): -->
<!--   $\widehat{m_1-m_2}=0.782$, $\text{SE}=0.348$, -->
<!--   95% CI: $(0.149,\ 1.414)$. -->

<!-- (Values rounded to three decimals; both groups have $\hat\alpha>1$, so interior-mode delta method applies.) -->

Calculating these quantities gives us the following table: 

```{r, echo = F, warning = F, message = F}
fit1 <- fit_gamma_sr(dat$group1)
fit2 <- fit_gamma_sr(dat$group2)

m1 <- mode_from_fit(fit1)
m2 <- mode_from_fit(fit2)

# Difference in modes (independent groups): Var(diff) = Var(m1) + Var(m2)
diff_mode <- m1$mode - m2$mode
se_diff   <- sqrt(m1$se^2 + m2$se^2)
z <- qnorm(0.975)
ci_diff   <- c(diff_mode - z*se_diff, diff_mode + z*se_diff)

summary_q6 <- data.frame(
  group = c("group1", "group2", "difference (1 - 2)"),
  mode_hat = c(m1$mode, m2$mode, diff_mode),
  se       = c(m1$se,   m2$se,   se_diff),
  ci_L     = c(m1$ci[1], m2$ci[1], ci_diff[1]),
  ci_U     = c(m1$ci[2], m2$ci[2], ci_diff[2])
)

# Round only numeric columns
summary_q6_out <- summary_q6
num_cols <- sapply(summary_q6_out, is.numeric)
summary_q6_out[num_cols] <- lapply(summary_q6_out[num_cols], function(x) round(x, 3))

# print(summary_q6_out)
knitr::kable(summary_q6_out, caption = "Gamma modes and 95% Wald intervals")
```

\newpage 

## 7. 

Although model assessment has not yet been covered formally, it is intuitive that the estimated distribution function (CDF) under our model and the empirical distribution function of the data should be similar. Produce plots of the estimated distribution function for each group with the empirical distribution function overlaid.

### Answer

To avoid any potential "code dump" regarding this question, I just wanted to note that the `stats` package contains an `ecdf` function. This was used for the following comparisons with the "expected" line (based on the MLEs $\hat{\alpha_g}, \hat{\beta_g}$, $g \in \{1, 2 \}$). 

```{r, echo = F, warning = F, message = F}
y1 <- dat$group1; y2 <- dat$group2
f1 <- fit_gamma_sr(y1); f2 <- fit_gamma_sr(y2)

# make sure we're NOT in multi-panel mode
par(mfrow = c(1,1))

## Plot 1: Group 1
# x1 <- seq(0, max(y1, na.rm = TRUE), length.out = 400)
# plot(ecdf(y1), main = "Group 1: ECDF vs MLE Gamma CDF",
#      xlab = "y", ylab = "CDF", vert = TRUE, do.points = FALSE, col = "black")
# lines(x1, pgamma(x1, shape = f1$alpha, rate = f1$beta), lwd = 2, col = "blue")
# legend("bottomright", c("Empirical CDF", "Gamma CDF (MLE)"),
#        lty = 1, lwd = c(1,2), col = c("black","blue"), bty = "n")
# 
# ## Plot 2: Group 2
# x2 <- seq(0, max(y2, na.rm = TRUE), length.out = 400)
# plot(ecdf(y2), main = "Group 2: ECDF vs MLE Gamma CDF",
#      xlab = "y", ylab = "CDF", vert = TRUE, do.points = FALSE, col = "black")
# lines(x2, pgamma(x2, shape = f2$alpha, rate = f2$beta), lwd = 2, col = "red")
# legend("bottomright", c("Empirical CDF", "Gamma CDF (MLE)"),
#        lty = 1, lwd = c(1,2), col = c("black","red"), bty = "n")

# Base plot: ECDF of group 1
plot(ecdf(y1), main = "ECDF vs MLE Gamma CDF (Group 1 vs. 2)",
     xlab = "y", ylab = "CDF", vert = TRUE, do.points = FALSE, col = "blue")

# Add ECDF of group 2
lines(ecdf(y2), col = "red", vert = TRUE, do.points = FALSE)
# Add fitted Gamma CDFs
lines(x, pgamma(x, shape = f1$alpha, rate = f1$beta), col = "blue", lwd = 2, lty = 2)
lines(x, pgamma(x, shape = f2$alpha, rate = f2$beta), col = "red", lwd = 2, lty = 2)

# Legend
legend("bottomright",
       legend = c("ECDF (Group 1)", "ECDF (Group 2)",
                  "Gamma CDF MLE (Group 1)", "Gamma CDF MLE (Group 2)"),
       col = c("blue", "red", "blue", "red"),
       lwd = c(1,1,2,2), lty = c(1,1,2,2), bty = "n")

```

\newpage 

## 8.  

Write a short paragraph giving your conclusions about this group comparison.

### Answer

Comparing the ECDFs to their fitted Gamma distributions, both groups match reasonably well, although Group 1 has more visually apparent differences in the 2–4 range (domain). In terms of expectation/central tendency, the two groups have very similar means, as evidenced when comparing means via the two-sample t-tests. It is then perhaps unsurprising that the Wald confidence interval for the difference in expected values includes zero. However, the likelihood ratio test provides some evidence that the overall distributions differ, with Group 1 and Group 2 having distinct shape–rate parameter combinations (their individual parameter values). This is also evidenced somewhat in the above visual, as the ECDFs and fitted Gamma CDFs look different between the two groups. To put it hopefully succinctly: The results from the prior parts (1 through 7) suggest that while the groups do not differ significantly in their means, their distributional shapes are different enough to warrant modeling them separately.
