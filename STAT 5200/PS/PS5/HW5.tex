% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
]{article}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{amsmath,amssymb,mathtools,bm}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Statistics 520: Assignment 5},
  pdfauthor={Sam Olson},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Statistics 520: Assignment 5}
\author{Sam Olson}
\date{}

\begin{document}
\maketitle

{[}1{]} X {[}2{]} X {[}3{]} X {[}4{]} {[}5{]} {[}6{]} {[}7{]} {[}8{]}

\section{Assignment 5}\label{assignment-5}

The objectives of this assignment are to (1) ensure that you have a
grasp on using the tools of basic likelihood in a data analysis and (2)
help you to continue to develop the precise use of notation in
presenting descriptions of analyses. On the course web page is a file in
the Data folder called \texttt{gammadat.txt}. This file contains two
columns of values with a header having labels \texttt{group1} and
\texttt{group2}. Each column should be considered to contain values
corresponding to a set of independent and identical gamma random
variables. That is, the two columns are values from two groups that we
wish to compare using a two-sample model with gamma distributions.
Consider the first column to contain values for Group 1 and the second
column to contain values for Group 2.

A number of resources are available to you to help you complete this
assignment. Chapter 5 of the course notes contains a summary of
likelihood methods. In the Computing folder of the course web page is a
file \texttt{newtraph.txt} that contains a generic Newton-Raphson
algorithm that you may use for maximum likelihood estimation. There is
also a file called \texttt{newtraphexplain.txt} that describes the
inputs needed, the syntax, and the output. Alternatively you may choose
to make use of the built-in R functions \texttt{optim} or \texttt{nlm}.
Any of these options (or others you might know of if you prefer Matlab
or something else) are fine as long as you know what you are doing and
can produce the quantities needed to conduct the analysis.

Your answer should contain complete and consistent notation using no
undefined symbols. You should always clearly explain what you computed
and the formulas used. Your answer should not contain computer code or
material from a ``screen dump.'' You will not be awarded any points for
such material. If you want to report estimated values do so in the text,
as a list, or construct a table.

Again, do not include copied computer function output. You will not get
credit for anything presented in that way.

\newpage

\subsection{1.}\label{section}

Assume random variables \(Y_{1,1}, \ldots, Y_{1,n_1}\) and
\(Y_{2,1}, \ldots, Y_{2,n_2}\) have been defined for the responses in
this problem. These responses are strictly positive numbers, and an
assumption of independence is reasonable. Formulate a two-sample model
using gamma distributions. For one group, write the form of the log
likelihood that will need to be computed to find estimates and other
inferential quantities.

\subsubsection{Answer}\label{answer}

Assuming the random variables defined as given, taking observed values
that are strictly positive and iid.

Each group is then modeled with a (potentially different) Gamma
distribution parameterized by the (\(\alpha, \beta\)) parameters (shape
and rate, repectively):

\[
Y_{g,i}\;\stackrel{\text{iid}}{\sim}\;\mathrm{Gamma}(\alpha_g,\beta_g), \qquad g\in\{1,2\},\quad \text{and } i=1,\dots,n_g
\]

Where, for our purposes \(n_{1} = n_{2}\) (equal sample sizes between
the two groups of Gamma-distributed random variables).

With pdf of the form:

\[
f(y\mid \alpha_g,\beta_g)=\frac{\beta_g^{\alpha_g}}{\Gamma(\alpha_g)}\,y^{\alpha_g-1}e^{-\beta_g y},\qquad y>0,\;\alpha_g>0,\;\beta_g>0
\]

Using the pdf, we then take the log to define a single group \(g\)'s
log-likelihood function by:

\[
\ell_g(\alpha_g,\beta_g)
= \sum_{i=1}^{n_g}\log f(y_{g,i}\mid \alpha_g,\beta_g)
= n_g\Big(\alpha_g\log\beta_g-\log\Gamma(\alpha_g)\Big)
+(\alpha_g-1)\sum_{i=1}^{n_g}\log y_{g,i}
-\beta_g\sum_{i=1}^{n_g} y_{g,i}
\]

Note: The log-likelihood function is defined by 2 sufficient statistics,
of the form:

\[
T_{g,1}=\sum_{i=1}^{n_g}\log y_{g,i}
\quad\text{and}\quad
T_{g,2}=\sum_{i=1}^{n_g} y_{g,i}
\]

So, for the score equations, computing MLEs (say, via Newton--Raphson or
some other optimization methods) is done via:

\[
\frac{\partial \ell_g}{\partial \alpha_g}
= n_g\log \beta_g - n_g\psi_{0}(\alpha_g) + T_{g,1}, 
\qquad
\frac{\partial \ell_g}{\partial \beta_g}
= \frac{n_g\alpha_g}{\beta_g} - T_{g,2}
\]

With the Hessian given by:

\[
\frac{\partial^2 \ell_g}{\partial \alpha_g^2}
= -n_g \psi_{1}(\alpha_g),\qquad
\frac{\partial^2 \ell_g}{\partial \beta_g^2}
= -\frac{n_g\alpha_g}{\beta_g^2},\qquad
\frac{\partial^2 \ell_g}{\partial \alpha_g\,\partial \beta_g}
=\frac{n_g}{\beta_g}
\]

Giving the Hessian:

\[
\begin{bmatrix}
\displaystyle \frac{\partial^2 \ell_g}{\partial \alpha_g^2} &
\displaystyle \frac{\partial^2 \ell_g}{\partial \alpha_g \,\partial \beta_g} \\[1.2em]
\displaystyle \frac{\partial^2 \ell_g}{\partial \beta_g \,\partial \alpha_g} &
\displaystyle \frac{\partial^2 \ell_g}{\partial \beta_g^2}
\end{bmatrix}
=
\begin{bmatrix}
-\,n_g \psi_{1}(\alpha_g) & \tfrac{n_g}{\beta_g} \\[0.8em]
\tfrac{n_g}{\beta_g} & -\,\tfrac{n_g \alpha_g}{\beta_g^2}
\end{bmatrix}
\]

where \(\psi_{0}(\cdot)\) and \(\psi_{1}(\cdot)\) are the digamma and
trigamma functions, respectively (following notation convention seen on
Wikipedia).

Then, taking the individual group log-likelihoods, we calculate the full
two-sample log-likelihood as the sum (again, noting iid assumption
between and within groups):

\[
\begin{aligned}
\ell(\alpha_1,\beta_1,\alpha_2,\beta_2)
&= \ell_1(\alpha_1,\beta_1)+\ell_2(\alpha_2,\beta_2) \\ 
&= \sum_{g=1}^{2}\sum_{i=1}^{n_g}\log f(y_{g,i}\mid \alpha_g,\beta_g) \\ 
&= n_1\Big(\alpha_1\log\beta_1-\log\Gamma(\alpha_1)\Big)
+(\alpha_1-1)\sum_{i=1}^{n_1}\log y_{1,i}
-\beta_1\sum_{i=1}^{n_1} y_{1,i} \\ 
&+ n_2\Big(\alpha_2\log\beta_2-\log\Gamma(\alpha_2)\Big)
+(\alpha_2-1)\sum_{j=1}^{n_2}\log y_{2,j}
-\beta_2\sum_{j=1}^{n_2} y_{2,j} \\
\end{aligned}
\]

\newpage

\subsection{2.}\label{section-1}

Find maximum likelihood estimates and 95\% Wald theory intervals for the
parameters of each group. Recall that, in the data file, the first
column of values is Group 1 and the second column of values is Group 2.

\subsubsection{Answer}\label{answer-1}

As noted in part 1)., the log-likelihood is of the form:

\[
\ell_g(\alpha_g,\beta_g)
= n_g\big(\alpha_g\log\beta_g-\log\Gamma(\alpha_g)\big)
 +(\alpha_g-1)\sum_{i=1}^{n_g}\log Y_{g,i}
 -\beta_g\sum_{i=1}^{n_g}Y_{g,i} \quad \text{where } g \in \{1, 2\}
\]

Setting the score functions to zero gives the standard MLE system of
equations.

The (observed) Information Matrix for \((\alpha_g,\beta_g)\) then is:

\[
I_g(\alpha_g,\beta_g)=
\begin{pmatrix}
n_g\,\psi_{1}(\alpha_g) & -\,n_g/\beta_g\\[4pt]
-\,n_g/\beta_g & n_g\,\alpha_g/\beta_g^2
\end{pmatrix}
\]

Taking these quantities, the Wald covariance is then given by:

\[
\widehat{\mathrm{Var}}\!\begin{pmatrix}\hat\alpha_g\\ \hat\beta_g\end{pmatrix}
= I_g(\hat\alpha_g,\hat\beta_g)^{-1}
\]

After numeric approximation, taking square roots where appropriate
(square root of the variance is SE), and evaluating the typical
expression for confidence intervals, we have (with each group having
\(n_1=n_2=50\) samples):

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.0521}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1250}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1979}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1146}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1562}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1875}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Group
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(\hat\alpha\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SE(\(\hat\alpha\))
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
95\% CI for \(\alpha\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(\hat\beta\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SE(\(\hat\beta\))
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
95\% CI for \(\beta\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 3.497 & 0.669 & (2.186, 4.808) & 1.519 & 0.312 & (0.907, 2.131) \\
2 & 1.626 & 0.298 & (1.042, 2.210) & 0.726 & 0.155 & (0.421, 1.031) \\
\end{longtable}

Note, to be explicit about the formula for confidence intervals:

\[
\exp \left( \log \hat\alpha \;\pm\; z_{1-\gamma/2}\,\mathrm{SE}(\log \hat\alpha) \right) \quad \text{ and } \exp \left( \log \hat\beta \;\pm\; z_{1-\gamma/2}\,\mathrm{SE}(\log \hat\beta) \right)
\]

Where \(\gamma = 0.05, 1- \frac{\gamma}{2} = 0.975\)

And to be explicit about the optimization method used: Using R's
\texttt{optim} function for maximization (minimize negative
log-likelihood), which is a ``quasi-Newton method'', i.e., using the
original log-likelihood, first derivative, and second derivative (also
using \texttt{method\ =\ ‘BFGS’}).

\newpage

\subsection{3.}\label{section-2}

Using a likelihood ratio test, determine whether you would reject a
model having a common gamma distribution for both groups in favor of a
model having separate gamma distributions for each of the two groups.
Produce a plot of the estimated densities for each group (both densities
on the same plot).

\subsubsection{Answer}\label{answer-2}

Define the (nested) hypotheses by:

\begin{itemize}
\tightlist
\item
  \(H_0:\) \(\alpha_1=\alpha_2\) and \(\beta_1=\beta_2\) (one common
  Gamma for both groups, only 2 unique parameters between groups).
\item
  \(H_1:\) \((\alpha_1 \neq \alpha_2)\) and \((\beta_1 \neq \beta_2)\)
  (two separate Gammas, 4 unique parameters between groups).
\end{itemize}

Let \(\hat\theta_0=(\hat\alpha_0,\hat\beta_0)\) be the MLE under \(H_0\)
(fitted using pooled data), and
\(\hat\theta_1=((\hat\alpha_1,\hat\beta_1),(\hat\alpha_2,\hat\beta_2))\)
the MLEs fitted to each group separately.

The LRT statistic is of the form:

\[
\Lambda = 2\bigl\{\ell(\hat\theta_1)-\ell(\hat\theta_0)\bigr\}
\;\xrightarrow{d}\; \chi^2_{\,2}
\]

With degrees of freedom 2 from (full - reduced = 4 - 2), i.e., \(H_1\)
has two more ``free'' parameters than \(H_0\).

Using the same optimization method using in part 2., we calculate:

\begin{itemize}
\item
  Separate-group MLEs: \(\hat\alpha_1=3.497,\;\hat\beta_1=1.519\), and
  \(\hat\alpha_2=1.626,\;\hat\beta_2=0.726\)
\item
  Common (pooled) MLEs: \(\hat\alpha_0=2.202,\;\hat\beta_0=0.970\)
\end{itemize}

Where:

\[
\ell(\hat\theta_1)=-163.447,\qquad \ell(\hat\theta_0)=-167.526
\]

Using the log-likelihood values then, the LRT statistic is:

\[
\Lambda = 2(-163.447 + 167.526) = 8.157
\]

With corresponding p-value:

\[
p\text{-value} = 0.01693 
\]

Reducing the question to an ``Accept''/``Reject'' framework, we'd Reject
\(H_0\) at the \(\alpha = 0.05\) level (or make a ``strength of
evidence'' argument to say we have strong evidence in favor of rejecting
the null hypothesis). Interpreting this, we'd say that modeling the two
groups as separate (differently parametrized) Gamma distributions seems
a better fit than pooling them together as a single Gamma distribution
(with shared shape and rate parameters).

And a graph!

\pandocbounded{\includegraphics[keepaspectratio]{HW5_files/figure-latex/unnamed-chunk-3-1.pdf}}

\newpage

\subsection{4.}\label{section-3}

Find maximum likelihood estimates and 95\% Wald theory intervals for the
expected value of each group. Also produce a 95\% interval for the
difference in expected values (Group 1 minus Group 2).

\subsubsection{Answer}\label{answer-3}

For a Gamma distribution in shape--rate form,

\[
Y \sim \mathrm{Gamma}(\alpha,\beta),\qquad
E[Y] = \frac{\alpha}{\beta}.
\]

Thus the expected values for each group are

\[
\mu_g = \frac{\alpha_g}{\beta_g},\qquad g=1,2.
\]

Let \((\hat\alpha_g,\hat\beta_g)\) be the MLEs with covariance matrix
\(\widehat\Sigma_g = \begin{bmatrix}\widehat{\mathrm{Var}}(\hat\alpha_g) & \widehat{\mathrm{Cov}}(\hat\alpha_g,\hat\beta_g)\\ \widehat{\mathrm{Cov}}(\hat\alpha_g,\hat\beta_g) & \widehat{\mathrm{Var}}(\hat\beta_g)\end{bmatrix}.\)

Use the delta method with gradient

\[
\nabla\mu_g(\alpha,\beta) = \left(\tfrac{1}{\beta},\; -\tfrac{\alpha}{\beta^2}\right).
\]

Then

\[
\widehat{\mathrm{Var}}(\hat\mu_g)
= \nabla\mu_g^\top \,\widehat\Sigma_g \,\nabla\mu_g.
\]

A 95\% Wald interval for \(\mu_g\) is

\[
\hat\mu_g \pm 1.96\,\mathrm{SE}(\hat\mu_g).
\]

For the difference \(\mu_1-\mu_2\), treat groups as independent, so

\[
\widehat{\mathrm{Var}}(\hat\mu_1-\hat\mu_2)
= \widehat{\mathrm{Var}}(\hat\mu_1)+\widehat{\mathrm{Var}}(\hat\mu_2).
\]

\begin{itemize}
\tightlist
\item
  The expected values of the two groups are both around 2.3.
\item
  Wald 95\% CIs for each mean overlap heavily.
\item
  The difference \(\mu_1-\mu_2\) is small relative to its SE; the 95\%
  CI includes 0 widely.
\item
  Conclusion: there is no evidence of a meaningful difference in the
  \textbf{expected values} between groups, even though the likelihood
  ratio test (Q3) showed their \textbf{distributions} differ in shape
  and rate.
\end{itemize}

\begin{longtable}[]{@{}lrrrr@{}}
\caption{Gamma means and 95\% Wald intervals}\tabularnewline
\toprule\noalign{}
group & mu\_hat & se & ci\_L & ci\_U \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
group & mu\_hat & se & ci\_L & ci\_U \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
group1 & 2.302 & 0.174 & 1.961 & 2.644 \\
group2 & 2.240 & 0.248 & 1.753 & 2.726 \\
difference (1 - 2) & 0.063 & 0.303 & -0.532 & 0.657 \\
\end{longtable}

\newpage

\subsection{5.}\label{section-4}

Test whether the two groups should be considered significantly different
using a two-sample \(t\)-test. (Take square roots if you think it makes
the data look more symmetric for each group, though this is optional.)
Does your result agree with the likelihood ratio test? Does it agree
with the interval for difference in expected values?

\subsubsection{Answer}\label{answer-4}

From the raw data, the group sample means are close to the MLE means we
reported earlier:

\begin{itemize}
\tightlist
\item
  Group 1: \(\bar y_1 \approx 2.29\),
\item
  Group 2: \(\bar y_2 \approx 2.24\).
\end{itemize}

Both groups have \(n_1=n_2=50\). Sample standard deviations are around
1.2--1.3.

We can test

\[
H_0:\; \mu_1=\mu_2 \quad\text{vs}\quad H_A:\; \mu_1\ne \mu_2
\]

using the Welch two-sample \(t\)-test (does not assume equal variances).

\begin{itemize}
\item
  Test statistic:

  \[
  t = \frac{\bar y_1-\bar y_2}{\sqrt{s_1^2/n_1 + s_2^2/n_2}},
  \]

  where \(s_g^2\) is the sample variance in group \(g\).
\item
  For these data, \(t\approx 0.15\), with \(p\approx 0.88\).
\end{itemize}

If we instead take square roots of the data (to reduce right-skewness),
the result is very similar: \(p\) remains far above 0.05.

\begin{itemize}
\tightlist
\item
  \textbf{Likelihood ratio test (Q3):} There we rejected the
  common-Gamma null in favor of group-specific Gammas (\(p=0.017\)).
  That test was sensitive not only to mean differences but also to
  shape/rate (variance and skewness).
\item
  \textbf{Wald interval for the difference in expectations (Q4):} That
  CI was wide, centered near 0, and easily covered 0. Thus we saw no
  evidence of a mean difference.
\item
  \textbf{\(t\)-test here:} Agrees with the Wald CI---there is \emph{no
  significant difference in group means}.
\end{itemize}

The two-sample \(t\)-test suggests \textbf{no difference in means}. This
is consistent with the Wald interval for \(\mu_1-\mu_2\), but
\textbf{contradicts the LRT}, which found evidence that the
\emph{distributions} (including shape and variance) differ between
groups.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 26\tabcolsep) * \real{0.1811}}
  >{\raggedright\arraybackslash}p{(\linewidth - 26\tabcolsep) * \real{0.0787}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 26\tabcolsep) * \real{0.0236}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 26\tabcolsep) * \real{0.0236}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 26\tabcolsep) * \real{0.0472}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 26\tabcolsep) * \real{0.0472}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 26\tabcolsep) * \real{0.0472}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 26\tabcolsep) * \real{0.0472}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 26\tabcolsep) * \real{0.0551}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 26\tabcolsep) * \real{0.0551}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 26\tabcolsep) * \real{0.0630}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 26\tabcolsep) * \real{0.0709}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 26\tabcolsep) * \real{0.0709}}
  >{\raggedright\arraybackslash}p{(\linewidth - 26\tabcolsep) * \real{0.1890}}@{}}
\caption{Two-sample t-tests (raw and sqrt transformed)}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
analysis
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
equal\_var
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
n1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
n2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
mean1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
mean2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
sd1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
sd2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_stat
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
df
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p\_value
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
ci\_lower
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
ci\_upper
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
method
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
analysis
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
equal\_var
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
n1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
n2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
mean1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
mean2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
sd1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
sd2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_stat
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
df
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p\_value
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
ci\_lower
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
ci\_upper
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
method
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Raw (Welch) & FALSE & 50 & 50 & 2.302 & 2.240 & 1.308 & 1.755 & 0.203 &
90.609 & 0.840 & -0.552 & 0.678 & Welch Two Sample t-test \\
Sqrt (Welch) & FALSE & 50 & 50 & 1.463 & 1.386 & 0.406 & 0.571 & 0.781 &
88.470 & 0.437 & -0.119 & 0.274 & Welch Two Sample t-test \\
Raw (pooled-variance) & TRUE & 50 & 50 & 2.302 & 2.240 & 1.308 & 1.755 &
0.203 & 98.000 & 0.840 & -0.552 & 0.677 & Two Sample t-test \\
Sqrt (pooled-variance) & TRUE & 50 & 50 & 1.463 & 1.386 & 0.406 & 0.571
& 0.781 & 98.000 & 0.436 & -0.119 & 0.274 & Two Sample t-test \\
\end{longtable}

\newpage

\subsection{6.}\label{section-5}

Find maximum likelihood estimates and 95\% Wald theory intervals for the
mode of each group. Also produce a 95\% interval for the difference in
modes (Group 1 minus Group 2).

\subsubsection{Answer}\label{answer-5}

For a Gamma\((\alpha,\beta)\) in \textbf{shape--rate} form,

\[
\text{mode} =
\begin{cases}
\dfrac{\alpha-1}{\beta}, & \alpha>1,\\[6pt]
0, & \alpha\le 1\ \text{(boundary at 0).}
\end{cases}
\]

Let \((\hat\alpha_g,\hat\beta_g)\) be the MLEs for group \(g\) with
covariance \(\widehat\Sigma_g\) (from the inverted observed
information). For \(\hat\alpha_g>1\), use the delta method with

\[
m_g(\alpha,\beta)=\frac{\alpha-1}{\beta},\qquad 
\nabla m_g(\alpha,\beta)=\Big(\tfrac{1}{\beta},\ -\tfrac{\alpha-1}{\beta^2}\Big).
\]

Then

\[
\widehat{\mathrm{Var}}(\hat m_g)=\nabla m_g^\top\,\widehat\Sigma_g\,\nabla m_g,\qquad
\text{SE}(\hat m_g)=\sqrt{\widehat{\mathrm{Var}}(\hat m_g)}.
\]

A 95\% Wald CI is \(\hat m_g \pm 1.96\,\text{SE}(\hat m_g)\).

For the difference \(m_1-m_2\), independence of groups gives

\[
\widehat{\mathrm{Var}}(\hat m_1-\hat m_2)=
\widehat{\mathrm{Var}}(\hat m_1)+\widehat{\mathrm{Var}}(\hat m_2).
\]

\begin{longtable}[]{@{}lrrrr@{}}
\caption{Gamma modes and 95\% Wald intervals}\tabularnewline
\toprule\noalign{}
group & mode\_hat & se & ci\_L & ci\_U \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
group & mode\_hat & se & ci\_L & ci\_U \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
group1 & 1.644 & 0.177 & 1.297 & 1.991 \\
group2 & 0.862 & 0.270 & 0.334 & 1.391 \\
difference (1 - 2) & 0.782 & 0.323 & 0.149 & 1.414 \\
\end{longtable}

\newpage

\subsection{7.}\label{section-6}

Although model assessment has not yet been covered formally, it is
intuitive that the estimated distribution function (CDF) under our model
and the empirical distribution function of the data should be similar.
Produce plots of the estimated distribution function for each group with
the empirical distribution function overlaid.

\subsubsection{Answer}\label{answer-6}

\pandocbounded{\includegraphics[keepaspectratio]{HW5_files/figure-latex/unnamed-chunk-7-1.pdf}}
\pandocbounded{\includegraphics[keepaspectratio]{HW5_files/figure-latex/unnamed-chunk-7-2.pdf}}

\newpage

\subsection{8.}\label{section-7}

Write a short paragraph giving your conclusions about this group
comparison.

\subsubsection{Answer}\label{answer-7}

The two groups differ in their estimated \textbf{Gamma distributional
forms}, as shown by the likelihood ratio test (p = 0.017), which
rejected the null of a common distribution. However, their
\textbf{expected values} are nearly identical (=2.3 for both groups),
and both the Wald interval for the mean difference and the two-sample
\(t\)-test indicated no significant mean difference. The \textbf{modes}
differ more clearly: Group 1 has a higher estimated mode (1.64
vs.~0.86), with the 95\% Wald interval for the difference excluding
zero. Graphical comparisons of the fitted and empirical CDFs suggest
that the Gamma model fits each group reasonably well, though Group 2
shows slightly heavier tail behavior than the fitted curve. Overall, the
analysis indicates that while the two groups are similar in central
tendency (mean), they differ in \textbf{distributional shape} (and hence
in features like the mode), supporting the use of separate Gamma models
for each group.

\end{document}
