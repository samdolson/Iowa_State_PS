---
title: "5200 Take-Home"
author: "Sam Olson" 
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=F, warning=F, echo=F}
library(knitr)
library(kableExtra)
library(dplyr)
lakesDat <- read.table("C:/Users/samue/OneDrive/Desktop/Iowa_State_PS/STAT 5200/Midterm2/takehomedata_2025.txt", header = T)
source("C:/Users/samue/OneDrive/Desktop/Iowa_State_PS/STAT 5200/Misc/basicglm.txt")
source("C:/Users/samue/OneDrive/Desktop/Iowa_State_PS/STAT 5200/Misc/newtraph.txt")
source("C:/Users/samue/OneDrive/Desktop/Iowa_State_PS/STAT 5200/Midterm2/boxcoxfctns.txt")
```

# Q1: CHL & TN (Plains vs. Ozarks)

## Overall Distribution & Approach

```{r, message=F, warning=F, echo=F, fig.height = 10, fig.width = 10}
layout(matrix(c(1, 1,
                2, 3),
              nrow = 2, byrow = TRUE))

par(mar = c(4, 4, 3, 1),
    oma = c(0, 0, 0, 0),
    xaxs = "i", yaxs = "i")

plot(y = lakesDat$chl, x = lakesDat$tn, xlab = "TN", ylab = "CHL", main = "Overall Scatterplot CHL(y) to TN(x)")

# plot(y = lakesDat$chl, x = lakesDat$tn, col = as.factor(lakesDat$region),
#      xlab = "TN", ylab = "CHL", main = "Overall Scatterplot CHL(y) to TN(x)")
xlim <- range(lakesDat$tn,  na.rm = TRUE)
ylim <- range(lakesDat$chl, na.rm = TRUE)
# Plains
with(subset(lakesDat, region == "plains"),
     plot(tn, chl,
          xlab = "TN", ylab = "CHL",
          main = "Plains",
          col = "#0072B2", pch = 19,
          xlim = xlim, ylim = ylim))

# Ozark
with(subset(lakesDat, region == "ozarks"),
     plot(tn, chl,
          xlab = "TN", ylab = "CHL",
          main = "Ozarks",
          col = "#D55E00", pch = 19,
          xlim = xlim, ylim = ylim))

par(mfrow = c(1, 1))
```

<!-- # ```{r, message=F, warning=F, echo=F} -->
<!-- # hist(lakesDat$chl, freq = T, xlab = "CHL", main = "Marginal Distribution of CHL") -->

CHL is right-skewed and increases with TN, with variance also rising at higher TN values. The regional scatterplots (Plains vs. Ozarks) show the same general pattern but suggest possible differences in the strength of the TN–CHL relationship.

Let $Y_i$ de a random variable denoting CHL for lake $i$ ($i=1,\cdots,134$), and also denote the corresponding covariate TN value $x_i$. Each lake appears once, judging by the column `lake` in the dataset, so observations may be treated as independent.

Assuming a common underlying biological mechanism across Missouri reservoirs, which seems generally reasonable, we first fit an overall TN–CHL model to determine an appropriate mean–variance structure (in addition to potential link functions). Once a suitable overall model is identified, we then apply this same model structure separately to the Plains and Ozarks so that any differences in fitted curves or parameters may be assessed as indications of regional differences rather than artifacts of using different model families (a risk we would run if we instead assessed model fit by region).

We then proceed with potential candidate models, starting with GLMs. 

## Generalized Linear Models

```{r, eval=F, message=F, warning=F, echo=F}
# Box Cox
# log(lakesDat$chl)
bc_plot <- function(y, nbins, main) {
  q <- quantile(y, probs = seq(0, 1, length.out = nbins + 1), na.rm = TRUE)
  cuts <- cut(y, breaks = q, include.lowest = TRUE)
  m <- tapply(y, cuts, mean, na.rm = TRUE)
  s <- tapply(y, cuts,  sd,   na.rm = TRUE)

  plot(x = log(m), y = log(s),
       xlab = "log(mean CHL)", ylab = "log(sd CHL)", main = main)
  fit <- lm(log(s) ~ log(m))
  abline(fit, col = "red", lwd = 2)
  invisible(fit)
}

op <- par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))
fit4 <- bc_plot(lakesDat$chl, nbins = 4,  main = "Equal-counts bins (4)")
fit8 <- bc_plot(lakesDat$chl, nbins = 8, main = "Equal-counts bins (8)")
fit12 <- bc_plot(lakesDat$chl, nbins = 12,  main = "Equal-counts bins (12)")
fit16 <- bc_plot(lakesDat$chl, nbins = 16, main = "Equal-counts bins (16)")
fit20 <- bc_plot(lakesDat$chl, nbins = 20,  main = "Equal-counts bins (20)")
fit24 <- bc_plot(lakesDat$chl, nbins = 24, main = "Equal-counts bins (24)")
par(op)

summ_tbl <- function(fit, nbins) {
  s <- summary(fit)
  ct <- coef(s)
  data.frame(
    Bins      = nbins,
    Term      = rownames(ct),
    Estimate  = round(ct[, 1], 4),
    SE        = round(ct[, 2], 4),
    t_value   = round(ct[, 3], 2),
    p_value   = formatC(ct[, 4], format = "e", digits = 2),
    R2        = round(s$r.squared, 3),
    Adj_R2    = round(s$adj.r.squared, 3),
    N         = s$df[1] + s$df[2] + 1,
    check.names = FALSE
  )
}

tab <- rbind(summ_tbl(fit4, 4), summ_tbl(fit8, 8), summ_tbl(fit12, 12), summ_tbl(fit16, 16), summ_tbl(fit20, 20), summ_tbl(fit24, 24))

kbl(tab, booktabs = TRUE,
    caption = "Linear regressions for Box–Cox mean–sd plots (log(sd) ~ log(mean))") |>
  kable_styling(full_width = FALSE, position = "center")

bc_plot_equal <- function(y, nbins, main) {
  # equal-width break points
  q <- seq(from = min(y, na.rm = TRUE),
           to   = max(y, na.rm = TRUE),
           length.out = nbins + 1)

  cuts <- cut(y, breaks = q, include.lowest = TRUE)
  m <- tapply(y, cuts, mean, na.rm = TRUE)
  s <- tapply(y, cuts,  sd,   na.rm = TRUE)

  plot(x = log(m), y = log(s),
       xlab = "log(mean CHL)", ylab = "log(sd CHL)", main = main)
  fit <- lm(log(s) ~ log(m))
  abline(fit, col = "red", lwd = 2)
  invisible(fit)
}

op <- par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))
fit4 <- bc_plot_equal(lakesDat$chl, nbins = 4,  main = "Equal-counts bins (4)")
fit8 <- bc_plot_equal(lakesDat$chl, nbins = 8, main = "Equal-counts bins (8)")
fit12 <- bc_plot_equal(lakesDat$chl, nbins = 12,  main = "Equal-counts bins (12)")
fit16 <- bc_plot_equal(lakesDat$chl, nbins = 16, main = "Equal-counts bins (16)")
fit20 <- bc_plot_equal(lakesDat$chl, nbins = 20,  main = "Equal-counts bins (20)")
fit24 <- bc_plot_equal(lakesDat$chl, nbins = 24, main = "Equal-counts bins (24)")
par(op)

tab <- rbind(summ_tbl(fit4, 4), summ_tbl(fit8, 8), summ_tbl(fit12, 12), summ_tbl(fit16, 16), summ_tbl(fit20, 20), summ_tbl(fit24, 24))
kbl(tab, booktabs = TRUE,
    caption = "Linear regressions for Box–Cox mean–sd plots (log(sd) ~ log(mean))") |>
  kable_styling(full_width = FALSE, position = "center")
```

```{r, message=F, warning=F, echo=F}
bc1 <- boxcox1(list(x = lakesDat$tn, y = lakesDat$chl), nbins = 12)
bc2 <- boxcox2(list(x = lakesDat$tn, y = lakesDat$chl), nbins = 12)
plot(log(bc1$m), log(bc1$v), main = "Box-Cox, 12 Equal-Count Bins", xlab = "log(Mean)", ylab = "log(Sd)")
fit <- lm(log(v) ~ log(m), data = bc1)
abline(fit, col = "red", lwd = 2)

nbins_vec <- c(6, 10, 12, 14, 16, 18, 22)

extract_slope_theta <- function(k) {
  bc_count  <- boxcox1(list(x = lakesDat$tn, y = lakesDat$chl), nbins = k)  # equal-count
  bc_spaced <- boxcox2(list(x = lakesDat$tn, y = lakesDat$chl), nbins = k)  # equal-spaced

  slope_count  <- coef(lm(log(v) ~ log(m), data = bc_count))[["log(m)"]]
  slope_spaced <- coef(lm(log(v) ~ log(m), data = bc_spaced))[["log(m)"]]

  data.frame(
    nbins = k,
    `Equal-Count`  = round(slope_count,  2),
    `Equal-Spaced` = round(slope_spaced, 2)
  )
}

bc_table <- do.call(rbind, lapply(nbins_vec, extract_slope_theta))
kable(bc_table)
```

To select an appropriate GLM family, I used `boxcoxfctns` to estimate Box–Cox log(mean)–log(variance) relationships across various specifications. Across all binning schemes, the slopes lay between 1.6 and 2.1, indicating that $\operatorname{Var}(CHL)$ increases roughly by a factor of $\mu^{2}$ to $\mu^{3}$. Such variance patterns are consistent with Gamma or Inverse Gaussian random components, and as such were considered candidate random components.

With the random component identified, the next step is to determine a suitable link function for modeling the TN–CHL mean relationship.

```{r, message=F, warning=F, echo=F, fig.height = 8, fig.width = 8}
op <- par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))

# plot(y = log(lakesDat$chl), x = lakesDat$tn, xlab = "TN", ylab ="log(CHL)", main = "Transform Response Log")
# plot(y = 1/(lakesDat$chl), x = lakesDat$tn, xlab = "TN", ylab ="1/CHL",  main = "Transform Response Inverse")
plot(y = sqrt(lakesDat$chl), x = lakesDat$tn, xlab = "TN", ylab ="sqrt(CHL)",  main = "Transform Response Sqrt")
# plot(y = (lakesDat$chl)^(2/3), x = lakesDat$tn, xlab = "TN", ylab ="CHL^2/3",  main = "Transform Response 2/3rd root")
plot(y = (lakesDat$chl)^(1/3), x = lakesDat$tn, xlab = "TN", ylab ="CHL^1/3",  main = "Transform Response 1/3rd root")
# plot(y = (lakesDat$chl), x = lakesDat$tn, xlab = "TN", ylab ="CHL",  main = "Transform Response Identity")
op <- par(mfrow = c(1, 1), mar = c(4, 4, 2, 1))
```

To select a link function, I examined transformations of CHL that approximately linearize its relationship with TN. Exploratory plots showed that the cube-root transformation yielded the most linear trend, with the square-root transformation performing reasonably well (these two being shown, though multiple link functions were considered, including link functions not available in the `basic.glm` function). Notably, these transformations were used only to guide link choice and under no circumstances used to motivate a transformation of the response in the GLM itself.

Taken together, this motivated fitting Gamma and Inverse Gaussian GLMs paired with power links corresponding to the cube-root and square-root transformations.

```{r, include=F, message=F, warning=F, echo=F}
X <- cbind(1, lakesDat$tn)
Y <- lakesDat$chl

mod_invgauss_sqrt <- basic.glm(
  xmat   = X,
  y      = Y,
  link   = 8,
  random = 6,
  pwr = 1/2
)

mod_invgauss_13 <- basic.glm(
  xmat   = X,
  y      = Y,
  link   = 8,
  random = 6,
  pwr = 1/3
)

mod_gamma_sqrt <- basic.glm(
  xmat   = X,
  y      = Y,
  link   = 8,
  random = 5,
  pwr = 1/2
)

mod_gamma_13 <- basic.glm(
  xmat   = X,
  y      = Y,
  link   = 8,
  random = 5,
  pwr = 1/3
)

combine_glm_results <- function(mod, model_name) {
  beta_hat <- mod$estb[, 1]
  se_hat <- sqrt(diag(mod$invinf))
  z <- 1.96
  LCL <- beta_hat - z * se_hat
  UCL <- beta_hat + z * se_hat

  phi <- mod$ests$phi
  # unscaled deviance
  udev <- mod$ests$udev
  sdev <- mod$ests$sdev
  # scaled deviance
  # sdev <- udev / phi

  data.frame(
    Term        = c("Intercept", "Body Mass"),
    Estimate    = beta_hat,
    SE          = se_hat,
    Phi         = phi,
    UnscaledDev = udev,
    ScaledDev   = sdev,
    LogLik      = mod$ests$loglik.fitted,
    LCL         = LCL,
    UCL         = UCL,
    Model       = model_name,
    check.names = FALSE
  )
}

tab_combined <- rbind(
  combine_glm_results(mod_invgauss_sqrt, "Inverse Gaussian (sqrt link)"),
  combine_glm_results(mod_invgauss_13, "Inverse Gaussian 1/3rd link"),
  combine_glm_results(mod_gamma_sqrt,    "Gamma (sqrt link)"),
  combine_glm_results(mod_gamma_13, "Gamma 1/3rd link")
)

kableExtra::kbl(
  tab_combined,
  digits = 4,
  align  = "lrrrrrrrrl",
  row.names = FALSE,
  caption = "Model comparison with Wald 95\\% CIs and both unscaled and scaled deviances"
) |>
  kableExtra::kable_styling(
    full_width = FALSE,
    position   = "center",
    latex_options = c("hold_position", "scale_down")
  )
```

Within each GLM family, the scaled deviance provides an appropriate criterion for comparing link functions. The cube-root power link consistently produced lower deviances (raw and scaled) than the square-root link within both the Gamma and Inverse Gaussian families; so the cube-root versions of these two models were treated as viable candidates in toto. However, the Gamma and IG models themselves are not compared by deviance, as they are not nested and thus do not allow LRT-like comparisons.

```{r, message=F, warning=F, echo=F, fig.height=10, fig.width=10}
layout(matrix(c(1, 2,
                3, 4),
              nrow = 2, byrow = TRUE))

par(mar = c(4, 4, 3, 1),
    oma = c(0, 0, 0, 0),
    xaxs = "i", yaxs = "i")

## ----- Setup ----- ##
x_grid <- seq(min(lakesDat$tn), max(lakesDat$tn), length.out = 200)
X_grid <- cbind(1, x_grid)
z90 <- qnorm(0.95)


## =========================
##  GAMMA MODEL
## =========================

beta_hat  <- mod_gamma_13$estb[,1]
vcov_mat  <- mod_gamma_13$invinf
se_hat    <- sqrt(diag(vcov_mat))

eta_hat <- X_grid %*% beta_hat
se_eta  <- sqrt(rowSums((X_grid %*% vcov_mat) * X_grid))

eta_lo <- eta_hat - z90 * se_eta
eta_hi <- eta_hat + z90 * se_eta

mu_hat <- exp(eta_hat)
mu_lo  <- exp(eta_lo)
mu_hi  <- exp(eta_hi)


## =========================
##  INVERSE GAUSSIAN MODEL
## =========================

beta_hat2 <- mod_invgauss_13$estb[,1]
vcov_mat2 <- mod_invgauss_13$invinf
se_hat2   <- sqrt(diag(vcov_mat2))

eta_hat2 <- X_grid %*% beta_hat2
se_eta2  <- sqrt(rowSums((X_grid %*% vcov_mat2) * X_grid))

eta_lo2 <- eta_hat2 - z90 * se_eta2
eta_hi2 <- eta_hat2 + z90 * se_eta2

mu_hat2 <- exp(eta_hat2)
mu_lo2  <- exp(eta_lo2)
mu_hi2  <- exp(eta_hi2)


## =========================
##  TWO SIDE-BY-SIDE PLOTS
## =========================

# par(mfrow = c(1, 2),
#     oma = c(0,0,0,0),
#     mar = c(4,4,2,1),
#     xaxs="i", yaxs="i")

## ----- Left panel: Gamma ----- ##
plot(lakesDat$tn, lakesDat$chl, col = "black",
     xlab = "TN", ylab = "CHL",
     main = "Gamma Model (90% CI)")

lines(x_grid, mu_hat, lwd = 2, col = "black")
lines(x_grid, mu_lo,  lwd = 1, col = "black", lty = 3)
lines(x_grid, mu_hi,  lwd = 1, col = "black", lty = 3)

legend("topleft",
       legend = c("Gamma mean", "Gamma 90% band"),
       col = c("black", "black"),
       lty = c(1, 3), lwd = c(2, 1), bty = "n")


## ----- Right panel: Inverse Gaussian ----- ##
plot(lakesDat$tn, lakesDat$chl, col = "black",
     xlab = "TN", ylab = "CHL",
     main = "Inverse Gaussian Model (90% CI)")

lines(x_grid, mu_hat2, lwd = 2, col = "blue")
lines(x_grid, mu_lo2,  lwd = 1, col = "blue", lty = 3)
lines(x_grid, mu_hi2,  lwd = 1, col = "blue", lty = 3)

legend("topleft",
       legend = c("InvGaussian mean", "InvGaussian 90% band"),
       col = c("blue", "blue"),
       lty = c(1, 3), lwd = c(2, 1), bty = "n")

# GAMMA
mu_g       <- mod_gamma_13$vals$muhat
res_g_p    <- mod_gamma_13$vals$pearsonres
res_g_dev  <- mod_gamma_13$vals$devres
res_g_std  <- mod_gamma_13$vals$stdevres

# INVGAUSSIAN
mu_ig      <- mod_invgauss_13$vals$muhat
res_ig_p   <- mod_invgauss_13$vals$pearsonres
res_ig_dev <- mod_invgauss_13$vals$devres
res_ig_std <- mod_invgauss_13$vals$stdevres

# par(mfrow = c(2, 2),
#     mar = c(4, 4, 3, 1),
#     oma = c(0, 0, 0, 0),
#     xaxs = "i", yaxs = "i")

# plot(mu_g, res_g_dev,
#      xlab = "Fitted (Gamma)", ylab = "Deviance residual",
#      main = "Gamma: Deviance Residuals")
# abline(h = 0, lty = 2)
# 
# plot(mu_ig, res_ig_dev,
#      xlab = "Fitted (InvGaussian)", ylab = "Deviance residual",
#      main = "InvGaussian: Deviance Residuals")
# abline(h = 0, lty = 2)

# par(mfrow = c(1, 1))

# par(mfrow = c(1, 2))

plot(mu_g, res_g_std,
     xlab = "Fitted (Gamma)", ylab = "Std. Deviance residual",
     main = "Gamma: Std. Deviance Residuals")
abline(h = 0, lty = 2)

plot(mu_ig, res_ig_std,
     xlab = "Fitted (InvGaussian)", ylab = "Std. Deviance residual",
     main = "InvGaussian: Std. Deviance Residuals")
abline(h = 0, lty = 2)

par(mfrow = c(1, 1))

# par(mfrow = c(1, 2))
#
# qqnorm(res_g_std, main = "Gamma: Std Deviance QQ Plot")
# qqline(res_g_std)
#
# qqnorm(res_ig_std, main = "InvGaussian: Std Deviance QQ Plot")
# qqline(res_ig_std)
#
# par(mfrow = c(1, 1))
```

To compare the Gamma and Inverse Gaussian models, we then turn to a general assessment of deviance spread and fitted curves. Oddly, both yield nearly identical fitted curves, so scatterplots alone cannot distinguish them. Residual diagnostics provide clearer evidence: The Gamma model exhibits more stable variance and better-behaved deviance residuals, whereas the IG model shows mild heteroscedasticity. Thus, the Gamma GLM is selected as the preferred model, despite evidence that there may be some outliers in both.

With the GLM choice established, additive error models are then considered for completeness before selecting a final model for the region-specific comparisons.

## Other Models Considered -- Additive Error Models

### Transform Both Sides

<!-- # ```{r, message=F, warning=F, echo=F, fig.height=10, fig.width=10} -->
<!-- # ## Fit basic OLS model: CHL ~ TN -->
<!-- # fit_ols <- lm(chl ~ tn, data = lakesDat) -->
<!-- #  -->
<!-- # ## Layout: 2 small on top, 1 big on bottom -->
<!-- # layout(matrix(c(1, 2, -->
<!-- #                 3, 3), nrow = 2, byrow = TRUE)) -->
<!-- #  -->
<!-- # par(mar = c(4, 4, 3, 1), oma = c(0, 0, 0, 0)) -->
<!-- #  -->
<!-- # ## -------- 1. Residuals vs Fitted (OLS) -------- -->
<!-- # plot(fitted(fit_ols), rstudent(fit_ols), -->
<!-- #      xlab = "Fitted CHL", -->
<!-- #      ylab = "Studentized residuals", -->
<!-- #      main = "OLS: Residuals vs Fitted", -->
<!-- #      pch  = 1, col = "black") -->
<!-- # abline(h = 0, lty = 2) -->
<!-- #  -->
<!-- # ## -------- 2. Observed vs Fitted CHL -------- -->
<!-- # plot(fitted(fit_ols), lakesDat$chl, -->
<!-- #      xlab = "Fitted CHL", -->
<!-- #      ylab = "Observed CHL", -->
<!-- #      main = "Observed vs Fitted (OLS)", -->
<!-- #      pch  = 1, col = "black") -->
<!-- # abline(0, 1, lty = 2)  # 45-degree line -->
<!-- #  -->
<!-- # ## -------- 3. TN vs CHL with fitted line (big bottom plot) -------- -->
<!-- # tn_grid <- seq(min(lakesDat$tn, na.rm = TRUE), -->
<!-- #                max(lakesDat$tn, na.rm = TRUE), -->
<!-- #                length.out = 200) -->
<!-- #  -->
<!-- # pred_ols <- predict(fit_ols, -->
<!-- #                     newdata = data.frame(tn = tn_grid), -->
<!-- #                     type = "response") -->
<!-- #  -->
<!-- # plot(lakesDat$tn, lakesDat$chl, -->
<!-- #      pch = 1, col = "black", -->
<!-- #      xlab = "TN", ylab = "CHL", -->
<!-- #      main = "OLS Model: Observed vs Fitted") -->
<!-- #  -->
<!-- # lines(tn_grid, pred_ols, -->
<!-- #       lwd = 3, col = "blue") -->
<!-- #  -->
<!-- # legend("topleft", -->
<!-- #        legend = c("Observed CHL", "Fitted (OLS)"), -->
<!-- #        col    = c("black", "blue"), -->
<!-- #        pch    = c(1, NA), -->
<!-- #        lty    = c(NA, 1), -->
<!-- #        lwd    = c(NA, 3), -->
<!-- #        bty    = "n") -->
<!-- #  -->
<!-- # ## Reset layout -->
<!-- # layout(1) -->
<!-- # ``` -->

<!-- # ```{r, message=F, warning=F, echo=F, fig.height=10, fig.width=10} -->
<!-- # par(mfrow = c(3, 1), -->
<!-- #     mar = c(4, 4, 3, 1), -->
<!-- #     oma = c(0, 0, 0, 0), -->
<!-- #     xaxs = "i", yaxs = "i") -->
<!-- #  -->
<!-- # plot(y = log(lakesDat$chl), x = log(lakesDat$tn), ylab = "log(CHL)", xlab = "log(TN)", main = "Transform Both Sides (Log)") -->
<!-- # plot(y = sqrt(lakesDat$chl), x = sqrt(lakesDat$tn), ylab = "sqrt(CHL)", xlab = "sqrt(TN)", main = "Transform Both Sides (Sqrt)") -->
<!-- # plot(y = (lakesDat$chl^(1/3)), x = (lakesDat$tn^(1/3)), ylab = "CHL^1/3", xlab = "TN^1/3", main = "Transform Both Sides (Cube Root)") -->
<!-- # par(mfrow = c(1, 1)) -->
<!-- # ``` -->

<!-- # ```{r, message=F, warning=F, echo=F, fig.height=10, fig.width=10} -->
<!-- # ## Fit TBS cube-root model -->
<!-- # fit_tbs_cr <- lm(I(chl^(1/3)) ~ I(tn^(1/3)), data = lakesDat) -->
<!-- #  -->
<!-- # ## Layout: 2 small on top, 1 big on bottom -->
<!-- # layout(matrix(c(1, 2, -->
<!-- #                 3, 3), nrow = 2, byrow = TRUE)) -->
<!-- #  -->
<!-- # par(mar = c(4, 4, 3, 1), oma = c(0, 0, 0, 0)) -->
<!-- #  -->
<!-- # ## -------- 1. Residuals vs Fitted (cube-root scale) -------- -->
<!-- # plot(fitted(fit_tbs_cr), rstudent(fit_tbs_cr), -->
<!-- #      xlab = "Fitted (cube-root scale)", -->
<!-- #      ylab = "Studentized residuals", -->
<!-- #      main = "TBS (cube root): Residuals vs Fitted", -->
<!-- #      pch  = 1, col = "black") -->
<!-- # abline(h = 0, lty = 2) -->
<!-- #  -->
<!-- # ## -------- 2. Observed vs Fitted CHL -------- -->
<!-- # fitted_chl <- fitted(fit_tbs_cr)^3 -->
<!-- #  -->
<!-- # plot(fitted_chl, lakesDat$chl, -->
<!-- #      xlab = "Fitted CHL", -->
<!-- #      ylab = "Observed CHL", -->
<!-- #      main = "Observed vs Fitted (TBS cube root)", -->
<!-- #      pch  = 1, col = "black") -->
<!-- # abline(0, 1, lty = 2)  # 45-degree line -->
<!-- #  -->
<!-- # ## -------- 3. TN vs CHL with fitted curve (big bottom plot) -------- -->
<!-- # tn_grid <- seq(min(lakesDat$tn, na.rm = TRUE), -->
<!-- #                max(lakesDat$tn, na.rm = TRUE), -->
<!-- #                length.out = 200) -->
<!-- #  -->
<!-- # pred_cr <- predict(fit_tbs_cr, -->
<!-- #                    newdata = data.frame(tn = tn_grid), -->
<!-- #                    type = "response") -->
<!-- #  -->
<!-- # pred_cr_chl <- pred_cr^3 -->
<!-- #  -->
<!-- # plot(lakesDat$tn, lakesDat$chl, -->
<!-- #      pch = 1, col = "black",         # open circles here -->
<!-- #      xlab = "TN", ylab = "CHL", -->
<!-- #      main = "TBS Cube-Root Model: Observed vs Fitted") -->
<!-- #  -->
<!-- # lines(tn_grid, pred_cr_chl, -->
<!-- #       lwd = 3, col = "blue") -->
<!-- #  -->
<!-- # legend("topleft", -->
<!-- #        legend = c("Observed CHL", "Fitted (TBS cube-root)"), -->
<!-- #        col    = c("black", "blue"), -->
<!-- #        pch    = c(1, NA), -->
<!-- #        lty    = c(NA, 1), -->
<!-- #        lwd    = c(NA, 3), -->
<!-- #        bty    = "n") -->
<!-- #  -->
<!-- # ## Reset layout -->
<!-- # layout(1) -->
<!-- # ``` -->

A simple OLS fit suggests a roughly linear TN–CHL relationship, but the studentized residuals exhibit clear heteroscedasticity. This motivates considering transform both sides (TBS) additive-error models, whose primary use would be to stabilize the variance. Several variance-stabilizing transformations were evaluated, with the cube-root transformation providing the best balance between stabilizing the variance and maintaining approximate linearity.

However, TBS models introduce some drawbacks. Because inference must be back-transformed to the original scale, certain quantities of interest such as confidence intervals, differences between regions, and predictive intervals do not back-transform in a straightforward way. Nonlinear back-transformation interacts with uncertainty (via Jensen’s inequality), so intervals on the original scale require additional correction or simulation. This is to say, the TBS framework becomes substantially more cumbersome and should certainly justify its use by adding some benefit over the GLMs considered thus far.

However, because the transformed-scale residuals (again, studentized) still showed heteroscedasticity, the benefits of the TBS linear approach are limited. In contrast, the GLMs considered earlier directly model the nonconstant variance structure through an appropriate mean–variance relationship and avoid back-transformation issues. Therefore, the GLM formulations remain the more appealing modeling strategy.

### Power of the Mean

<!-- # ```{r power-setup1, message=FALSE, warning=FALSE, echo=FALSE} -->
<!-- # base_fit <- lm(chl ~ tn, data = lakesDat) -->
<!-- # mu0      <- pmax(fitted(base_fit), 0.01)  # ensure positivity -->
<!-- #  -->
<!-- # fit_power_model <- function(delta) { -->
<!-- #   w   <- 1 / (mu0^(2 * delta)) -->
<!-- #   fit <- lm(chl ~ tn, data = lakesDat, weights = w) -->
<!-- #   list(delta = delta, fit = fit) -->
<!-- # } -->
<!-- #  -->
<!-- # deltas <- c(1/4, 1/3, 1/2, 1) -->
<!-- # fits   <- lapply(deltas, fit_power_model) -->
<!-- # names(fits) <- paste0("delta_", deltas) -->
<!-- # ``` -->

<!-- # ```{r power-residuals2, echo=FALSE, message=FALSE, warning=FALSE, fig.height=8, fig.width=8} -->
<!-- # par(mfrow = c(2, 2), -->
<!-- #     mar   = c(4, 4, 3, 1), -->
<!-- #     oma   = c(0, 0, 0, 0)) -->
<!-- #  -->
<!-- # for (obj in fits) { -->
<!-- #   fit   <- obj$fit -->
<!-- #   delta <- obj$delta -->
<!-- #  -->
<!-- #   plot(fitted(fit), rstudent(fit), -->
<!-- #        pch  = 1, col = "black", -->
<!-- #        xlab = "Fitted", -->
<!-- #        ylab = "Studentized residual", -->
<!-- #        main = paste("Residuals vs Fitted\n(delta =", delta, ")")) -->
<!-- #   abline(h = 0, lty = 2) -->
<!-- # } -->
<!-- #  -->
<!-- # par(mfrow = c(1, 1)) -->
<!-- # ``` -->

<!-- # ```{r power-fitted-curves1, echo=FALSE, message=FALSE, warning=FALSE, fig.height=8, fig.width=8} -->
<!-- # tn_grid <- seq(min(lakesDat$tn, na.rm = TRUE), -->
<!-- #                max(lakesDat$tn, na.rm = TRUE), -->
<!-- #                length.out = 300) -->
<!-- # X_grid  <- cbind(1, tn_grid) -->
<!-- # -->
<!-- # predict_curve <- function(obj) { -->
<!-- #   fit   <- obj$fit -->
<!-- #   delta <- obj$delta -->
<!-- #   mu_hat <- as.vector(X_grid %*% coef(fit)) -->
<!-- #   list(delta = delta, tn = tn_grid, mu = mu_hat) -->
<!-- # } -->
<!-- # -->
<!-- # curves <- lapply(fits, predict_curve) -->
<!-- # -->
<!-- # par(mfrow = c(2, 2), -->
<!-- #     mar   = c(4, 4, 3, 1), -->
<!-- #     oma   = c(0, 0, 0, 0)) -->
<!-- # -->
<!-- # for (crv in curves) { -->
<!-- #   plot(lakesDat$tn, lakesDat$chl, -->
<!-- #        pch = 1, col = "black", -->
<!-- #        xlab = "TN", ylab = "CHL", -->
<!-- #        main = paste("Fitted Mean Curve\n(delta =", crv$delta, ")")) -->
<!-- # -->
<!-- #   lines(crv$tn, crv$mu, lwd = 3, col = "blue") -->
<!-- # } -->
<!-- # -->
<!-- # par(mfrow = c(1, 1)) -->
<!-- # ``` -->

Given the apparent nonlinearity and mean–variance relationship in the data, power of the mean (POM) additive error models were also considered. Several values of the variance-stabilizing power $\theta$ were tested, including models that allowed the mean curve itself to be nonlinear. Although $\theta = 0.5$ performed best in terms of consistency with the Box–Cox plots and visually when plotted against the observed data, its studentized (absolute and squared) residuals still indicated heteroscedasticity, and small fitted means made the weighting scheme unstable for larger $\theta$.

Overall, even the best POM combined with TBS that was considered for modelling the CHL-TN relationship failed to produce generally satisfactory variance stabilization while also failing to predict higher observed CHL values to boot. So neither the POM, TBS, nor their combination was ultimately retained as a candidate model.

## Comparing Different Models

Overall, the Gamma GLM with a cube-root link best matched the Box–Cox variance diagnostics, produced well-behaved residuals, and offered the most coherent and interpretable mean–variance structure. For such reasons, the Gamma GLM with cube-root link was used for region-specific comparisons.

## Extending to regions

### Comparing Two Regions

```{r, include=F, message=F, warning=F, echo=F}
## Helper: fit optimized GLM by region
fit_region_glm <- function(region_label) {
  dat <- subset(lakesDat, region == region_label)
  X   <- cbind(1, dat$tn)
  Y   <- dat$chl

  basic.glm(
    xmat   = X,
    y      = Y,
    link   = 8,   # log link
    random = 5,   # 5 = Gamma, 6 = InvGaussian
    pwr    = 1/3  # optimized power
  )
}

mod_gamma_13_plains <- fit_region_glm("plains")
mod_gamma_13_ozark  <- fit_region_glm("ozarks")

combine_glm_results_region <- function(mod, region_label) {
  beta_hat <- mod$estb[, 1]
  se_hat   <- sqrt(diag(mod$invinf))
  z        <- 1.96
  LCL      <- beta_hat - z * se_hat
  UCL      <- beta_hat + z * se_hat

  data.frame(
    Region   = region_label,
    Term     = c("Intercept", "TN"),
    Estimate = beta_hat,
    SE       = se_hat,
    LCL      = LCL,
    UCL      = UCL,
    check.names = FALSE
  )
}

coef_tab <- rbind(
  combine_glm_results_region(mod_gamma_13_plains, "Plains"),
  combine_glm_results_region(mod_gamma_13_ozark,  "Ozark")
)
```

```{r, message=F, warning=F, echo=F, fig.height=8, fig.width=8}
## ----- Coefficient table (unchanged) ----- ##
kableExtra::kbl(
  coef_tab,
  digits = 4,
  align  = "llrrrr",
  row.names = FALSE,
  caption = "Region-specific optimized GLM coefficients with 95\\% Wald CIs"
) |>
  kableExtra::kable_styling(
    full_width = FALSE,
    position   = "center",
    latex_options = c("hold_position", "scale_down")
  )

## ----- Predicted values table (unchanged) ----- ##
x_pred  <- c(0.4, 0.6, 0.8, 1.0)
X_pred  <- cbind(1, x_pred)
z       <- 1.96

pred_table_region <- function(mod, region_label) {
  beta_hat <- mod$estb[, 1]
  vcov_mat <- mod$invinf

  eta_hat <- as.vector(X_pred %*% beta_hat)
  se_eta  <- sqrt(rowSums((X_pred %*% vcov_mat) * X_pred))

  eta_lo <- eta_hat - z * se_eta
  eta_hi <- eta_hat + z * se_eta

  mu_hat <- exp(eta_hat)
  mu_lo  <- exp(eta_lo)
  mu_hi  <- exp(eta_hi)

  data.frame(
    Region = region_label,
    TN     = x_pred,
    Fit    = mu_hat,
    LCL    = mu_lo,
    UCL    = mu_hi
  )
}

pred_tab <- rbind(
  pred_table_region(mod_gamma_13_plains, "Plains"),
  pred_table_region(mod_gamma_13_ozark,  "Ozark")
)

kableExtra::kbl(
  pred_tab,
  digits = 4,
  align  = "llrrrr",
  row.names = FALSE,
  caption = "Predicted CHL for TN = 0.4, 0.6, 0.8, 1.0 with 95\\% CIs by region"
) |>
  kableExtra::kable_styling(
    full_width = FALSE,
    position   = "center",
    latex_options = c("hold_position", "scale_down")
  )

## ----- GLOBAL SCALES FOR PLOTS ----- ##
x_range_raw <- range(lakesDat$tn,  na.rm = TRUE)
y_range_raw <- range(lakesDat$chl, na.rm = TRUE)

pad_x <- 0.05 * diff(x_range_raw)
pad_y <- 0.05 * diff(y_range_raw)

x_range <- c(x_range_raw[1] - pad_x,
             x_range_raw[2] + pad_x)

y_range <- c(y_range_raw[1] - pad_y,
             y_range_raw[2] + pad_y)

par(mfrow = c(1, 2),
    oma = c(0, 0, 0, 0),
    mar = c(4, 4, 2, 1),
    xaxs = "i", yaxs = "i")

z95 <- qnorm(0.975)
```

```{r overlay-plot2, echo=FALSE, message=FALSE, warning=FALSE, fig.height=8, fig.width=12}
layout(matrix(c(1, 1,
                2, 3),
              nrow = 2, byrow = TRUE))

par(mar = c(4, 4, 3, 1),
    oma = c(0, 0, 0, 0),
    xaxs = "i", yaxs = "i")

## ---- GLOBAL SCALES WITH PADDING ---- ##
x_range_raw <- range(lakesDat$tn,  na.rm = TRUE)
y_range_raw <- range(lakesDat$chl, na.rm = TRUE)

pad_x <- 0.05 * diff(x_range_raw)
pad_y <- 0.05 * diff(y_range_raw)

x_range <- c(x_range_raw[1] - pad_x,
             x_range_raw[2] + pad_x)
y_range <- c(y_range_raw[1] - pad_y,
             y_range_raw[2] + pad_y)

## Colors for curves
col_plains <- "#0072B2"
col_ozark  <- "#D55E00"

## z for 95% CI
z95 <- qnorm(0.975)

## ---- PLAINS PREDICTION GRID & FIT ---- ##
dat_plains <- subset(lakesDat, region == "plains")
x_grid_p   <- seq(min(dat_plains$tn, na.rm = TRUE),
                  max(dat_plains$tn, na.rm = TRUE),
                  length.out = 200)
X_grid_p   <- cbind(1, x_grid_p)

beta_p  <- mod_gamma_13_plains$estb[, 1]
vcov_p  <- mod_gamma_13_plains$invinf

eta_p    <- X_grid_p %*% beta_p
se_eta_p <- sqrt(rowSums((X_grid_p %*% vcov_p) * X_grid_p))

eta_lo_p <- eta_p - z95 * se_eta_p
eta_hi_p <- eta_p + z95 * se_eta_p

mu_p    <- exp(eta_p)
mu_lo_p <- exp(eta_lo_p)
mu_hi_p <- exp(eta_hi_p)

## ---- OZARKS PREDICTION GRID & FIT ---- ##
dat_ozark <- subset(lakesDat, region == "ozarks")
x_grid_o  <- seq(min(dat_ozark$tn, na.rm = TRUE),
                 max(dat_ozark$tn, na.rm = TRUE),
                 length.out = 200)
X_grid_o  <- cbind(1, x_grid_o)

beta_o  <- mod_gamma_13_ozark$estb[, 1]
vcov_o  <- mod_gamma_13_ozark$invinf

eta_o    <- X_grid_o %*% beta_o
se_eta_o <- sqrt(rowSums((X_grid_o %*% vcov_o) * X_grid_o))

eta_lo_o <- eta_o - z95 * se_eta_o
eta_hi_o <- eta_o + z95 * se_eta_o

mu_o    <- exp(eta_o)
mu_lo_o <- exp(eta_lo_o)
mu_hi_o <- exp(eta_hi_o)

## ---- NEW PLOT PANEL ---- ##
# par(mfrow = c(1,1),
#     mar = c(4,4,2,1),
#     xaxs = "i", yaxs = "i")

plot(lakesDat$tn, lakesDat$chl,
     xlab = "TN", ylab = "CHL",
     main = "Optimized GLM Fits: Plains vs Ozark",
     pch  = 1, col = "gray50",
     xlim = x_range, ylim = y_range)

## ---- ADD PLAINS CURVE ---- ##
lines(x_grid_p, mu_p,    lwd = 2, col = col_plains)
lines(x_grid_p, mu_lo_p, lwd = 1, col = col_plains, lty = 3)
lines(x_grid_p, mu_hi_p, lwd = 1, col = col_plains, lty = 3)

## ---- ADD OZARK CURVE ---- ##
lines(x_grid_o, mu_o,    lwd = 2, col = col_ozark)
lines(x_grid_o, mu_lo_o, lwd = 1, col = col_ozark, lty = 3)
lines(x_grid_o, mu_hi_o, lwd = 1, col = col_ozark, lty = 3)

## ---- LEGEND ---- ##
legend("topleft",
       legend = c("Plains Mean", "Plains 95% CI",
                  "Ozark Mean",  "Ozark 95% CI"),
       col    = c(col_plains, col_plains, col_ozark, col_ozark),
       lty    = c(1, 3, 1, 3),
       lwd    = c(2, 1, 2, 1),
       bty    = "n")
# 
# par(mfrow = c(2, 1),
#     mar = c(4, 4, 3, 1),
#     oma = c(0, 0, 0, 0),
#     xaxs = "i", yaxs = "i")

## Plains
plot(mod_gamma_13_plains$vals$muhat,
     mod_gamma_13_plains$vals$stdevres,
     xlab = "Fitted (Plains)",
     ylab = "Studentized deviance residual",
     main = "Residuals vs Fitted: Plains",
     pch = 1)
abline(h = 0, lty = 2)

## Ozarks
plot(mod_gamma_13_ozark$vals$muhat,
     mod_gamma_13_ozark$vals$stdevres,
     xlab = "Fitted (Ozarks)",
     ylab = "Studentized deviance residual",
     main = "Residuals vs Fitted: Ozarks",
     pch = 1)
abline(h = 0, lty = 2)

par(mfrow = c(1, 1))
```

```{r deviance regions, echo=FALSE, message=FALSE, warning=FALSE, fig.height=8, fig.width=8}
kable(data.frame(
  Region = c("Plains", "Ozarks"),
  Scaled_Deviance = c(mod_gamma_13_plains$ests$sdev,
                      mod_gamma_13_ozark$ests$sdev)
)
)
```

Using the selected Gamma GLM with a cube-root link, models were separately fit to the Plains and Ozarks regions. The fitted curves and pointwise 95% confidence bands show a fair amount of overlap at lower TN values ($\approx$ 0.4–0.6), with very similar predicted means. At higher TN ($\approx$ 0.8–1.0), the Ozarks curve becomes noticeably steeper, and the bands begin to separate, indicating higher CHL in the Ozarks at the upper end of the TN range.

The coefficient estimates reflect this pattern: The Ozarks model has a larger slope and a slightly smaller intercept, though the 95% Wald intervals overlap for both parameters. Model diagnostics support using the same GLM form in both regions. Studentized deviance plots show no systematic patterns and are generally well-behaved, albeit with some potential outlying points. In part due to not considering likelihoods at this stage of the exam, it bears noting the potential shortfall of Wald intervals, both in terms of sample size per region (53 in Ozarks, 81 in Plains), as well as possible issues with the presumed asymptotic normality of the estimator, even though there are no clear issues present (such as parameter intervals containing values outside the parameter space). 

So overall, both regions share the same functional TN–CHL form, but the Ozarks appear to exhibit a stronger response at higher TN levels.

### Probability Assessments

For a fixed TN value $x_i$, let $Y_i$ denote the Plains CHL response and $Z_i$ the Ozarks CHL response. 

From the region-specific Gamma GLMs with cube-root link: 

$$
\mu_P(x_i) = E(Y_i \mid x_i), \qquad
\mu_O(x_i) = E(Z_i \mid x_i),
$$

with variance functions

$$
\operatorname{Var}(Y_i \mid x_i) = \phi_P \mu_P(x_i)^2, \qquad
\operatorname{Var}(Z_i \mid x_i) = \phi_O \mu_O(x_i)^2,
$$

where $\phi_P$ and $\phi_O$ are the dispersion parameters for the Plains and Ozarks models.

The target quantity is:

$$
\Pr(Y_i > Z_i \mid x_i)
$$

Note: Although the data model is Gamma, the quantity $\Pr(Y_i > Z_i)$ depends on the distribution of a difference of two independent Gamma variables, which has no closed form; thus the plug-in Normal approximation below is used for tractability.

Using a plug-in Normal approximation to the Gamma distribution,

$$
Y_i \mid x_i \approx N\big(\mu_P(x_i), \phi_P \mu_P(x_i)^2\big), \qquad
Z_i \mid x_i \approx N\big(\mu_O(x_i), \phi_O \mu_O(x_i)^2\big),
$$

and assuming conditional independence of $Y_i$ and $Z_i$ given $x_i$, a reasonable assumption given the setup, define: 

$$
D_i = Y_i - Z_i
$$

Then

$$
E(D_i \mid x_i) = \mu_P(x_i) - \mu_O(x_i),
$$
and

$$
\operatorname{Var}(D_i \mid x_i)
= \operatorname{Var}(Y_i \mid x_i) + \operatorname{Var}(Z_i \mid x_i)
= \phi_P \mu_P(x_i)^2 + \phi_O \mu_O(x_i)^2
$$

Such that 

$$
D_i \mid x_i \approx
N\Big(
\mu_P(x_i) - \mu_O(x_i),\,
\phi_P \mu_P(x_i)^2 + \phi_O \mu_O(x_i)^2
\Big),
$$

so that

$$
\Pr(Y_i > Z_i \mid x_i)
= \Pr(D_i > 0 \mid x_i)
\approx
\Phi\left(
\frac{\mu_P(x_i) - \mu_O(x_i)}
{\sqrt{\phi_P \mu_P(x_i)^2 + \phi_O \mu_O(x_i)^2}}
\right),
$$

where $\Phi$ is the standard Normal CDF.

Under the cube-root power link used in `basic.glm` (with `pwr = 1/3`),

$$
\eta(x) = \mu(x)^{1/3}, \qquad \eta(x) = x^\top \hat{\beta},
$$

so the fitted mean at covariate value $x$ is

$$
\hat{\mu}(x)
= \big(x^\top \hat{\beta}\big)^{1/(1/3)}
= \big(x^\top \hat{\beta}\big)^3
$$

For each region and each TN value $x_i$,

$$
\hat{\mu}_P(x_i) = \big((1, x_i)^\top \hat{\beta}_P\big)^3, \qquad
\hat{\mu}_O(x_i) = \big((1, x_i)^\top \hat{\beta}_O\big)^3,
$$

with corresponding dispersion estimates $\hat{\phi}_P$ and $\hat{\phi}_O$. The plug-in estimator of the desired probability is then

$$
\widehat{\Pr}(Y_i > Z_i \mid x_i)
=
\Phi\left(
\frac{\hat{\mu}_P(x_i) - \hat{\mu}_O(x_i)}
{\sqrt{\hat{\phi}_P \hat{\mu}_P(x_i)^2
+ \hat{\phi}_O \hat{\mu}_O(x_i)^2}}
\right)
$$

Note: The calculations that follow rely on the asymptotic Normal approximation to the Gamma GLM means and variances. Whether this approximation is appropriate for the sample sizes in each region should be considered.

```{r}
# Models fitted already
# mod_gamma_13_plains
# mod_gamma_13_ozark

# extract mean mu from basic.glm
predict_mu_power <- function(mod, x, pwr = 1/3) {
  # regression coefficients 
  beta_hat <- mod$estb[, 1]
  # design matrix
  X <- cbind(1, x)
  eta <- as.vector(X %*% beta_hat)
  # eta = mu^pwr
  mu <- eta^(1 / pwr)
  mu
}

# plug-in estimate
prob_YgtZ_normal <- function(x, mod_P, mod_O, pwr = 1/3) {
  # plains mean and dispersion
  mu_P <- predict_mu_power(mod_P, x, pwr = pwr)
  phi_P <- mod_P$ests$phi
  # ozarks mean and dispersion
  mu_O <- predict_mu_power(mod_O, x, pwr = pwr)
  phi_O <- mod_O$ests$phi
  # difference calculations
  mean_D <- mu_P - mu_O
  var_D <- phi_P * mu_P^2 + phi_O * mu_O^2
  sd_D <- sqrt(var_D)
  # probability 
  pnorm(mean_D / sd_D)
}

# estimate x = 0.70
prob_0.70 <- prob_YgtZ_normal(0.70,
                              mod_gamma_13_plains,
                              mod_gamma_13_ozark)
cat("Estimated probability is:", round(prob_0.70, 3), "\n")
```

Our estimate is: $\Pr(Y_i > Z_i \mid x_i = 0.70) = 0.46815$.

Now, the sequence of values $x_i \in \{0.4, 0.41, 0.42, \cdots, 1.0 \}$ is given by:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Grid x_i in {0.40, 0.41, ..., 1.00}
x_grid    <- seq(0.40, 1.00, by = 0.01)
prob_grid <- sapply(x_grid, prob_YgtZ_normal,
                    mod_P = mod_gamma_13_plains,
                    mod_O = mod_gamma_13_ozark)

# Plot
# TN on x-axis
# estimated probabilities on y-axis
plot(x_grid, prob_grid, type = "l",
     ylim = c(0.44, 0.51),
     xlab = "TN",
     ylab = "Estimated Pr",
     main = "Pr(Plains CHL > Ozarks CHL | TN))")
# reference line at 0.5
abline(h = 0.5, lty = 2)
```

So, as TN concentration increases, the probability that Plains CHL exceeds Ozarks CHL decreases (from roughly 0.5 to 0.45).

### Relation Between CHL and TN within Regions

In both regions, CHL increases with TN and the variance rises with the mean. The Gamma GLM with a cube-root link adequately captures this mean–variance relationship, with well-behaved residuals and no indication that different model forms are needed. Thus, the TN–CHL relationship has the same functional form in the Plains and Ozarks.

Fitting the model separately by region shows differences in magnitude. For TN $\approx$ 0.4–0.6, fitted curves and 95% bands nearly coincide, and predicted CHL is similar across regions. As TN increases toward 0.8–1.0, the Ozarks curve steepens and predicted CHL values in the Ozarks diverges upward; this also coincides with the confidence bands separating, and generally indicates a stronger TN response in the Ozarks at higher TN.

The coefficient estimates are consistent with this picture: both slopes are positive, with the Ozarks slope being larger. Although the 95% Wald intervals overlap and do not provide definitive parameter-wise separation, the combined evidence from curves, bands, and slopes suggests a stronger TN–CHL response in the Ozarks.

The probability assessment summarizes this difference: For moderate TN (0.4–0.7), $\Pr(\text{Plains CHL} > \text{Ozarks CHL} \mid \text{TN}) \approx 0.5$, but as TN approaches 1.0 this probability falls below 0.5, indicating that Ozarks CHL is more likely to exceed Plains CHL at higher TN.

Overall, the regions share the same basic TN–CHL form, but the Ozarks show a stronger response to increasing TN, especially near the upper end of the observed TN range.

\newpage

# Q2: SECCHI & CHL (Plains vs. Ozarks)

## Overall Distribution & Approach

```{r, message=F, warning=F, echo=F, fig.height = 10, fig.width = 10}
## Shared limits so all three panels use the same box
xlim <- range(lakesDat$chl,    na.rm = TRUE)
ylim <- range(lakesDat$secchi, na.rm = TRUE)

## Layout: 2 rows, 2 columns
## Row 1: [ 1 1 ]  (overall, full width)
## Row 2: [ 2 3 ]  (plains, ozarks)
layout(matrix(c(1, 1,
                2, 3),
              nrow = 2, byrow = TRUE))

par(mar = c(4, 4, 3, 1),
    oma = c(0, 0, 0, 0),
    xaxs = "i", yaxs = "i")

############################
## PANEL 1: OVERALL
############################
plot(lakesDat$chl, lakesDat$secchi,
     xlab = "CHL", ylab = "Secchi",
     main = "Overall Scatterplot Secchi(y) to CHL(x)",
     pch = 1, col = "black",
     xlim = xlim, ylim = ylim)

############################
## PANEL 2: PLAINS
############################
with(subset(lakesDat, region == "plains"),
     plot(chl, secchi,
          xlab = "CHL", ylab = "Secchi",
          main = "Plains",
          pch = 19, col = "#0072B2",
          xlim = xlim, ylim = ylim))

############################
## PANEL 3: OZARKS
############################
with(subset(lakesDat, region == "ozarks"),
     plot(chl, secchi,
          xlab = "CHL", ylab = "Secchi",
          main = "Ozarks",
          pch = 19, col = "#D55E00",
          xlim = xlim, ylim = ylim))

## Reset layout
layout(1)
```

<!-- # ```{r, message=F, warning=F, echo=F} -->
<!-- # hist(lakesDat$secchi, freq = T, xlab = "CHL", main = "Marginal Distribution of Secchi") -->
<!-- # ``` -->

We start by examining the scatterplot of Secchi depth versus CHL and also examine the scatterplot by region. Secchi depth is right-skewed, consistent with ecological expectations, and shows a clear negative, potentially nonlinear relationship with CHL, with variance possibly decreasing at larger CHL values. By region, the same basic pattern is also shown, while still suggesting possible differences in the strength of the CHL-Secchi relationship

Let the random variable $Y_i$ denote the Secchi depth for lake $i$ ($i = 1,\cdots,134$), with corresponding CHL covariate value $x_i$. Because each lake appears only once, it is reasonable to treat observations as independent.

Generally, the modeling strategy mirrors Part I: We first identify an appropriate overall model for Secchi as a function of CHL, then fit the same model structure separately to the Plains and Ozarks. Under this approach then, differences in intercept, slope, curvature, or dispersion may be interpreted as potential regional differences in the CHL–Secchi relationship rather than artifacts of using different model families or transformations. 

Henceforth, for brevity, I omit explicit “as in Part I” references, though the underlying modeling logic is the same.

## Generalized Linear Model

```{r, message=F, warning=F, echo=F}
bc1 <- boxcox1(list(x = lakesDat$chl, y = lakesDat$secchi), nbins = 12)
bc2 <- boxcox2(list(x = lakesDat$chl, y = lakesDat$secchi), nbins = 12)
plot(log(bc1$m), log(bc1$v), main = "Box-Cox, 12 Equal-Count Bins", xlab = "log(Mean)", ylab = "log(Sd)")
fit <- lm(log(v) ~ log(m), data = bc1)
abline(fit, col = "red", lwd = 2)

nbins_vec <- c(6, 10, 12, 14, 16, 18, 22)

extract_slope_theta <- function(k) {
  bc_count  <- boxcox1(list(x = lakesDat$chl, y = lakesDat$secchi), nbins = k)  # equal-count
  bc_spaced <- boxcox2(list(x = lakesDat$chl, y = lakesDat$secchi), nbins = k)  # equal-spaced

  slope_count  <- coef(lm(log(v) ~ log(m), data = bc_count))[["log(m)"]]
  slope_spaced <- coef(lm(log(v) ~ log(m), data = bc_spaced))[["log(m)"]]

  data.frame(
    nbins = k,
    `Equal-Count`  = round(slope_count,  2),
    `Equal-Spaced` = round(slope_spaced, 2)
  )
}

bc_table <- do.call(rbind, lapply(nbins_vec, extract_slope_theta))
kable(bc_table)
```

We first consider GLMs, and begin by identifying a suitable random component. Using `boxcoxfctns`, Box–Cox plots and slopes were calculated; across both equal-count and equal-width binning schemes, and across variable number of bins, the slopes ranged from 1.6 to 2.1. This indicates that $\operatorname{Var}(Secchi)$ grows roughly like $\mu^{2}$–$\mu^{3}$, which is consistent with Gamma or Inverse Gaussian random components.

With the random component thus narrowed to Gamma or IG, the next step is to select an appropriate link function for modeling the CHL–Secchi mean relationship.

```{r, message=F, warning=F, echo=F, fig.height=8, fig.width=8}
op <- par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))

# plot(y = log(lakesDat$secchi), x = lakesDat$chl, xlab = "CHL", ylab ="log(Secchi)", main = "Transform Response Log")
# plot(y = -log(-log(lakesDat$secchi)), x = lakesDat$chl, xlab = "CHL", ylab ="log(Secchi)", main = "Transform Response Log-Log")
# plot(y = log(-log(1-lakesDat$secchi)), x = lakesDat$chl, xlab = "CHL", ylab ="log(Secchi)", main = "Transform Response Complementary Log-Log")

plot(y = 1/(lakesDat$secchi), x = lakesDat$chl, xlab = "CHL", ylab ="1/Secchi",  main = "Transform Response Inverse")
plot(y = 1/(lakesDat$secchi^2), x = lakesDat$chl, xlab = "CHL", ylab ="1/Secchi^2",  main = "Transform Response Squared Inverse")
# plot(y = 1/sqrt(lakesDat$secchi), x = lakesDat$chl, xlab = "CHL", ylab ="1/sqrt(Secchi)",  main = "Transform Response Sqrt Inverse")

# plot(y = sqrt(lakesDat$secchi), x = lakesDat$chl, xlab = "CHL", ylab ="sqrt(Secchi)",  main = "Transform Response Sqrt")
# plot(y = (lakesDat$secchi)^(2/3), x = lakesDat$chl, xlab = "CHL", ylab ="Secchi^2/3",  main = "Transform Response 2/3rd root")
# plot(y = (lakesDat$secchi)^(1/3), x = lakesDat$chl, xlab = "CHL", ylab ="Secchi^1/3",  main = "Transform Response 1/3rd root")
# plot(y = (lakesDat$secchi), x = lakesDat$chl, xlab = "CHL", ylab ="Secchi",  main = "Transform Response Identity")
op <- par(mfrow = c(1, 1), mar = c(4, 4, 2, 1))
```

To identify a suitable link function, we seek a transformation of the mean that both linearizes the Secchi–CHL relationship and preserves the expected monotonically decreasing pattern. Exploratory transformation plots indicate that an inverse transformation of the mean yields an approximately linear trend, with the square-inverse transformation performing similarly well. Importantly, these transformations are used only to guide link selection, and under no circumstance should they been seen as justification for transforming the random variable for Secchi directly.

Combined with the earlier mean–variance assessment, this motivates fitting Gamma and Inverse Gaussian GLMs ($\theta = 2, 3$) paired with inverse and square-inverse links. These candidate models are then compared.

```{r, include=F, message=F, warning=F, echo=F}
X <- cbind(1, lakesDat$chl)
Y <- lakesDat$secchi

ok <- is.finite(Y) & is.finite(X[,2]) & (Y > 0)
X_use <- X[ok, , drop = FALSE]
Y_use <- Y[ok]

start_fit <- glm(
  Y_use ~ X_use[,2],
  family = inverse.gaussian(link = "1/mu^2")
)

startb_inv <- coef(start_fit)  # (Intercept, CHL)
# startb_inv

start_fit2 <- glm(
  Y_use ~ X_use[,2],
  family = Gamma(link = "1/mu^2")
)

startb_g <- coef(start_fit2)  # (Intercept, CHL)
# startb_g
```

```{r, include=F, message=F, warning=F, echo=F}
X <- cbind(1, lakesDat$chl)
Y <- lakesDat$secchi

mod_invgauss_inv <- basic.glm(
  xmat   = X,
  y      = Y,
  link   = 6,
  random = 6
)

# mod_invgauss_inv2 <- basic.glm(
#   xmat   = X,
#   y      = Y,
#   link   = 7,
#   random = 6, 
#   startb = startb_inv
# )

mod_gamma_inv <- basic.glm(
  xmat   = X,
  y      = Y,
  link   = 6,
  random = 5
)

# mod_gamma_inv2 <- basic.glm(
#   xmat   = X,
#   y      = Y,
#   link   = 7,
#   random = 5, 
#   startb = startb_g
# )

combine_glm_results <- function(mod, model_name) {
  beta_hat <- mod$estb[, 1]
  se_hat <- sqrt(diag(mod$invinf))
  z <- 1.96
  LCL <- beta_hat - z * se_hat
  UCL <- beta_hat + z * se_hat

  phi <- mod$ests$phi
  # unscaled deviance
  udev <- mod$ests$udev
  sdev <- mod$ests$sdev
  # scaled deviance
  # sdev <- udev / phi

  data.frame(
    Term        = c("Intercept", "Body Mass"),
    Estimate    = beta_hat,
    SE          = se_hat,
    Phi         = phi,
    UnscaledDev = udev,
    ScaledDev   = sdev,
    LogLik      = mod$ests$loglik.fitted,
    LCL         = LCL,
    UCL         = UCL,
    Model       = model_name,
    check.names = FALSE
  )
}

tab_combined <- rbind(
  combine_glm_results(mod_invgauss_inv, "Inverse Gaussian (inverse link)"),
  # combine_glm_results(mod_invgauss_inv2, "Inverse Gaussian 1/3rd link"),
  combine_glm_results(mod_gamma_inv,    "Gamma (inverse link)")
  # combine_glm_results(mod_gamma_inv2, "Gamma 1/3rd link")
)

kableExtra::kbl(
  tab_combined,
  digits = 4,
  align  = "lrrrrrrrrl",
  row.names = FALSE,
  caption = "Model comparison with Wald 95\\% CIs and both unscaled and scaled deviances"
) |>
  kableExtra::kable_styling(
    full_width = FALSE,
    position   = "center",
    latex_options = c("hold_position", "scale_down")
  )
```

Several GLMs were considered, including the Inverse Gaussian model with its canonical inverse-squared link. Even after generating reasonable starting values from a preliminary `glm()` fit and restricting initial linear predictors to be positive, the `basic.glm` routine failed to converge because iterations quickly produced invalid $\eta$ values. Due to this numerical instability, only a subset of models could be reliably fitted.

The Gamma GLM with inverse link and the Inverse Gaussian GLM with inverse link both converged and exhibited stable behavior. These models were therefore carried forward for comparison and then evaluated them using fitted-curve overlays on the scatterplot of the observed in addition to deviance residual analysis. 

```{r, message=F, warning=F, echo=F, fig.height=10, fig.width=10}
layout(matrix(c(1, 2,
                3, 4),
              nrow = 2, byrow = TRUE))

par(mar = c(4, 4, 3, 1),
    oma = c(0, 0, 0, 0),
    xaxs = "i", yaxs = "i")

## ----- Setup ----- ##
x_grid <- seq(min(lakesDat$tn), max(lakesDat$tn), length.out = 200)
X_grid <- cbind(1, x_grid)
z90 <- qnorm(0.95)

## ----- Setup ----- ##
x_grid <- seq(min(lakesDat$chl), max(lakesDat$chl), length.out = 200)
X_grid <- cbind(1, x_grid)
z90    <- qnorm(0.95)


## =========================
##  GAMMA MODEL (inverse link)
## =========================

beta_g <- mod_gamma_inv$estb[, 1]
V_g    <- mod_gamma_inv$invinf

eta_g     <- as.vector(X_grid %*% beta_g)
se_eta_g  <- sqrt(rowSums((X_grid %*% V_g) * X_grid))

eta_lo_g  <- eta_g - z90 * se_eta_g
eta_hi_g  <- eta_g + z90 * se_eta_g

## inverse link: mu = 1 / eta  (monotone decreasing)
mu_g    <- 1 / eta_g
mu_lo_g <- 1 / eta_hi_g   # swap hi/lo
mu_hi_g <- 1 / eta_lo_g


## =========================
##  INVERSE GAUSSIAN MODEL (inverse link)
## =========================

beta_ig <- mod_invgauss_inv$estb[, 1]
V_ig    <- mod_invgauss_inv$invinf

eta_ig     <- as.vector(X_grid %*% beta_ig)
se_eta_ig  <- sqrt(rowSums((X_grid %*% V_ig) * X_grid))

eta_lo_ig  <- eta_ig - z90 * se_eta_ig
eta_hi_ig  <- eta_ig + z90 * se_eta_ig

mu_ig    <- 1 / eta_ig
mu_lo_ig <- 1 / eta_hi_ig   # swap hi/lo
mu_hi_ig <- 1 / eta_lo_ig


## =========================
##  TWO SIDE-BY-SIDE PLOTS
## =========================

# par(mfrow = c(1, 2),
#     oma  = c(0, 0, 0, 0),
#     mar  = c(4, 4, 2, 1),
#     xaxs = "i", yaxs = "i")

## ----- Left panel: Gamma ----- ##
plot(lakesDat$chl, lakesDat$secchi,
     pch = 1, col = "black",
     xlab = "CHL", ylab = "Secchi",
     main = "Gamma Model (90% CI)")

lines(x_grid, mu_g,    lwd = 2, col = "black")
lines(x_grid, mu_lo_g, lwd = 1, col = "black", lty = 3)
lines(x_grid, mu_hi_g, lwd = 1, col = "black", lty = 3)

legend("topleft",
       legend = c("Gamma mean", "Gamma 90% band"),
       col    = c("black", "black"),
       lty    = c(1, 3),
       lwd    = c(2, 1),
       bty    = "n")

## ----- Right panel: Inverse Gaussian ----- ##
plot(lakesDat$chl, lakesDat$secchi,
     pch = 1, col = "black",
     xlab = "CHL", ylab = "Secchi",
     main = "Inverse Gaussian Model (90% CI)")

lines(x_grid, mu_ig,    lwd = 2, col = "blue")
lines(x_grid, mu_lo_ig, lwd = 1, col = "blue", lty = 3)
lines(x_grid, mu_hi_ig, lwd = 1, col = "blue", lty = 3)

legend("topleft",
       legend = c("InvGaussian mean", "InvGaussian 90% band"),
       col    = c("blue", "blue"),
       lty    = c(1, 3),
       lwd    = c(2, 1),
       bty    = "n")

# GAMMA
mu_g       <- mod_gamma_inv$vals$muhat
res_g_p    <- mod_gamma_inv$vals$pearsonres
res_g_dev  <- mod_gamma_inv$vals$devres
res_g_std  <- mod_gamma_inv$vals$stdevres

# INVGAUSSIAN
mu_ig      <- mod_invgauss_inv$vals$muhat
res_ig_p   <- mod_invgauss_inv$vals$pearsonres
res_ig_dev <- mod_invgauss_inv$vals$devres
res_ig_std <- mod_invgauss_inv$vals$stdevres
# 
# par(mfrow = c(2, 2),
#     mar = c(4, 4, 3, 1),
#     oma = c(0, 0, 0, 0),
#     xaxs = "i", yaxs = "i")

# plot(mu_g, res_g_dev,
#      xlab = "Fitted (Gamma)", ylab = "Deviance residual",
#      main = "Gamma: Deviance Residuals")
# abline(h = 0, lty = 2)
# 
# plot(mu_ig, res_ig_dev,
#      xlab = "Fitted (InvGaussian)", ylab = "Deviance residual",
#      main = "InvGaussian: Deviance Residuals")
# abline(h = 0, lty = 2)

# par(mfrow = c(1, 1))

# par(mfrow = c(1, 2))

plot(mu_g, res_g_std,
     xlab = "Fitted (Gamma)", ylab = "Std. Deviance residual",
     main = "Gamma: Std. Deviance Residuals")
abline(h = 0, lty = 2)

plot(mu_ig, res_ig_std,
     xlab = "Fitted (InvGaussian)", ylab = "Std. Deviance residual",
     main = "InvGaussian: Std. Deviance Residuals")
abline(h = 0, lty = 2)

par(mfrow = c(1, 1))

# par(mfrow = c(1, 2))
#
# qqnorm(res_g_std, main = "Gamma: Std Deviance QQ Plot")
# qqline(res_g_std)
#
# qqnorm(res_ig_std, main = "InvGaussian: Std Deviance QQ Plot")
# qqline(res_ig_std)
#
# par(mfrow = c(1, 1))
```

The fitted curves from the Gamma and Inverse Gaussian GLMs are visually similar, and the scatterplot alone does not clearly favor one model. Although the Inverse Gaussian model appears to reflect the increased variability at low CHL values (CHL < 10), its fitted mean curve misses most observations in this range, indicating a poorer fit to the central trend.

Deviance residual diagnostics provide more discerning evidence to compare the two models. The Inverse Gaussian deviance residuals show more pronounced heteroscedasticity and noticeable asymmetry compared to the Gamma model. In contrast, the Gamma GLM with an inverse link produces fairly symmetric and homoscedastic deviance residuals. These patterns persist when using studentized residuals, which ultimately drove the model choice, though there are some potential outliers in both model fits.

On this basis, the Gamma GLM with inverse link is selected as the preferred model. With this GLM established, several additive error models were then assessed for completeness before identifying a single working model for the region-specific comparisons.

## Other Models Considered -- Additive Error Models

### Transform Both Sides 

<!-- # ```{r, message=F, warning=F, echo=F, fig.height=10, fig.width=10} -->
<!-- # ## Fit basic OLS model: Secchi ~ CHL -->
<!-- # fit_ols_secchi <- lm(secchi ~ chl, data = lakesDat) -->
<!-- #  -->
<!-- # ## Layout: 2 small on top, 1 big on bottom -->
<!-- # layout(matrix(c(1, 2, -->
<!-- #                 3, 3), nrow = 2, byrow = TRUE)) -->
<!-- #  -->
<!-- # par(mar = c(4, 4, 3, 1), oma = c(0, 0, 0, 0)) -->
<!-- #  -->
<!-- # ## -------- 1. Residuals vs Fitted (OLS) -------- -->
<!-- # plot(fitted(fit_ols_secchi), rstudent(fit_ols_secchi), -->
<!-- #      xlab = "Fitted Secchi", -->
<!-- #      ylab = "Studentized residuals", -->
<!-- #      main = "OLS: Residuals vs Fitted (Secchi ~ CHL)", -->
<!-- #      pch  = 1, col = "black") -->
<!-- # abline(h = 0, lty = 2) -->
<!-- #  -->
<!-- # ## -------- 2. Observed vs Fitted Secchi -------- -->
<!-- # plot(fitted(fit_ols_secchi), lakesDat$secchi, -->
<!-- #      xlab = "Fitted Secchi", -->
<!-- #      ylab = "Observed Secchi", -->
<!-- #      main = "Observed vs Fitted (OLS)", -->
<!-- #      pch  = 1, col = "black") -->
<!-- # abline(0, 1, lty = 2)  # 45-degree line -->
<!-- #  -->
<!-- # ## -------- 3. CHL vs Secchi with fitted line (big bottom plot) -------- -->
<!-- # chl_grid <- seq(min(lakesDat$chl, na.rm = TRUE), -->
<!-- #                 max(lakesDat$chl, na.rm = TRUE), -->
<!-- #                 length.out = 200) -->
<!-- #  -->
<!-- # pred_ols_secchi <- predict(fit_ols_secchi, -->
<!-- #                            newdata = data.frame(chl = chl_grid), -->
<!-- #                            type = "response") -->
<!-- #  -->
<!-- # plot(lakesDat$chl, lakesDat$secchi, -->
<!-- #      pch = 1, col = "black", -->
<!-- #      xlab = "CHL", ylab = "Secchi", -->
<!-- #      main = "OLS Model: Observed vs Fitted") -->
<!-- #  -->
<!-- # lines(chl_grid, pred_ols_secchi, -->
<!-- #       lwd = 3, col = "blue") -->
<!-- #  -->
<!-- # legend("topright", -->
<!-- #        legend = c("Observed Secchi", "Fitted (OLS)"), -->
<!-- #        col    = c("black", "blue"), -->
<!-- #        pch    = c(1, NA), -->
<!-- #        lty    = c(NA, 1), -->
<!-- #        lwd    = c(NA, 3), -->
<!-- #        bty    = "n") -->
<!-- #  -->
<!-- # ## Reset layout -->
<!-- # layout(1) -->
<!-- # ``` -->

<!-- # ```{r, message=F, warning=F, echo=F, fig.height=10, fig.width=10} -->
<!-- # par(mfrow = c(3, 1), -->
<!-- #     mar = c(4, 4, 3, 1), -->
<!-- #     oma = c(0, 0, 0, 0), -->
<!-- #     xaxs = "i", yaxs = "i") -->
<!-- #  -->
<!-- # plot(y = log(lakesDat$secchi), x = log(lakesDat$chl), ylab = "log(secchi)", xlab = "log(chl)", main = "Transform Both Sides (Log)") -->
<!-- # plot(y = sqrt(lakesDat$secchi), x = sqrt(lakesDat$chl), ylab = "sqrt(secchi)", xlab = "sqrt(chl)", main = "Transform Both Sides (Sqrt)") -->
<!-- # plot(y = (lakesDat$secchi^(1/3)), x = sqrt(lakesDat$chl), ylab = "secchi^1/3", xlab = "chl^1/3", main = "Transform Both Sides (Cube Root)") -->
<!-- #  -->
<!-- # par(mfrow = c(1, 1)) -->
<!-- # ``` -->

<!-- # ```{r, message=F, warning=F, echo=F, fig.height=10, fig.width=10} -->
<!-- # par(mfrow = c(3, 1), -->
<!-- #     mar = c(4, 4, 3, 1), -->
<!-- #     oma = c(0, 0, 0, 0), -->
<!-- #     xaxs = "i", yaxs = "i") -->
<!-- #  -->
<!-- # plot(y = 1/(lakesDat$secchi), x = 1/(lakesDat$chl), ylab = "1/(secchi)", xlab = "1/(chl)", main = "Transform Both Sides (Inverse)") -->
<!-- # plot(y = 1/sqrt(lakesDat$secchi), x = 1/sqrt(lakesDat$chl), ylab = "sqrt(secchi)", xlab = "sqrt(chl)", main = "Transform Both Sides (Inverse Sqrt.)") -->
<!-- # plot(y = 1/(lakesDat$secchi^(2)), x = 1/(lakesDat$chl^(2)), ylab = "secchi^1/3", xlab = "chl^1/3", main = "Transform Both Sides (Inverse Sq.)") -->
<!-- # par(mfrow = c(1, 1)) -->
<!-- # ``` -->

<!-- # ```{r, message=F, warning=F, echo=F, fig.height=10, fig.width=10} -->
<!-- # ## Fit TBS inverse model: 1/secchi ~ 1/chl -->
<!-- # fit_tbs_inv <- lm(I(1/secchi) ~ I(1/chl), data = lakesDat) -->
<!-- #  -->
<!-- # ## Layout: 2 small on top, 1 big on bottom -->
<!-- # layout(matrix(c(1, 2, -->
<!-- #                 3, 3), nrow = 2, byrow = TRUE)) -->
<!-- #  -->
<!-- # par(mar = c(4, 4, 3, 1), oma = c(0, 0, 0, 0)) -->
<!-- #  -->
<!-- # ## -------- 1. Residuals vs Fitted (inverse scale) -------- -->
<!-- # plot(fitted(fit_tbs_inv), rstudent(fit_tbs_inv), -->
<!-- #      xlab = "Fitted (1 / Secchi scale)", -->
<!-- #      ylab = "Studentized residuals", -->
<!-- #      main = "TBS (inverse): Residuals vs Fitted", -->
<!-- #      pch  = 1, col = "black") -->
<!-- # abline(h = 0, lty = 2) -->
<!-- #  -->
<!-- # ## -------- 2. Observed vs Fitted Secchi -------- -->
<!-- # ## Back-transform: if z = 1/secchi, then secchi = 1/z -->
<!-- # fitted_secchi <- 1 / fitted(fit_tbs_inv) -->
<!-- #  -->
<!-- # plot(fitted_secchi, lakesDat$secchi, -->
<!-- #      xlab = "Fitted Secchi", -->
<!-- #      ylab = "Observed Secchi", -->
<!-- #      main = "Observed vs Fitted (TBS inverse)", -->
<!-- #      pch  = 1, col = "black") -->
<!-- # abline(0, 1, lty = 2)  # 45-degree line -->
<!-- #  -->
<!-- # ## -------- 3. CHL vs Secchi with fitted curve (big bottom plot) -------- -->
<!-- # chl_grid <- seq(min(lakesDat$chl, na.rm = TRUE), -->
<!-- #                 max(lakesDat$chl, na.rm = TRUE), -->
<!-- #                 length.out = 200) -->
<!-- #  -->
<!-- # ## Need to pass 'chl' since model has I(1/chl) -->
<!-- # pred_inv <- predict(fit_tbs_inv, -->
<!-- #                     newdata = data.frame(chl = chl_grid), -->
<!-- #                     type = "response") -->
<!-- #  -->
<!-- # ## Back-transform to Secchi -->
<!-- # pred_inv_secchi <- 1 / pred_inv -->
<!-- #  -->
<!-- # plot(lakesDat$chl, lakesDat$secchi, -->
<!-- #      pch = 1, col = "black", -->
<!-- #      xlab = "CHL", ylab = "Secchi", -->
<!-- #      main = "TBS Inverse Model: Observed vs Fitted") -->
<!-- #  -->
<!-- # lines(chl_grid, pred_inv_secchi, -->
<!-- #       lwd = 3, col = "blue") -->
<!-- #  -->
<!-- # legend("topleft", -->
<!-- #        legend = c("Observed Secchi", "Fitted (TBS inverse)"), -->
<!-- #        col    = c("black", "blue"), -->
<!-- #        pch    = c(1, NA), -->
<!-- #        lty    = c(NA, 1), -->
<!-- #        lwd    = c(NA, 3), -->
<!-- #        bty    = "n") -->
<!-- #  -->
<!-- # ## Reset layout -->
<!-- # layout(1) -->
<!-- # ``` -->

<!-- # ```{r, message=F, warning=F, echo=F, fig.height=10, fig.width=10} -->
<!-- # ## Fit TBS cube-root model: secchi^(1/3) ~ chl^(1/3) -->
<!-- #  -->
<!-- # fit_tbs_cr <- lm(I(secchi^(1/3)) ~ I(chl^(1/3)), data = lakesDat) -->
<!-- #  -->
<!-- # ## Layout: 2 small on top, 1 big on bottom -->
<!-- #  -->
<!-- # layout(matrix(c(1, 2, -->
<!-- # 3, 3), nrow = 2, byrow = TRUE)) -->
<!-- #  -->
<!-- # par(mar = c(4, 4, 3, 1), oma = c(0, 0, 0, 0)) -->
<!-- #  -->
<!-- # ## -------- 1. Residuals vs Fitted (cube-root scale) -------- -->
<!-- #  -->
<!-- # plot(fitted(fit_tbs_cr), rstudent(fit_tbs_cr), -->
<!-- # xlab = "Fitted (cube-root Secchi scale)", -->
<!-- # ylab = "Studentized residuals", -->
<!-- # main = "TBS (cube root): Residuals vs Fitted", -->
<!-- # pch  = 1, col = "black") -->
<!-- # abline(h = 0, lty = 2) -->
<!-- #  -->
<!-- # ## -------- 2. Observed vs Fitted Secchi -------- -->
<!-- #  -->
<!-- # ## Back-transform: if z = secchi^(1/3), then secchi = z^3 -->
<!-- #  -->
<!-- # fitted_secchi <- fitted(fit_tbs_cr)^3 -->
<!-- #  -->
<!-- # plot(fitted_secchi, lakesDat$secchi, -->
<!-- # xlab = "Fitted Secchi", -->
<!-- # ylab = "Observed Secchi", -->
<!-- # main = "Observed vs Fitted (TBS cube root)", -->
<!-- # pch  = 1, col = "black") -->
<!-- # abline(0, 1, lty = 2)  # 45-degree line -->
<!-- #  -->
<!-- # ## -------- 3. CHL vs Secchi with fitted curve (big bottom plot) -------- -->
<!-- #  -->
<!-- # chl_grid <- seq(min(lakesDat$chl, na.rm = TRUE), -->
<!-- # max(lakesDat$chl, na.rm = TRUE), -->
<!-- # length.out = 200) -->
<!-- #  -->
<!-- # ## Need to pass 'chl' since model has I(chl^(1/3)) -->
<!-- #  -->
<!-- # pred_cr <- predict(fit_tbs_cr, -->
<!-- # newdata = data.frame(chl = chl_grid), -->
<!-- # type = "response") -->
<!-- #  -->
<!-- # ## Back-transform to Secchi -->
<!-- #  -->
<!-- # pred_cr_secchi <- pred_cr^3 -->
<!-- #  -->
<!-- # plot(lakesDat$chl, lakesDat$secchi, -->
<!-- # pch = 1, col = "black", -->
<!-- # xlab = "CHL", ylab = "Secchi", -->
<!-- # main = "TBS Cube-Root Model: Observed vs Fitted") -->
<!-- #  -->
<!-- # lines(chl_grid, pred_cr_secchi, -->
<!-- # lwd = 3, col = "blue") -->
<!-- #  -->
<!-- # legend("topleft", -->
<!-- # legend = c("Observed Secchi", "Fitted (TBS cube root)"), -->
<!-- # col    = c("black", "blue"), -->
<!-- # pch    = c(1, NA), -->
<!-- # lty    = c(NA, 1), -->
<!-- # lwd    = c(NA, 3), -->
<!-- # bty    = "n") -->
<!-- #  -->
<!-- # ## Reset layout -->
<!-- #  -->
<!-- # layout(1) -->
<!-- # ``` -->

We begin by fitting a simple linear regression of Secchi on CHL. Although the linear trend is directionally fitting, the studentized residuals show heteroscedasticity, motivating consideration (but not necessarily adoption) of a TBS additive error model.

Several variance–stabilizing transformations were evaluated, with the inverse and cube-root transformations appearing most promising based on exploratory plots. The purpose of these transformations is not to model Secchi on a transformed scale per se, but to obtain an additive-error model with approximately constant variance.

The resulting TBS fits, however, indicate that this approach is not suitable for the Secchi–CHL relationship. Even under the best-performing transformations, the fitted curves fail to capture the pattern in the data, and the studentized residuals retain noticeable patterns and heteroscedasticity. Also, the cube-root transformation performs better than the inverse transformation while avoiding instability near very small Secchi values, though substantial funnel patterns remain.

Overall, the TBS additive error models provide a poorer fit than the GLMs and do not adequately account for the nonlinear mean–variance structure. For this reason, they are not competitive as working models, though they offer some guidance for selecting transformations within POM models considered next.

### Power of the Mean 

<!-- # ```{r, message=F, warning=F, echo=F} -->
<!-- # ## Single Box–Cox plot (equal-count bins, k = 12) -->
<!-- # bc1 <- boxcox1(list(x = lakesDat$chl, y = lakesDat$secchi), nbins = 12) -->
<!-- # bc2 <- boxcox2(list(x = lakesDat$chl, y = lakesDat$secchi), nbins = 12) -->
<!-- #  -->
<!-- # plot(log(bc1$m), log(bc1$v), -->
<!-- #      main = "Box-Cox, 12 Equal-Count Bins", -->
<!-- #      xlab = "log(Mean)", ylab = "log(Sd)") -->
<!-- #  -->
<!-- # fit_bc1 <- lm(log(v) ~ log(m), data = bc1) -->
<!-- # abline(fit_bc1, col = "red", lwd = 2) -->
<!-- #  -->
<!-- # delta_hat_12 <- coef(fit_bc1)[["log(m)"]]   # this is deltâ for k = 12 -->
<!-- #  -->
<!-- #  -->
<!-- # ## Table of deltâ for multiple nbins, equal-count vs equal-spaced -->
<!-- # nbins_vec <- c(6, 10, 12, 14, 16, 18, 22) -->
<!-- #  -->
<!-- # extract_delta <- function(k) { -->
<!-- #   # equal-count bins -->
<!-- #   bc_count  <- boxcox1(list(x = lakesDat$chl, y = lakesDat$secchi), nbins = k) -->
<!-- #   # equal-spaced bins -->
<!-- #   bc_spaced <- boxcox2(list(x = lakesDat$chl, y = lakesDat$secchi), nbins = k) -->
<!-- #  -->
<!-- #   delta_count  <- coef(lm(log(v) ~ log(m), data = bc_count))[["log(m)"]] -->
<!-- #   delta_spaced <- coef(lm(log(v) ~ log(m), data = bc_spaced))[["log(m)"]] -->
<!-- #  -->
<!-- #   data.frame( -->
<!-- #     nbins        = k, -->
<!-- #     `Equal-Count delta`  = round(delta_count,  3), -->
<!-- #     `Equal-Spaced delta` = round(delta_spaced, 3) -->
<!-- #   ) -->
<!-- # } -->
<!-- #  -->
<!-- # bc_delta_table <- do.call(rbind, lapply(nbins_vec, extract_delta)) -->
<!-- #  -->
<!-- # kable(bc_delta_table, -->
<!-- #       caption = "Estimated delta (power of the mean) from Box-Cox SD–Mean plots") -->
<!-- # ``` -->

<!-- # ```{r power-setup2, message=FALSE, warning=FALSE, echo=FALSE} -->
<!-- # ## Base fit to get mu0 for the power of the mean variance model -->
<!-- # base_fit_secchi <- lm(secchi ~ chl, data = lakesDat) -->
<!-- # mu0_secchi      <- pmax(fitted(base_fit_secchi), 0.01)  # ensure positivity -->
<!-- #  -->
<!-- # fit_power_model_secchi <- function(delta) { -->
<!-- #   ## Var(Y | X) ∝ mu^(2 * delta), weights = 1 / mu^(2 * delta) -->
<!-- #   w   <- 1 / (mu0_secchi^(2 * delta)) -->
<!-- #   fit <- lm(secchi ~ chl, data = lakesDat, weights = w) -->
<!-- #   list(delta = delta, fit = fit) -->
<!-- # } -->
<!-- #  -->
<!-- # ## Use larger deltas: 1.5, 2, 2.5, 3 -->
<!-- # deltas_secchi <- c(1.5, 2, 2.5, 3) -->
<!-- # secchi_fits   <- lapply(deltas_secchi, fit_power_model_secchi) -->
<!-- # names(secchi_fits) <- paste0("delta_", deltas_secchi) -->
<!-- # ``` -->

<!-- # ```{r power-residuals3, echo=FALSE, message=FALSE, warning=FALSE, fig.height=8, fig.width=8} -->
<!-- # par(mfrow = c(2, 2), -->
<!-- #     mar   = c(4, 4, 3, 1), -->
<!-- #     oma   = c(0, 0, 0, 0)) -->
<!-- #  -->
<!-- # for (obj in secchi_fits) { -->
<!-- #   fit   <- obj$fit -->
<!-- #   delta <- obj$delta -->
<!-- #  -->
<!-- #   plot(fitted(fit), rstudent(fit), -->
<!-- #        pch  = 1, col = "black", -->
<!-- #        xlab = "Fitted Secchi", -->
<!-- #        ylab = "Studentized residual", -->
<!-- #        main = paste("Residuals vs Fitted\n(delta =", delta, ")")) -->
<!-- #   abline(h = 0, lty = 2) -->
<!-- # } -->
<!-- #  -->
<!-- # par(mfrow = c(1, 1)) -->
<!-- # ``` -->

<!-- # ```{r power-fitted-curves2, echo=FALSE, message=FALSE, warning=FALSE, fig.height=8, fig.width=8} -->
<!-- # chl_grid <- seq(min(lakesDat$chl, na.rm = TRUE), -->
<!-- #                 max(lakesDat$chl, na.rm = TRUE), -->
<!-- #                 length.out = 300) -->
<!-- # X_grid_secchi <- cbind(1, chl_grid) -->
<!-- #  -->
<!-- # predict_curve_secchi <- function(obj) { -->
<!-- #   fit    <- obj$fit -->
<!-- #   delta  <- obj$delta -->
<!-- #   mu_hat <- as.vector(X_grid_secchi %*% coef(fit))  # fitted mean secchi -->
<!-- #   list(delta = delta, chl = chl_grid, mu = mu_hat) -->
<!-- # } -->
<!-- #  -->
<!-- # secchi_curves <- lapply(secchi_fits, predict_curve_secchi) -->
<!-- #  -->
<!-- # par(mfrow = c(2, 2), -->
<!-- #     mar   = c(4, 4, 3, 1), -->
<!-- #     oma   = c(0, 0, 0, 0)) -->
<!-- #  -->
<!-- # for (crv in secchi_curves) { -->
<!-- #   plot(lakesDat$chl, lakesDat$secchi, -->
<!-- #        pch = 1, col = "black", -->
<!-- #        xlab = "CHL", ylab = "Secchi", -->
<!-- #        main = paste("Fitted Mean Curve\n(delta =", crv$delta, ")")) -->
<!-- #  -->
<!-- #   lines(crv$chl, crv$mu, lwd = 3, col = "blue") -->
<!-- # } -->
<!-- #  -->
<!-- # par(mfrow = c(1, 1)) -->
<!-- # ``` -->

We next consider POM additive error models as an alternative to the GLM. The Box–Cox mean–standard-deviation patterns for Secchi versus CHL suggested

$$
\operatorname{Var}(Y\mid x)\propto \mu(x)^{\theta},
\qquad
\theta\approx 1.5\text{–}3,
$$

which motivates variance weights of the form

$$
w_i\propto \mu(x_i)^{-\theta},
$$

So we examined POM models with

$$
\theta = 1.5, 2, 2.5, 3
$$

Across these specifications, the fitted curves and diagnostics revealed serious issues. For many choices of $\theta$, the fitted Secchi–CHL curve becomes nearly flat—or even slightly increasing—across much of the CHL range, with an abrupt “upturn" for CHL > 40. This behavior contradicts the strongly decreasing relationship evident in the raw data and in the GLM fits.

Residual diagnostics reinforce this conclusion: Even after weighting by $\mu(x)^{-\theta}$, substantial heteroscedasticity remains. Large residuals (absolute and squared) persist at both low and moderate CHL, and the spread does not stabilize across fitted means, indicating that the assumed POM variance form does not capture the true mean–variance relationship.

Notably, the POM models above were all formulated directly with CHL as the predictor. Because Secchi depth reflects light penetration through a water column, it may be more fitting to express the model in terms of a simple geometric argument linking depth to the amount of material per unit volume. This motivates the "cylinder" formulation considered next.

### A Possibly Better Approach? Disk & Volume 

The motivation for an additive model based on $1/\mathrm{CHL}$ follows from a geometric argument. If the visible water column above the Secchi disk is approximated as a cylinder of radius $r$ and height $h$ (the Secchi depth), then its volume is given by: 

$$
V = \pi r^{2} h,
\qquad
h = \frac{V}{\pi r^{2}}
$$

As CHL is measured as mass per unit volume (1 liter is equivalent to $1,000 \text{ cm}^3$), if visibility is "lost" when a roughly fixed mass of CHL is present above the disk, then: 

$$
V \propto \frac{1}{\mathrm{CHL}}
$$

and

$$
h \propto \frac{1}{\mathrm{CHL}}
$$

This suggests a reciprocal relationship between Secchi depth and CHL, motivating an additive model in the transformed predictor $\phi = 1/\mathrm{CHL}$:

$$
Y_i = \beta_0 + \beta_1 \phi_i + \varepsilon_i,
\qquad
E[\varepsilon_i \mid \phi_i] = 0
$$

Expressed in the original CHL scale, the regression function is then: 

$$
m(x) = E[Y \mid x] = \beta_0 + \frac{\beta_1}{x},
$$

After initial fit, the residual diagnostics (absolute and squared studentized residuals) for this model still show heteroscedasticity. 

As this model seems fairly promising, further adjustments were considered. Following the cube-root approach used in the fish-weight (walleye) example, consider specifying a TBS model where:

$$
Z = Y^{1/3},
\qquad
\psi = \phi^{1/3} = x^{-1/3},
$$

and fit

$$
Z = \alpha_0 + \alpha_1 \psi + \eta,
\qquad
E[\eta \mid \psi] = 0
$$

Back-transforming yields the approximate mean function

$$
m(x) \approx \big(\alpha_0 + \alpha_1 x^{-1/3}\big)^3
$$

which happens to be a smooth, strictly decreasing curve analogous to the cube-root linearization in the fish-weight model.

<!-- # ```{r secchi-phi-basic, message=FALSE, warning=FALSE, echo=FALSE, fig.height = 10, fig.width = 10} -->
<!-- # par(mfrow = c(2, 1), -->
<!-- #     mar = c(4, 4, 3, 1), -->
<!-- #     oma = c(0, 0, 0, 0), -->
<!-- #     xaxs = "i", yaxs = "i") -->
<!-- # ## Define phi = 1 / CHL -->
<!-- # lakesDat$phi <- 1 / lakesDat$chl -->
<!-- #  -->
<!-- # ## Fit additive model on transformed predictor (no summary printed) -->
<!-- # fit_phi <- lm(secchi ~ phi, data = lakesDat) -->
<!-- #  -->
<!-- # ############################################################ -->
<!-- # ## --- 1. FITTED CURVE (FIRST) ------------------------------ -->
<!-- # ############################################################ -->
<!-- #  -->
<!-- # plot(lakesDat$chl, lakesDat$secchi, -->
<!-- #      xlab = "CHL", ylab = "Secchi depth", -->
<!-- #      main = "Fitted Curve (Secchi ~ 1/CHL)") -->
<!-- #  -->
<!-- # chl.grid <- seq(min(lakesDat$chl, na.rm = TRUE), -->
<!-- #                 max(lakesDat$chl, na.rm = TRUE), -->
<!-- #                 length.out = 200) -->
<!-- #  -->
<!-- # pred_sec <- predict(fit_phi, -->
<!-- #                     newdata = data.frame(phi = 1 / chl.grid)) -->
<!-- #  -->
<!-- # lines(chl.grid, pred_sec, col = "red", lwd = 2) -->
<!-- #  -->
<!-- # ############################################################ -->
<!-- # ## --- 2. RESIDUALS VS FITTED (SECOND) ---------------------- -->
<!-- # ############################################################ -->
<!-- #  -->
<!-- # stud_resid <- rstudent(fit_phi) -->
<!-- #  -->
<!-- # plot(fitted(fit_phi), stud_resid, -->
<!-- #      xlab = "Fitted values", -->
<!-- #      ylab = "Studentized residuals", -->
<!-- #      main = "Studentized residuals vs Fitted\n(Secchi ~ 1/CHL)") -->
<!-- #  -->
<!-- # abline(h = 0, lty = 2) -->
<!-- #  -->
<!-- # ``` -->

<!-- # ```{r secchi-phi-cuberoot, message=FALSE, warning=FALSE, echo=FALSE, fig.height = 10, fig.width = 10} -->
<!-- # par(mfrow = c(2, 1), -->
<!-- #     mar = c(4, 4, 3, 1), -->
<!-- #     oma = c(0, 0, 0, 0), -->
<!-- #     xaxs = "i", yaxs = "i") -->
<!-- #  -->
<!-- # ## Transform both response and predictor -->
<!-- # lakesDat$phi       <- 1 / lakesDat$chl -->
<!-- # lakesDat$secchi_cr <- lakesDat$secchi^(1/3) -->
<!-- # lakesDat$phi_cr    <- lakesDat$phi^(1/3) -->
<!-- #  -->
<!-- # fit_phi_cr <- lm(secchi_cr ~ phi_cr, data = lakesDat) -->
<!-- #  -->
<!-- # ############################################################ -->
<!-- # ## --- 1. FITTED CURVE ON TOP ------------------------------ -->
<!-- # ############################################################ -->
<!-- #  -->
<!-- # plot(lakesDat$chl, lakesDat$secchi, -->
<!-- #      xlab = "CHL", ylab = "Secchi depth", -->
<!-- #      main = "Fitted Curve: Secchi^(1/3) ~ (1/CHL)^(1/3)") -->
<!-- #  -->
<!-- # chl.grid <- seq(min(lakesDat$chl, na.rm = TRUE), -->
<!-- #                 max(lakesDat$chl, na.rm = TRUE), -->
<!-- #                 length.out = 200) -->
<!-- #  -->
<!-- # phi_cr_grid <- (1 / chl.grid)^(1/3) -->
<!-- #  -->
<!-- # pred_cr <- predict(fit_phi_cr, -->
<!-- #                    newdata = data.frame(phi_cr = phi_cr_grid)) -->
<!-- #  -->
<!-- # pred_secchi <- pred_cr^3   # back-transform -->
<!-- #  -->
<!-- # lines(chl.grid, pred_secchi, col = "red", lwd = 2) -->
<!-- #  -->
<!-- # ############################################################ -->
<!-- # ## --- 2. RESIDUALS ON BOTTOM ------------------------------ -->
<!-- # ############################################################ -->
<!-- #  -->
<!-- # stud_resid <- rstudent(fit_phi_cr) -->
<!-- #  -->
<!-- # plot(fitted(fit_phi_cr), stud_resid, -->
<!-- #      xlab = "Fitted values (cube-root scale)", -->
<!-- #      ylab = "Studentized residuals", -->
<!-- #      main = "Studentized residuals vs Fitted\n(Secchi^(1/3) ~ (1/CHL)^(1/3))") -->
<!-- #  -->
<!-- # abline(h = 0, lty = 2) -->
<!-- #  -->
<!-- # ``` -->

<!-- # ```{r secchi-phi-pom, message=FALSE, warning=FALSE, echo=FALSE} -->
<!-- # ## Base additive model on phi = 1/CHL -->
<!-- # lakesDat$phi <- 1 / lakesDat$chl -->
<!-- # fit_phi <- lm(secchi ~ phi, data = lakesDat) -->
<!-- #  -->
<!-- # mu_hat <- fitted(fit_phi)   # estimated mean secchi -->
<!-- # e_hat  <- resid(fit_phi)    # residuals -->
<!-- #  -->
<!-- # ## Auxiliary regression to estimate delta -->
<!-- # aux <- lm(log(e_hat^2) ~ log(mu_hat)) -->
<!-- # # coef(aux) -->
<!-- # delta_hat <- 0.5 * coef(aux)[2]   # slope $\approx$ 2*delta => delta = slope/2 -->
<!-- # # delta_hat -->
<!-- #  -->
<!-- # delta_hat1 <- 0.5 -->
<!-- # delta_hat2 <- 0.75 -->
<!-- #  -->
<!-- # par(mfrow = c(2, 2), -->
<!-- #     mar = c(4, 4, 3, 1), -->
<!-- #     oma = c(0, 0, 0, 0), -->
<!-- #     xaxs = "i", yaxs = "i") -->
<!-- #  -->
<!-- # ############################################################ -->
<!-- # ## --- 1. FITTED (delta = 0.5) ----------------------------- -->
<!-- # ############################################################ -->
<!-- #  -->
<!-- # w1 <- 1 / (mu_hat^(2 * delta_hat1)) -->
<!-- # fit_phi_pom1 <- lm(secchi ~ phi, data = lakesDat, weights = w1) -->
<!-- #  -->
<!-- # plot(lakesDat$chl, lakesDat$secchi, -->
<!-- #      xlab = "CHL", ylab = "Secchi depth", -->
<!-- #      main = "Fitted Curve (delta = 0.5)") -->
<!-- # chl.grid <- seq(min(lakesDat$chl, na.rm = TRUE), -->
<!-- #                 max(lakesDat$chl, na.rm = TRUE), -->
<!-- #                 length.out = 200) -->
<!-- # pred1 <- predict(fit_phi_pom1, -->
<!-- #                  newdata = data.frame(phi = 1 / chl.grid)) -->
<!-- # lines(chl.grid, pred1, col = "red", lwd = 2) -->
<!-- #  -->
<!-- # ############################################################ -->
<!-- # ## --- 2. FITTED (delta = 0.75) ---------------------------- -->
<!-- # ############################################################ -->
<!-- #  -->
<!-- # w2 <- 1 / (mu_hat^(2 * delta_hat2)) -->
<!-- # fit_phi_pom2 <- lm(secchi ~ phi, data = lakesDat, weights = w2) -->
<!-- #  -->
<!-- # plot(lakesDat$chl, lakesDat$secchi, -->
<!-- #      xlab = "CHL", ylab = "Secchi depth", -->
<!-- #      main = "Fitted Curve (delta = 0.75)") -->
<!-- # pred2 <- predict(fit_phi_pom2, -->
<!-- #                  newdata = data.frame(phi = 1 / chl.grid)) -->
<!-- # lines(chl.grid, pred2, col = "red", lwd = 2) -->
<!-- #  -->
<!-- # ############################################################ -->
<!-- # ## --- 3. RESIDUALS (delta = 0.5) -------------------------- -->
<!-- # ############################################################ -->
<!-- #  -->
<!-- # stud_resid1 <- rstudent(fit_phi_pom1) -->
<!-- # plot(fitted(fit_phi_pom1), stud_resid1, -->
<!-- #      xlab = "Fitted values", -->
<!-- #      ylab = "Studentized residuals", -->
<!-- #      main = "Residuals vs Fitted (delta = 0.5)") -->
<!-- # abline(h = 0, lty = 2) -->
<!-- #  -->
<!-- # ############################################################ -->
<!-- # ## --- 4. RESIDUALS (delta = 0.75) ------------------------- -->
<!-- # ############################################################ -->
<!-- #  -->
<!-- # stud_resid2 <- rstudent(fit_phi_pom2) -->
<!-- # plot(fitted(fit_phi_pom2), stud_resid2, -->
<!-- #      xlab = "Fitted values", -->
<!-- #      ylab = "Studentized residuals", -->
<!-- #      main = "Residuals vs Fitted (delta = 0.75)") -->
<!-- # abline(h = 0, lty = 2) -->
<!-- # ``` -->

```{r secchi-phi-pom-delta75, include = F, message=FALSE, warning=FALSE, echo=FALSE, fig.height=10, fig.width=10}
## Base additive model on phi = 1/CHL
lakesDat$phi <- 1 / lakesDat$chl
fit_phi <- lm(secchi ~ phi, data = lakesDat)

mu_hat <- fitted(fit_phi)   # estimated mean secchi
e_hat  <- resid(fit_phi)    # residuals

## Auxiliary regression to estimate delta
aux <- lm(log(e_hat^2) ~ log(mu_hat))
# coef(aux)
delta_hat <- 0.5 * coef(aux)[2]   # slope $\approx$ 2*delta => delta = slope/2
delta_hat <- 0.75

par(mfrow = c(2, 1),
    mar = c(4, 4, 3, 1),
    oma = c(0, 0, 0, 0),
    xaxs = "i", yaxs = "i")

############################################################
## --- 1. FITTED (delta = 0.75) ----------------------------
############################################################

w <- 1 / (mu_hat^(2 * delta_hat))
fit_phi_pom <- lm(secchi ~ phi, data = lakesDat, weights = w)

plot(lakesDat$chl, lakesDat$secchi,
     xlab = "CHL", ylab = "Secchi depth",
     main = "Fitted Curve (delta = 0.75)")

chl.grid <- seq(min(lakesDat$chl, na.rm = TRUE),
                max(lakesDat$chl, na.rm = TRUE),
                length.out = 200)

pred <- predict(fit_phi_pom,
                newdata = data.frame(phi = 1 / chl.grid))

lines(chl.grid, pred, col = "red", lwd = 2)

############################################################
## --- 2. RESIDUALS (delta = 0.75) -------------------------
############################################################

stud_resid <- rstudent(fit_phi_pom)

plot(fitted(fit_phi_pom), stud_resid,
     xlab = "Fitted values",
     ylab = "Studentized residuals",
     main = "Residuals vs Fitted (delta = 0.75)")
abline(h = 0, lty = 2)
```

Although both the reciprocal additive model ($\phi = 1/\mathrm{CHL}$) and its cube-root TBS extension capture the overall decreasing Secchi–CHL trend, neither achieves satisfactory variance stabilization. Extending these models using the POM framework with powers $\theta = 0.5$ and $\theta = 0.75$ improves the fitted curve somewhat, in that their weighted fits yield smoother curves and better behavior at higher CHL values than the unweighted additive alternatives.

However, even these "improved" POM models show patterns and heteroscedasticity in their residuals (absolute, squared, or $\frac{2}{3}$-root absolute studentized residuals), indicating that variance stabilization remains incomplete. Between the two, the $\theta = 0.75$ model performs slightly better in the upper tail, where the fitted decline in Secchi depth more closely tracks the observed data. Still, the residual diagnostics for both POM models fall short of our desires, especially when compared against the Gamma GLM with an inverse link.

Accordingly, while the $\theta = 0.75$ POM model performs better than the other additive-error candidates, its remaining heteroscedasticity and lack of fit to the observed Secchi–CHL relationship limits its usefulness. The POM–TBS model is therefore retained only for comparison, with the Gamma GLM adopted as the primary model for the region-specific analyses.

Note: The “best” additive model is not presented in full here, but its fitted curve and representative residual diagnostics appear in the figures that follow.

## Comparing Models 

```{r secchi-phi-pom-fits2, message=FALSE, warning=FALSE, echo=FALSE, fig.height=10, fig.width=10}
layout(matrix(c(1, 1,
                2, 3),
              nrow = 2, byrow = TRUE))

par(mar = c(4, 4, 3, 1),
    oma = c(0, 0, 0, 0),
    xaxs = "i", yaxs = "i")

## ============================================
## Precompute prediction grid
## ============================================
chl.grid <- seq(min(lakesDat$chl, na.rm = TRUE),
                max(lakesDat$chl, na.rm = TRUE),
                length.out = 400)

z975 <- qnorm(0.975)  # for 95% CI


## ============================================
## 1. POM delta = 0.75: fitted curve + 95% CI
## ============================================
delta_hat2 <- 0.75
w2 <- 1 / (mu_hat^(2 * delta_hat2))
fit_phi_pom2 <- lm(secchi ~ phi, data = lakesDat, weights = w2)
  
pom075_pred <- predict(
  fit_phi_pom2,
  newdata = data.frame(phi = 1 / chl.grid),
  se.fit  = TRUE
)

fit_pom075 <- pom075_pred$fit
se_pom075  <- pom075_pred$se.fit

lo_pom075 <- fit_pom075 - z975 * se_pom075
hi_pom075 <- fit_pom075 + z975 * se_pom075


## ============================================
## 2. Gamma GLM (inverse link): fitted curve + 95% CI
##      mu(x) = 1 / eta,   eta = X * beta
## ============================================

X_grid <- cbind(1, chl.grid)

beta_g <- mod_gamma_inv$estb[, 1]
V_g    <- mod_gamma_inv$invinf

eta_g    <- as.vector(X_grid %*% beta_g)
se_eta_g <- sqrt(rowSums((X_grid %*% V_g) * X_grid))

eta_lo_g <- eta_g - z975 * se_eta_g
eta_hi_g <- eta_g + z975 * se_eta_g

## inverse link: mu = 1 / eta  (monotone decreasing)
mu_g    <- 1 / eta_g
mu_lo_g <- 1 / eta_hi_g   # swap hi/lo on eta
mu_hi_g <- 1 / eta_lo_g


## ============================================
## PLOT: Gamma vs POM delta = 0.75 with 95% CIs
## ============================================

plot(lakesDat$chl, lakesDat$secchi,
     pch = 1, col = "grey40",
     xlab = "CHL",
     ylab = "Secchi depth",
     main = "Gamma GLM vs POM (delta = 0.75)")

## --- POM delta = 0.75 ---
lines(chl.grid, fit_pom075,
      col = "blue", lwd = 2)
lines(chl.grid, lo_pom075,
      col = "blue", lwd = 1, lty = 3)
lines(chl.grid, hi_pom075,
      col = "blue", lwd = 1, lty = 3)

## --- Gamma GLM (inverse link) ---
lines(chl.grid, mu_g,
      col = "black", lwd = 3)
lines(chl.grid, mu_lo_g,
      col = "black", lwd = 1, lty = 2)
lines(chl.grid, mu_hi_g,
      col = "black", lwd = 1, lty = 2)

legend("topright",
       legend = c("POM delta = 0.75 (mean)", "Gamma GLM (mean)",
                  "POM 95% CI", "Gamma 95% CI"),
       col    = c("blue", "black", "blue", "black"),
       lty    = c(1, 1, 3, 2),
       lwd    = c(2, 3, 1, 1),
       bty    = "n")


## ============================================
## Precompute prediction grid
## ============================================
chl.grid <- seq(min(lakesDat$chl, na.rm = TRUE),
                max(lakesDat$chl, na.rm = TRUE),
                length.out = 400)

z975 <- qnorm(0.975)  # for 95% CI


## ============================================
## 1. POM delta = 0.75: fitted curve + 95% CI
## ============================================
delta_hat2 <- 0.75
w2 <- 1 / (mu_hat^(2 * delta_hat2))
fit_phi_pom2 <- lm(secchi ~ phi, data = lakesDat, weights = w2)
  
pom075_pred <- predict(
  fit_phi_pom2,
  newdata = data.frame(phi = 1 / chl.grid),
  se.fit  = TRUE
)

fit_pom075 <- pom075_pred$fit
se_pom075  <- pom075_pred$se.fit

lo_pom075 <- fit_pom075 - z975 * se_pom075
hi_pom075 <- fit_pom075 + z975 * se_pom075


## ============================================
## 2. Gamma GLM (inverse link): fitted curve + 95% CI
##      mu(x) = 1 / eta,   eta = X * beta
## ============================================

X_grid <- cbind(1, chl.grid)

beta_g <- mod_gamma_inv$estb[, 1]
V_g    <- mod_gamma_inv$invinf

eta_g    <- as.vector(X_grid %*% beta_g)
se_eta_g <- sqrt(rowSums((X_grid %*% V_g) * X_grid))

eta_lo_g <- eta_g - z975 * se_eta_g
eta_hi_g <- eta_g + z975 * se_eta_g

## inverse link: mu = 1 / eta  (monotone decreasing)
mu_g    <- 1 / eta_g
mu_lo_g <- 1 / eta_hi_g   # swap hi/lo on eta
mu_hi_g <- 1 / eta_lo_g

## ============================================
## PANEL 2 (bottom-left): GLM studentized deviance residuals
## ============================================
# GAMMA
mu_g       <- mod_gamma_inv$vals$muhat
res_g_p    <- mod_gamma_inv$vals$pearsonres
res_g_dev  <- mod_gamma_inv$vals$devres
res_g_std  <- mod_gamma_inv$vals$stdevres

stud_dev_g <- res_g_std

plot(mod_gamma_inv$vals$muhat, stud_dev_g,
     xlab = "Fitted values (Gamma GLM)",
     ylab = "Studentized deviance residuals",
     main = "Gamma GLM: deviance residuals")
abline(h = 0, lty = 2)

## ============================================
## PANEL 3 (bottom-right): 2/3-root |studentized residuals|
##      for POM additive error model (delta = 0.75)
## ============================================

stud_pom <- rstudent(fit_phi_pom2)
trans_pom <- abs(stud_pom)^(2/3)

plot(fitted(fit_phi_pom2), trans_pom,
     xlab = "Fitted values (POM, delta = 0.75)",
     ylab = expression("|studentized residual|^{2/3}"),
     main = "POM (delta = 0.75): 2/3-root |stud. residuals|")
```

The two models that performed adequately were the Gamma GLM with an inverse link and the POM additive model with $\delta = 0.75$ combined with a cube-root TBS transformation. Although both produce broadly similar decreasing curves, the Gamma GLM remains the preferred model.

First, the Gamma GLM better models the observed variance structure. The POM–TBS model, by contrast, imposes variance stabilization through estimated weights, making it more sensitive in the fitted means and more prone to irregularities at the extremes of CHL. Second, the fitted curve and confidence intervals from the Gamma GLM are smoother and more stable across the entire CHL range. The POM–TBS curve tends to flatten at higher CHL values, and its confidence bands widen noticeably at both ends due to back-transformation and weight variability.

A more solid nail in the coffin though is comparing respective residuals. The residual diagnostics generally favor the Gamma GLM: The deviance and studentized deviance residuals are more approximately symmetric and exhibit no major patterns, while the POM–TBS model retains curvature and uneven spread, indicating that its variance adjustments remain incomplete.

Taken together, these considerations make the Gamma GLM with inverse link the preferred modeling choice for the region-specific comparisons that follow.

## Applying to Regions 

```{r, include=F, message=F, warning=F, echo=F}
## Helper: fit optimized GLM by region for Secchi ~ CHL
fit_region_glm_inv <- function(region_label) {
  dat <- subset(lakesDat, region == region_label)
  X   <- cbind(1, dat$chl)      # predictor = CHL
  Y   <- dat$secchi             # response  = Secchi

  basic.glm(
    xmat   = X,
    y      = Y,
    link   = 6,   
    random = 5  
  )
}

mod_gamma_inv_plains <- fit_region_glm_inv("plains")
mod_gamma_inv_ozark  <- fit_region_glm_inv("ozarks")

combine_glm_results_region_inv <- function(mod, region_label) {
  beta_hat <- mod$estb[, 1]
  se_hat   <- sqrt(diag(mod$invinf))
  z        <- 1.96
  LCL      <- beta_hat - z * se_hat
  UCL      <- beta_hat + z * se_hat

  data.frame(
    Region   = region_label,
    Term     = c("Intercept", "CHL"),
    Estimate = beta_hat,
    SE       = se_hat,
    LCL      = LCL,
    UCL      = UCL,
    check.names = FALSE
  )
}

coef_tab <- rbind(
  combine_glm_results_region_inv(mod_gamma_inv_plains, "Plains"),
  combine_glm_results_region_inv(mod_gamma_inv_ozark,  "Ozark")
)
```

```{r, message=F, warning=F, echo=F, fig.height=8, fig.width=8}
## ----- Coefficient table ----- ##
kableExtra::kbl(
  coef_tab,
  digits = 4,
  align  = "llrrrr",
  row.names = FALSE,
  caption = "Region-specific optimized GLM coefficients for Secchi ~ CHL with 95\\% Wald CIs"
) |>
  kableExtra::kable_styling(
    full_width = FALSE,
    position   = "center",
    latex_options = c("hold_position", "scale_down")
  )

## ----- Predicted values table ----- ##
x_pred  <- c(1, 5, 10, 20)   # example CHL values; adjust as you like
X_pred  <- cbind(1, x_pred)
z       <- 1.96

pred_table_region_inv <- function(mod, region_label) {
  beta_hat <- mod$estb[, 1]
  vcov_mat <- mod$invinf

  eta_hat <- as.vector(X_pred %*% beta_hat)
  se_eta  <- sqrt(rowSums((X_pred %*% vcov_mat) * X_pred))

  eta_lo <- eta_hat - z * se_eta
  eta_hi <- eta_hat + z * se_eta

  ## inverse link: mu = 1 / eta
  ## note: 1/eta is decreasing, so CI endpoints flip
  mu_hat <- 1 / eta_hat
  mu_lo  <- 1 / eta_hi
  mu_hi  <- 1 / eta_lo

  data.frame(
    Region = region_label,
    CHL    = x_pred,
    Fit    = mu_hat,
    LCL    = mu_lo,
    UCL    = mu_hi
  )
}

pred_tab <- rbind(
  pred_table_region_inv(mod_gamma_inv_plains, "Plains"),
  pred_table_region_inv(mod_gamma_inv_ozark,  "Ozark")
)

kableExtra::kbl(
  pred_tab,
  digits = 4,
  align  = "llrrrr",
  row.names = FALSE,
  caption = "Predicted Secchi for CHL = 1, 5, 10, 20 with 95\\% CIs by region"
) |>
  kableExtra::kable_styling(
    full_width = FALSE,
    position   = "center",
    latex_options = c("hold_position", "scale_down")
  )

## ----- GLOBAL SCALES FOR PLOTS (CHL on x, Secchi on y) ----- ##
x_range_raw <- range(lakesDat$chl,    na.rm = TRUE)
y_range_raw <- range(lakesDat$secchi, na.rm = TRUE)

pad_x <- 0.05 * diff(x_range_raw)
pad_y <- 0.05 * diff(y_range_raw)

x_range <- c(x_range_raw[1] - pad_x,
             x_range_raw[2] + pad_x)

y_range <- c(y_range_raw[1] - pad_y,
             y_range_raw[2] + pad_y)

par(mfrow = c(1, 2),
    oma = c(0, 0, 0, 0),
    mar = c(4, 4, 2, 1),
    xaxs = "i", yaxs = "i")

z95 <- qnorm(0.975)
```

```{r overlay-plot3, echo=FALSE, message=FALSE, warning=FALSE, fig.height=8, fig.width=12}
layout(matrix(c(1, 1,
                2, 3),
              nrow = 2, byrow = TRUE))

par(mar = c(4, 4, 3, 1),
    oma = c(0, 0, 0, 0),
    xaxs = "i", yaxs = "i")

## ---- GLOBAL SCALES WITH PADDING (CHL, Secchi) ---- ##
x_range_raw <- range(lakesDat$chl,    na.rm = TRUE)
y_range_raw <- range(lakesDat$secchi, na.rm = TRUE)

pad_x <- 0.05 * diff(x_range_raw)
pad_y <- 0.05 * diff(y_range_raw)

x_range <- c(x_range_raw[1] - pad_x,
             x_range_raw[2] + pad_x)
y_range <- c(y_range_raw[1] - pad_y,
             y_range_raw[2] + pad_y)

## Colors for curves
col_plains <- "#0072B2"
col_ozark  <- "#D55E00"

## z for 95% CI
z95 <- qnorm(0.975)

## ---- PLAINS PREDICTION GRID & FIT ---- ##
dat_plains <- subset(lakesDat, region == "plains")
x_grid_p   <- seq(min(dat_plains$chl, na.rm = TRUE),
                  max(dat_plains$chl, na.rm = TRUE),
                  length.out = 200)
X_grid_p   <- cbind(1, x_grid_p)

beta_p  <- mod_gamma_inv_plains$estb[, 1]
vcov_p  <- mod_gamma_inv_plains$invinf

eta_p    <- X_grid_p %*% beta_p
se_eta_p <- sqrt(rowSums((X_grid_p %*% vcov_p) * X_grid_p))

eta_lo_p <- eta_p - z95 * se_eta_p
eta_hi_p <- eta_p + z95 * se_eta_p

## inverse link: mu = 1 / eta (decreasing, so CI endpoints flip)
mu_p    <- 1 / eta_p
mu_lo_p <- 1 / eta_hi_p
mu_hi_p <- 1 / eta_lo_p

## ---- OZARKS PREDICTION GRID & FIT ---- ##
dat_ozark <- subset(lakesDat, region == "ozarks")
x_grid_o  <- seq(min(dat_ozark$chl, na.rm = TRUE),
                 max(dat_ozark$chl, na.rm = TRUE),
                 length.out = 200)
X_grid_o  <- cbind(1, x_grid_o)

beta_o  <- mod_gamma_inv_ozark$estb[, 1]
vcov_o  <- mod_gamma_inv_ozark$invinf

eta_o    <- X_grid_o %*% beta_o
se_eta_o <- sqrt(rowSums((X_grid_o %*% vcov_o) * X_grid_o))

eta_lo_o <- eta_o - z95 * se_eta_o
eta_hi_o <- eta_o + z95 * se_eta_o

mu_o    <- 1 / eta_o
mu_lo_o <- 1 / eta_hi_o
mu_hi_o <- 1 / eta_lo_o

## ---- PLOT PANEL ---- ##
# par(mfrow = c(1,1),
#     mar = c(4,4,2,1),
#     xaxs = "i", yaxs = "i")

plot(lakesDat$chl, lakesDat$secchi,
     xlab = "CHL",
     ylab = "Secchi depth",
     main = "Optimized GLM Fits: Secchi ~ CHL (Plains vs Ozark)",
     pch  = 1, col = "gray50",
     xlim = x_range, ylim = y_range)

## ---- ADD PLAINS CURVE ---- ##
lines(x_grid_p, mu_p,    lwd = 2, col = col_plains)
lines(x_grid_p, mu_lo_p, lwd = 1, col = col_plains, lty = 3)
lines(x_grid_p, mu_hi_p, lwd = 1, col = col_plains, lty = 3)

## ---- ADD OZARK CURVE ---- ##
lines(x_grid_o, mu_o,    lwd = 2, col = col_ozark)
lines(x_grid_o, mu_lo_o, lwd = 1, col = col_ozark, lty = 3)
lines(x_grid_o, mu_hi_o, lwd = 1, col = col_ozark, lty = 3)

## ---- LEGEND ---- ##
legend("topleft",
       legend = c("Plains Mean", "Plains 95% CI",
                  "Ozark Mean",  "Ozark 95% CI"),
       col    = c(col_plains, col_plains, col_ozark, col_ozark),
       lty    = c(1, 3, 1, 3),
       lwd    = c(2, 1, 2, 1),
       bty    = "n")

# par(mfrow = c(2, 1),
#     mar = c(4, 4, 3, 1),
#     oma = c(0, 0, 0, 0),
#     xaxs = "i", yaxs = "i")

## Plains
plot(mod_gamma_inv_plains$vals$muhat,
     mod_gamma_inv_plains$vals$stdevres,
     xlab = "Fitted Secchi (Plains)",
     ylab = "Standardized deviance residual",
     main = "Residuals vs Fitted: Plains (Secchi ~ CHL)",
     pch = 1)
abline(h = 0, lty = 2)

## Ozarks
plot(mod_gamma_inv_ozark$vals$muhat,
     mod_gamma_inv_ozark$vals$stdevres,
     xlab = "Fitted Secchi (Ozarks)",
     ylab = "Standardized deviance residual",
     main = "Residuals vs Fitted: Ozarks (Secchi ~ CHL)",
     pch = 1)
abline(h = 0, lty = 2)

# par(mfrow = c(1, 1))
```

```{r overlay-plot4, echo=FALSE, message=FALSE, warning=FALSE, fig.height=8, fig.width=8}
kable(
  data.frame(
    Region          = c("Plains", "Ozarks"),
    Scaled_Deviance = c(mod_gamma_inv_plains$ests$sdev,
                        mod_gamma_inv_ozark$ests$sdev)
  ),
  caption = "Scaled deviance by region for Secchi ~ CHL Gamma GLMs"
)
```

## Interpreting the Model & Results  

Across both regions, Secchi depth shows the expected negative and nonlinear association with chlorophyll: Water clarity declines as CHL increases, and the Gamma GLM with an inverse link plausibly models the observed data. When the model is fit separately to the Plains and Ozarks using the same structure, the shapes of the fitted Secchi–CHL curves are nearly identical, indicating that the underlying ecological mechanism appears consistent across Missouri lakes in both regions.

The magnitude of the response differs, however. For any fixed CHL value, Secchi depth is higher in the Ozarks than in the Plains. This vertical offset is evident even at low CHL (Ozarks roughly 2.9–3.2m vs. 1.7–2.5m in the Plains) and becomes more pronounced as CHL increases. By CHL values around 20–30, the two fitted curves are visibly distinct, with minimal overlap in their confidence bands.

The coefficient estimates reinforce this interpretation. The regions have virtually identical slopes on the inverse–link scale, implying similar rates at which Secchi declines with CHL, though the intercepts differ. The Ozarks exhibit a larger fitted intercept, indicating that, for a given chlorophyll concentration, Ozark lakes tend to have higher water clarity than Plains lakes.

Residual diagnostics further support these conclusions. Both regional fits display well-behaved studentized deviance residuals under the Gamma GLM, with no evidence of region-specific misspecification, differing functional forms, or dispersion anomalies. Thus, the observed separation reflects a genuine regional difference rather than a modeling artifact.

Overall, there is evidence to support that the Plains and Ozarks share the same functional Secchi–CHL relationship but differ in overall water clarity: Ozark lakes exhibit consistently higher Secchi depth than Plains lakes at matched CHL values.

\newpage

# Q3: LRT SECCHI & CHL (Plains vs. Ozarks)

Let $Y_{ij}$ denote Secchi depth for observation $i$ in region $j \in \{P,O\}$ (Plains, Ozarks), with covariate $x_{ij} = \mathrm{CHL}$.

Assume a Gamma model with common shape $\alpha > 0$ and mean $\mu_{ij} > 0$:

$$
Y_{ij} \mid x_{ij} \sim \mathrm{Gamma}(\alpha,\mu_{ij})
$$

The density (shape–mean form) is

$$
f(y_{ij}\mid \mu_{ij},\alpha)
= \frac{1}{\Gamma(\alpha)}
\left( \frac{\alpha}{\mu_{ij}} \right)^{\alpha}
y_{ij}^{\alpha-1}
\exp\!\left(-\frac{\alpha y_{ij}}{\mu_{ij}}\right),
\qquad y_{ij}>0
$$

The GLM chosen uses the inverse link

$$
g(\mu)=\frac{1}{\mu},
\qquad
\eta_{ij}=\frac{1}{\mu_{ij}}
$$

The Reduced model is given by:

$$
\eta_{ij}=\beta_0+\beta_1 x_{ij},
\qquad
\mu_{ij}=\frac{1}{\beta_0+\beta_1 x_{ij}}
$$

The parameter vector is

$$
\theta_0 = (\beta_{0},\beta_{1}, \alpha)
$$

With reduced log–likelihood

$$
\ell_0(\beta_0,\beta_1,\alpha)
=
\sum_{i, j}
\Big[
(\alpha-1)\log y_{ij}
-\alpha y_{ij}(\beta_0+\beta_1 x_{ij})
+\alpha\log(\beta_0+\beta_1 x_{ij})
+\alpha\log\alpha
-\log\Gamma(\alpha)
\Big]
$$

The Full model is given by:

$$
\eta_{iP}=\beta_{0P}+\beta_{1P}x_{iP},
\qquad
\eta_{iO}=\beta_{0O}+\beta_{1O}x_{iO}, \tag{Regretting the subscripts right about now.}
$$
$$
\mu_{iP}=\frac{1}{\beta_{0P}+\beta_{1P}x_{iP}},
\qquad
\mu_{iO}=\frac{1}{\beta_{0O}+\beta_{1O}x_{iO}}
$$

The parameter vector is

$$
\theta_1 = (\beta_{0P},\beta_{1P},\beta_{0O},\beta_{1O},\alpha)
$$

The full log–likelihood is

$$
\begin{aligned}
\ell_1(\theta_1)
=&
\sum_{i=1}^{n_P}
\Big[
(\alpha-1)\log y_{iP}
-\alpha y_{iP}(\beta_{0P}+\beta_{1P}x_{iP})
+\alpha\log(\beta_{0P}+\beta_{1P}x_{iP})
+\alpha\log\alpha
-\log\Gamma(\alpha)
\Big] \\
&+
\sum_{i=1}^{n_O}
\Big[
(\alpha-1)\log y_{iO}
-\alpha y_{iO}(\beta_{0O}+\beta_{1O}x_{iO})
+\alpha\log(\beta_{0O}+\beta_{1O}x_{iO})
+\alpha\log\alpha
-\log\Gamma(\alpha)
\Big]
\end{aligned}
$$

The LRT compares (under regularity conditions): 

* $H_0$: reduced model is true (no region difference) 

* $H_1$: full model is true (regions differ)

Let $\hat\theta_0$ be the MLE under $H_0$ and $\hat\theta_1$ the MLE under $H_1$. The likelihood ratio statistic is of the form: 

$$
-2\big[
\ell_0(\hat\theta_0) - \ell_1(\hat\theta_1)
\big]
$$

Under $H_0$,

$$
-2\big[
\ell_0(\hat\theta_0) - \ell_1(\hat\theta_1)
\big] \overset{a}{\sim} \chi^2_2
$$

With degrees of freedom of the $\chi^2$ given by the difference in parameters between full and reduced ($5 - 3 = 2$). 

We reject $H_0$ at the $\alpha$ level when

$$
-2\big[
\ell_0(\hat\theta_0) - \ell_1(\hat\theta_1)
\big] > \chi^2_{2,\,1-\alpha}
$$

```{r, echo=FALSE, message=FALSE, warning=FALSE}
dat <- list(
  y = lakesDat$secchi,
  x = lakesDat$chl,
  region = lakesDat$region  
)

loglik_reduced <- function(par, dat) {
  y  <- dat$y
  x  <- dat$x

  beta0    <- par[1]
  beta1    <- par[2]
  logalpha <- par[3]
  alpha    <- exp(logalpha)

  eta <- beta0 + beta1 * x
  eta <- pmax(eta, 1e-8)   # keep eta > 0

  mu <- 1 / eta

  ll <- (alpha - 1) * log(y) -
        alpha * y / mu -
        alpha * log(mu) +
        alpha * log(alpha) -
        lgamma(alpha)

  sum(ll)
}

loglik_full <- function(par, dat) {
  y  <- dat$y
  x  <- dat$x
  rg <- dat$region

  betaP0   <- par[1]
  betaP1   <- par[2]
  betaO0   <- par[3]
  betaO1   <- par[4]
  logalpha <- par[5]
  alpha    <- exp(logalpha)

  # Start with Plains as default for everyone
  eta <- betaP0 + betaP1 * x

  # Overwrite with Ozarks where appropriate
  isO <- rg == "Ozarks"   # <- make sure this matches your actual labels
  eta[isO] <- betaO0 + betaO1 * x[isO]

  eta <- pmax(eta, 1e-8)
  mu  <- 1 / eta

  ll <- (alpha - 1) * log(y) -
        alpha * y / mu -
        alpha * log(mu) +
        alpha * log(alpha) -
        lgamma(alpha)

  sum(ll)
}

nll_reduced <- function(par, dat) -loglik_reduced(par, dat)
nll_full    <- function(par, dat) -loglik_full(par, dat)

# par starts
x0_red <- c(
  mod_gamma_inv$estb[1,1],   # beta0
  mod_gamma_inv$estb[2,1],   # beta1
  log(1 / mod_gamma_inv$ests$phi)  # log alpha
)

x0_full <- c(
  mod_gamma_inv_plains$estb[1,1], # betaP0
  mod_gamma_inv_plains$estb[2,1], # betaP1
  mod_gamma_inv_ozark$estb[1,1], # betaO0
  mod_gamma_inv_ozark$estb[2,1], # betaO1
  log(1 / mod_gamma_inv$ests$phi) # log alpha   (shared)
)

# Reduced model fit
fit_red <- optim(
  par     = x0_red,
  fn      = function(p) nll_reduced(p, dat),
  method  = "BFGS",
  hessian = TRUE
)

# Full model fit
fit_full <- optim(
  par     = x0_full,
  fn      = function(p) nll_full(p, dat),
  method  = "BFGS",
  hessian = TRUE
)

# Extract maximized log-likelihoods
ll_red_hat  <- -fit_red$value
ll_full_hat <- -fit_full$value

Lambda_raw <- 2 * (ll_full_hat - ll_red_hat)
Lambda     <- max(0, Lambda_raw)  # enforce non-negativity

df   <- length(x0_full) - length(x0_red)   # 5 - 3 = 2
pval <- pchisq(Lambda, df = df, lower.tail = FALSE)
# pval <- 1 - pchisq(Lambda_raw, df = df)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# cat("Convergence of Reduced fit (0 is converged):", fit_red$convergence, "\n")
# cat("Convergence of Full fit (0 is converged):", fit_full$convergence, "\n")
# cat("ll_red_hat =", ll_red_hat, "\n")
# cat("ll_full_hat =", ll_full_hat, "\n")
# cat("Lambda_raw =", Lambda_raw, "\n")
# cat("p-value of LRT is:", pval, "\n")
results_table <- tibble(
  Quantity = c(
    "Convergence (Reduced)",
    "Convergence (Full)",
    "LogLik (Reduced MLE)",
    "LogLik (Full MLE)",
    "Lambda (raw)",
    "LRT p-value"
  ),
  Value = c(
    as.character(as.integer(fit_red$convergence)),   # no decimals
    as.character(as.integer(fit_full$convergence)),  # no decimals
    format(round(ll_red_hat, 2), nsmall = 2),        # 2 decimals
    format(round(ll_full_hat, 2), nsmall = 2),       # 2 decimals
    format(round(Lambda_raw, 5), nsmall = 5),        # 5 decimals
    format(round(pval, 3), nsmall = 3)               # 3 decimals
  )
)

kbl(results_table,
    caption = "Model Fit, Log-Likelihoods, and LRT Results, Convergence using `optim`") |>
  kable_styling(full_width = FALSE, position = "center")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# 1. Loglik of reduced at its own parameters
# ll_red_at_red <- loglik_reduced(x0_red, dat)
# 
# # 2. Loglik of full at "nested" parameters (force Plains and Ozarks to share)
# par_full_nested <- c(
#   x0_red[1],  # betaP0 = beta0
#   x0_red[2],  # betaP1 = beta1
#   x0_red[1],  # betaO0 = beta0
#   x0_red[2],  # betaO1 = beta1
#   x0_red[3]   # logalpha (same)
# )
# ll_full_at_nested <- loglik_full(par_full_nested, dat)
# c(ll_red_at_red  = ll_red_at_red,
#   ll_full_nested = ll_full_at_nested)
```

In short, 

* Definitive Question: Do the regions differ in the relation between Chlorophyll and Secchi depth?

* Definitive Answer: No! 
