---
output:
  pdf_document: default
  html_document: default
---

# HW 2 
[Name: Sam Olson]{.smallcaps} \
[Collaborators: XXX]{.smallcaps} \

## Progress Report 
  - 1: PARTIAL
  - 2: DONE
  - 3: DONE
  - 4: DONE
  - 5: PARTIAL
  - 6: DONE
  - 7: PARTIAL
  - 8: 

>> # Fig. 1
Used in Q7, part (b)

\[
1 = \int\limits_{\infty}^{\infty}{\frac{1}{\sqrt{2\pi}}x^2 e^{\frac{-x^2}{2}}dx} = \frac{2}{\sqrt{2\pi}}\int\limits_{0}^{\infty}{x^2 e^{\frac{-x^2}{2}}dx}
\]

# 1. 
>> Q: Suppose a random variable X has the following cdf from class (which is neither a step function nor continuous):  

$$F(x) = 
\begin{cases}
  0 & x < 0 \\
  (1+x)/2 & 0 \leq x \leq 1 \\ 
  1 & x > 1
\end{cases}$$

(a): Find the following probabilities: 
  $P(X > \frac{1}{2})$
  $P(X \geq \frac{1}{2})$
  $P(0 < X \leq \frac{1}{2})$
  $P(0 \leq X \leq \frac{1}{2})$

(b): Conditional on the event "X > 0", the corresponding conditional pdf of X (i.e. given X > 0) is as follows at $x \in \mathbb{R}$: 

$$P(X \leq x | X > 0 ) = \frac{P(X \leq x , X > 0 )}{P(X > 0)} = \frac{P(0 < X \leq x)}{P(X > 0)} = \frac{F(x) - F(0)}{1 - F(0)}$$

Giving: 

$$P(X \leq x | X > 0 ) = 
\begin{cases}
  0 & x \leq 0 \\
  x & 0 < x \leq 1 \\ 
  1 & x > 1
\end{cases}$$

Based on the conditional cdf above, show that the distribution of X, conditional on "X > 0", is the same (i.e. has the same cdf) as that of a random variable Y which is "uniform" on the interval (0, 1), having constant pdf $f_Y(y) = 1$ for $0 < y < 1$ (with $f_Y(y) = 0$ for all other $y \in \mathbb{R}$)

>> A: 

(a):
$$F(x) = 
\begin{cases}
  1 & x < 0 \\
  1 & 0 \leq x \leq 1 \\ 
  0 & x > 1
\end{cases}$$

  $$P(X > \frac{1}{2}) = 1 - F(\frac{1}{2}) = 1 - \frac{3}{4} = \frac{1}{4}$$
  
  $$P(X \geq \frac{1}{2}) = 1 - F(\frac{1}{2}) = 1 - \frac{3}{4} = \frac{1}{4}$$
  
  $$P(0 < X \leq \frac{1}{2}) = F(\frac{1}{2}) - F(0) =\frac{3}{4} - \frac{1}{2} = \frac{1}{4}$$
  
  $$P(0 \leq X \leq \frac{1}{2})= F(\frac{1}{2}) - F(0) = \frac{3}{4} - \frac{1}{2} = \frac{1}{4}$$

(b): 



\newpage

# 2. 
>> Q: Statistical reliability involves studying the time to failure of manufactured units. In many reliability textbooks, one can find the exponential distribution: 

$$f(x) = 
\begin{cases}
  \frac{1}{\theta} e^{-\frac{x}{\theta}} & x > 0 \\
  0 & x \leq 0
\end{cases}$$

where $\theta > 0$ is a fixed value, for modeling the time X that a random unit runs until failure (i.e. X is a survival time). Show that if X has an exponential distribution as above, then: 

$P(X > s + t | X > t) = P(X > s)$

for any values t, s > 0; this feature is called the "memoryless" property of te exponential distribution. 

>> A: 

Let X be a random variable with Exponential distribution as given above, with parameter $\theta > 0$. Let t, s > 0. 

For x > 0, the pdf given is $\frac{1}{\theta} e^{-\frac{x}{\theta}}$, thus, for the same x > 0 the cdf is: 

$F_X(x) = \int_{x>0} f(x) dx = \int{\frac{1}{\theta} e^{-\frac{x}{\theta}}}dx = 1 - e^{-\frac{x}{\theta}}$

Thus: 

$P(X > s + t | X > t) = P(X > s) = \frac{P(X > s + t, X > t)}{P(X > t)}$

$P(X > s + t | X > t) = P(X > s) = \frac{P(X > s + t)}{P(X > t)}$

$P(X > s + t | X > t) = \frac{1 - F_X(s + t)}{1 - F_X(t)}$

$$P(X > s + t | X > t) = \frac{1 - (1-\frac{1}{\theta} e^{-\frac{s + t}{\theta}})}{1 - (1-\frac{1}{\theta} e^{-\frac{t}{\theta}})}$$

Cancelling out (most) like terms gives us: 

$P(X > s + t | X > t) = \frac{e^{-\frac{s + t}{\theta}}}{e^{-\frac{ t}{\theta}}} = e^{\frac{-(s+t)-(-t)}{\theta}} = e^{-\frac{s}{\theta}}$ 

However, we know that this is exactly $P(X > s)$!, giving us: 

$P(X > s + t | X > t) = e^{-\frac{s}{\theta}} = P(X > s)$

\newpage

# 3. 2.3: 
>> Q: Suppose X has the Geometrc pmf: 

$f_X(x) = \frac{1}{3}(\frac{2}{3})^x$, $x = 0, 1, 2, ...$
Determine the probability distribution of $Y = \frac{X}{X+1}$
Note that here X and Y are discrete random variables. To specify the probability distribution of Y, specify its pmf. 

>> A: 

$f_Y(y) = P(Y = y) = P(\frac{X}{X+1} = y)$

Using this relation we have: 
$y(X+1) = X \rightarrow yX + y = X \rightarrow y = X - yX \rightarrow y = X(1-y)$

Thus we have: 
$X = \frac{y}{1-y}$

Returning then to the original function for the pmf, we have: 

$f_Y(y) = P(X = \frac{y}{1-y}) = \frac{1}{3}(\frac{2}{3})^\frac{y}{1-y}$

We must then identify the support of Y given $x = 0, 1, 2, ...$

For the support of X as given, $x = 0, 1, 2, ... \rightarrow y = \frac{X}{X+1} = \frac{0}{1}, \frac{1}{2}, \frac{2}{3}, ...$

Thus we define the discrete random variable Y by its pmf and support respectively as: 

$f_y(y) = \frac{1}{3}(\frac{2}{3})^\frac{y}{1-y}$ for $y = 0, \frac{1}{2}, \frac{2}{3}, ...$

\newpage

# 4. 2.4: 
>> Q: 

Let $\lambda$ be a fixed positive constant, and define the function $f(x)$ by: 

$f(x) = \frac{1}{2}\lambda e^{-\lambda x}$ if $x \geq 0$ 
and $f(x) = \frac{1}{2}\lambda e^{\lambda x}$ if $x < 0$

(a): Verify that $f(x)$ is a pdf. 

(b): If X is a random variable with pdf given by $f(X)$, find $P(X < t)$ $\forall t$. Evaluate all integrals.

(c): Find $P(|X| < t)$ $\forall t$. Evaluate all integrals. 

>> A: 

(a): 
(1): $f(x)$ is a pdf so long as it is well defined, i.e. $f(x) \geq 0$ $\forall x \in \mathbb{X}$ 
(2): and so long as 
$\int\limits_{x \in \mathbb{X}}{f(x)dx} = 1$

Then $f(x)$ is a (proper) pdf

(1): $f(x)$ is well-defined, i.e. ever negative.  

For $x \geq 0$, $e^{-x} \geq 0$, so by including additional, fixed (positive!) constants such as $\lambda$, $f(x) \geq 0$ for $x \geq 0$. 

For $x < 0$, $f(x) = e^{\lambda x} \geq 0$, so by including additional, fixed positive constants such as $\lambda$, $f(x) \geq 0$ for $x < 0$

Taken collectively, $f(x) \geq 0$ for all $x \in \mathbb{X}$

(2):
$$\int\limits_{x \in \mathbb{X}}{f(x)dx} = \int\limits_{x < 0}{\frac{1}{2}\lambda e^{\lambda x}} + \int\limits_{x \geq 0}{\frac{1}{2}\lambda e^{-\lambda x}}$$ 

$$\int\limits_{x \in \mathbb{X}}{f(x)dx} = \int\limits_{-\infty}^{0}{\frac{1}{2}\lambda e^{\lambda x}} + \int\limits_{0}^{\infty}{\frac{1}{2}\lambda e^{-\lambda x}}$$

Note, we can factor out a constant term from both integrals, giving us: 

$$\int\limits_{x \in \mathbb{X}}{f(x)dx} = \frac{1}{2}\lambda(\int\limits_{-\infty}^{0}{e^{\lambda x}} + \int\limits_{0}^{\infty}{e^{-\lambda x}}) = \frac{1}{2}\lambda [\frac{e^{\lambda x}}{\lambda}\big|_{-\infty}^{0} + (-\frac{e^{-\lambda x}}{\lambda}\big|_{0}^{\infty})]$$

$$\int\limits_{x \in \mathbb{X}}{f(x)dx} = \frac{1}{2}\lambda(\frac{1}{\lambda} - (-\frac{1}{\lambda})) = \frac{1}{2}\lambda (\frac{2}{\lambda}) = 1$$

We may then conclude that $f(x)$ is a (proper) pdf. 

(b):

If X is a random variable with pdf given by $f(X)$, find $P(X < t)$ $\forall t$.

$$P(X < t) = 
\begin{cases}
  \int\limits_{-\infty}^{t}\frac{1}{2}\lambda{e^{\lambda x}}dx & t > 0 \\
  \int\limits_{-\infty}^{0}\frac{1}{2}\lambda{e^{\lambda x}}dx + \int\limits_{0}^{t}\frac{1}{2}\lambda{e^{-\lambda x}}dx & t \geq 0
\end{cases}$$

We then evaluate the integrals of each, giving: 

(1): 

$\int\limits_{-\infty}^{t}\frac{1}{2}\lambda{e^{\lambda x}}dx = \frac{1}{2}\lambda e^{\lambda t}\big|_{-\infty}^{t} = \frac{1}{2} e^{\lambda t} - 0 = \frac{1}{2} e^{\lambda t}$

(2)

$\int\limits_{0}^{t}\frac{1}{2}\lambda{e^{-\lambda x}}dx = {-\frac{1}{2}e^{-\lambda x}}\big|_{0}^{t} = \frac{1}{2} - \frac{1}{2}e^{-\lambda t}$ 

(3): 

$\int\limits_{-\infty}^{0}\frac{1}{2}\lambda{e^{\lambda x}}dx = {\frac{1}{2}e^{\lambda x}}\big|_{-\infty}^{0} = \frac{1}{2} - 0$

(4): For the case of (2) + (3), 

$$\frac{1}{2} + \frac{1}{2} - \frac{1}{2}e^{-\lambda t} = 1 - \frac{1}{2}e^{-\lambda t}$$

Thus we're left with:

$$P(X < t) = 
\begin{cases}
  \frac{1}{2}e^{\lambda t} & t > 0 \\
   1 - \frac{1}{2}e^{-\lambda t} & t \geq 0
\end{cases}$$


(c):

$P(|X| < t)$ $\forall t$, 

$$P(|X| < t) = P(-t < X < t)= \int\limits_{-t}^{0}\frac{1}{2}\lambda{e^{\lambda x}} + \int\limits_{0}^{t}\frac{1}{2}\lambda{e^{-\lambda x}}$$

$$P(|X| < t) = \frac{1}{2}[\frac{e^{\lambda x}}{\lambda}\big|_{-t}^{0} + (-\frac{e^{-\lambda x}}{\lambda}\big|_{0}^{t})] = \frac{1}{2} [(1 - e^{-\lambda t}) + (-e^{-\lambda t} + 1)] = \frac{1}{2}(2)(1 - e^{-\lambda t}) = 1-e^{-\lambda t}$$ 


\newpage

# 5. 2.6 (b, c): 
>> Q: In each of the following find the pdf of Y. (Do not need to verify the pdf/evaluate the integration, per Instructions).  

(b): $f_X(x) = \frac{3}{8}(x + 1)^2$, $-1 < x < 1$; $Y = 1 - X^2$

(c): $f_X(x) = \frac{3}{8}(x + 1)^2$, $-1 < x < 1$; $Y = 1 - X^2$ if $X \leq 0$ and $Y = 1 - X$ if $X > 0$

>> A: 

CHECK THM 2.1.8

(b): $Y = 1 - X^2 \rightarrow X = \sqrt{1 - Y} \equiv (1-Y)^{1/2}$, and $-1 < x < 1 \rightarrow 0 < y < 1$

Then for the pdf of Y, we have: 

$$f_Y(y) =
\begin{cases}
  \sum \limits_{i=1}^{k} f_X(g^{-1}_i(y)) & y \in \mathbb{Y} \\
  0 & otherwise \\ 
\end{cases}$$

$f_X(x) = \frac{3}{8}(x + 1)^2 \rightarrow f_Y(y) = \frac{3}{8}(\sqrt{1 - y})^2$

Such that: 

$$f_Y(y) = \frac{3}{8}(1-y)^{-1/2} + \frac{3}{8}(1-y)^{1/2}$$ for $0 < y < 1$

(c): 
(1): $Y = 1 - X^2 \rightarrow X = \sqrt{1 - Y}$, and $-1 < x \leq 0 \rightarrow 0 < y \leq  1$ for $X \leq 0$

(2): $Y = 1 - X \rightarrow X = 1 - Y$, and $0 < x < 1 \rightarrow 1 < y < \sqrt{2}$ for $X > 0$

Then for the pdf of Y, we have: 

(1): $f_X(x) = \frac{3}{8}(x + 1)^2 \rightarrow f_Y(y) = \frac{3}{8}(\sqrt{1 - y})^2$ for $0 < y \leq  1$

(2): $f_X(x) = \frac{3}{8}(x + 1)^2 \rightarrow f_Y(y) = \frac{3}{8}(1 - y)^2$ for $1 < y <  \sqrt{2}$

Taking (1) and (2) together, we may write the pdf of Y as: 

$$f_Y(y) =
\begin{cases}
  \sum \limits_{i=1}^{k} f_X(g^{-1}_i(y)) & y \in \mathbb{Y} \\
  0 & otherwise \\ 
\end{cases}$$

Such that: 

$$f_Y(y) = \frac{3}{16}(1-(1-y)^{1/2})^2(1-y)^{-1/2} + \frac{3}{8}(2-y)^{2}$$ for $0 < y < 1$

\newpage

# 6. 2.9: 
>> Q: If the random variable X has pdf: 
\[
f(x) = 
\begin{cases}
  \frac{x-1}{2} & 1 < x < 3 \\
  0 & \text{otherwise}
\end{cases}
\]

find a monotone function $u(x)$ such that the random variable $Y = u(X)$ has a Uniform(0,1) distribution.

>> A: 

We may take advantage of Thm 2.1.10, and let the random variable Y be defined as $Y = u(X) = F_x(x)$

Taking advantage of the fact that $u(x) = F_x(x) \rightarrow F_x(X)$ ~Uniform $(0, 1)$

That is to say define the random variable Y as the cdf of the random variable X. 

$F_x(x) = P(X \leq x) = \int\limits_{-\infty}^{x}{f(t)dt} = \int\limits_{-\infty}^{x} \frac{t-1}{2} = \int\limits_{-\infty}^{1} \frac{t-1}{2} + \int\limits_{1}^{x} \frac{t-1}{2} = 0 + \int\limits_{1}^{x} \frac{t-1}{2}$

$$F_x(x) = \int\limits_{1}^{x} \frac{t-1}{2} = \frac{(t-1)^2}{4}\big|_{1}^{x} = \frac{(x-1)^2}{4} - 0 = \frac{(x-1)^2}{4}$$

Such that we may define the monotone function $u(x)$ by: 

$$u(x) = 
\begin{cases}
  0 & x \leq 1 \\
  \frac{(x-1)^2}{4} & 1 < x < 3\\
  1 & x \geq 3
\end{cases}$$


\newpage

# 7. 2.22 (a, b): 
>> Q: Let X have the pdf: 

$$f(x) = \frac{4}{\beta^3\sqrt{\pi}}x^2e^{\frac{-x^2}{\beta^2}}$$, $0 < x < \infty$, $\beta>0$

(a): Verify that $f(x)$ is a pdf.  

(b): Find $E(X)$

>> A:

(a):

HINT: 
Integration by parts
$u = x$, $dv = xe^{-\frac{x^2}{\beta^2}}$

(b): 

HINT: $\mathbb{E}(X) = \frac{2\beta}{\sqrt{\pi}}$

Note: We assume 
$$1 = \int\limits_{\infty}^{\infty}{\frac{1}{\sqrt{2\pi}}x^2 e^{\frac{-x^2}{2}}dx} = \frac{2}{\sqrt{2\pi}}\int\limits_{0}^{\infty}{x^2 e^{\frac{-x^2}{2}}dx}$$

For $f(x)$ as specified, we have: 

$$E(X) = \int\limits_{0}^{\infty}{xf(x)dx} = \int\limits_{0}^{\infty}{x\frac{4}{\beta^3\sqrt{\pi}}x^2e^{\frac{-x^2}{\beta^2}}dx}$$

\newpage

# 8. 
>> Q: Suppose that a random variable U has a Uniform(0,1) distribution 

(i.e. pdf $f_U(u) = 1$ for 0 < u < 1)

(a): Suppose a random variable Xhas a cdf $F(x)$ which is strictly increasing and continuous on $x \in \mathbb{R}$; this implies that, for any real value of 0 < u < 1, there is an inverse $F^{-1}(u) = x \in \mathbb{R}$ so that $F(x) = F(F^{-1}(u)) = u$. Define a random variable $Y = F^{-1}(U)$ based on the random variable U. Show that X and Y have the same cdf (i.e. the same distributions).

Hint: Use that, because F is strictly increasing, $P(Y \leq y) = P(F(Y) \leq F(y))$ holds for any $y \in \mathbb{R}$, i.e., Y can be less than or equal to y if and only if F(Y) is less than or equal to $F(y)$. Noe that $F(y) \in (0, 1)$ for any real y. 

(b): If there is a computer program (i.e. random number generator) that produces numbers uniformly distributed between zero and one (i.e., according to the pdf $F_U(u)$), explain how these numbers could be  used to generate values distributed according to the pdf $f_Z(z) = \frac{e^{-|z|}}{2}$, $-\infty < z < \infty$. 

Hint: Use (a) where F now becomes the cdf of Z; you need to find $F^{-1}(u)$ for a given 0 < u < 1 by solving the expression $F(z) = u$ for $z \in \mathbb{R}$

>> A: 

(a): 

(b): 
