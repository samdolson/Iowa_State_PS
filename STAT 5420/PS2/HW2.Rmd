---
output:
  pdf_document: default
  html_document: default
---

# HW 2 
[Name: Sam Olson]{.smallcaps} \
[Collaborators: Kyu-Tae, Ben, Sarah, Sabrina]{.smallcaps} \

# 1. 
>> Q: Suppose a random variable X has the following cdf from class (which is neither a step function nor continuous):  

$$F(x) = 
\begin{cases}
  0 & x < 0 \\
  (1+x)/2 & 0 \leq x \leq 1 \\ 
  1 & x > 1
\end{cases}$$

(a): Find the following probabilities: 
  $P(X > \frac{1}{2})$
  $P(X \geq \frac{1}{2})$
  $P(0 < X \leq \frac{1}{2})$
  $P(0 \leq X \leq \frac{1}{2})$

(b): Conditional on the event "X > 0", the corresponding conditional pdf of X (i.e. given X > 0) is as follows at $x \in \mathbb{R}$: 

$$P(X \leq x | X > 0 ) = \frac{P(X \leq x , X > 0 )}{P(X > 0)} = \frac{P(0 < X \leq x)}{P(X > 0)} = \frac{F(x) - F(0)}{1 - F(0)}$$

Giving: 

$$P(X \leq x | X > 0 ) = 
\begin{cases}
  0 & x \leq 0 \\
  x & 0 < x \leq 1 \\ 
  1 & x > 1
\end{cases}$$

Based on the conditional cdf above, show that the distribution of X, conditional on "X > 0", is the same (i.e. has the same cdf) as that of a random variable Y which is "uniform" on the interval (0, 1), having constant pdf $f_Y(y) = 1$ for $0 < y < 1$ (with $f_Y(y) = 0$ for all other $y \in \mathbb{R}$)

>> A: 

(a):

Note: The random variable X is continous for $0 \leq x \leq 1$

$$F(x) = 
\begin{cases}
  0 & x < 0 \\
  (1+x)/2 & 0 \leq x \leq 1 \\ 
  1 & x > 1
\end{cases}$$

$$f(x) = 
\begin{cases}
  0 & x < 0 \\
  1/2 & 0 \leq x \leq 1 \\ 
  0 & x > 1
\end{cases}$$

Given the above cdf of X, we may write the pdf of X for $0 \leq x \leq 1$ as: 

$$\mathbb{d}(F(x)) / \mathbb{dx} = \mathbb{d}((1+x)/2) / \mathbb{dx} = \frac{1}{2}$$

We then solve the following relations: 

  $$P(X > \frac{1}{2}) = \int\limits_{1/2}^{1} f(x) dx = \int\limits_{1/2}^{1} \frac{1}{2}dx = \frac{1}{2} - \frac{1}{4} = \frac{1}{4}$$
  
  $$P(X \geq \frac{1}{2}) = \int\limits_{1/2}^{1} f(x) dx = \int\limits_{1/2}^{1} \frac{1}{2}dx = \frac{1}{2} - \frac{1}{4} = \frac{1}{4}$$
  
  $$P(0 < X \leq \frac{1}{2}) = \int\limits_{0}^{1/2} f(x) dx = \int\limits_{0}^{1/2} \frac{1}{2}dx = \frac{1}{4} - 0 = \frac{1}{4}$$
  
  $$P(0 \leq X \leq \frac{1}{2}) = \int\limits_{0}^{1/2} f(x) dx = \int\limits_{0}^{1/2} \frac{1}{2}dx = \frac{1}{4} - 0 = \frac{1}{4}$$

(b): 

We are given the following relation to hold (given the definition of conditional probability): 

$$P(X \leq x | X > 0 ) = \frac{P(X \leq x , X > 0 )}{P(X > 0)} = \frac{P(0 < X \leq x)}{P(X > 0)} = \frac{F(x) - F(0)}{1 - F(0)}$$

For $x > 1$,

$P(X \leq x | X > 0 ) =$

And for $x < 0$, 

$P(X \leq x | X > 0 ) =$

Then for $0 < x \leq 1$ we have: 

$$P(X \leq x | X > 0 ) = \frac{F(x) - F(0)}{1 - F(0)} = \frac{\frac{(x+1)}{2} - \frac{1}{2}}{1 - \frac{1}{2}} = = \frac{\frac{(x)}{2} + \frac{1}{2} - \frac{1}{2}}{\frac{1}{2}} = \frac{x}{2} / \frac{1}{2} = x$$



We may then conclude: 

$$P(X \leq x | X > 0 ) = 
\begin{cases}
  0 & x \leq 0 \\
  x & 0 < x \leq 1 \\ 
  1 & x > 1
\end{cases}$$

The above may be considered the cdf of the distribution of X, conditional on "X > 0". 

Consider then a random variable Y which is "uniform" on the interval (0, 1), having constant pdf $f_Y(y) = 1$ for $0 < y < 1$ (with $f_Y(y) = 0$ for all other $y \in \mathbb{R}$), as for a random variable $Y ~ \text{Uniform}(0,1)$. We may write its pdf as: 

$$f_Y(y) = 
\begin{cases}
  1 & 0 < y < 1 \\
  0 & otherwise \\ 
\end{cases}$$

Taking this, we may find its cdf as:

$$F_Y(y) = 
\begin{cases}
  0 & y \leq 0 \\
  y & 0 < y \leq 1 \\ 
  1 & y > 1
\end{cases}$$

Note: I am unsure what can be taken for granted in this instance of "what we know" about Y, i.e. "we know its cdf". In that vein, I want to emphasize that as $\int1dy = y$ and $\int\limits_{0}^{1} 1 dy = y \big|_{0}^{1} = 1 - 0 = 1$, we may write the above cdf of Y, $F_Y(y)$ as given. 

And conclude that: Based on the conditional cdf above, that the distribution of X, conditional on "X > 0", is the same (has the same cdf) as that of a random variable Y which is "uniform" on the interval (0, 1). 

\newpage

# 2. 
>> Q: Statistical reliability involves studying the time to failure of manufactured units. In many reliability textbooks, one can find the exponential distribution: 

$$f(x) = 
\begin{cases}
  \frac{1}{\theta} e^{-\frac{x}{\theta}} & x > 0 \\
  0 & x \leq 0
\end{cases}$$

where $\theta > 0$ is a fixed value, for modeling the time X that a random unit runs until failure (i.e. X is a survival time). Show that if X has an exponential distribution as above, then: 

$P(X > s + t | X > t) = P(X > s)$

for any values t, s > 0; this feature is called the "memoryless" property of te exponential distribution. 

>> A: 

Let X be a random variable with Exponential distribution as given above, with parameter $\theta > 0$. Let t, s > 0. 

For x > 0, the pdf given is $\frac{1}{\theta} e^{-\frac{x}{\theta}}$, thus, for the same x > 0 the cdf is: 

$F_X(x) = \int_{x>0} f(x) dx = \int{\frac{1}{\theta} e^{-\frac{x}{\theta}}}dx = 1 - e^{-\frac{x}{\theta}}$

Thus: 

$P(X > s + t | X > t) = P(X > s) = \frac{P(X > s + t, X > t)}{P(X > t)}$

$P(X > s + t | X > t) = P(X > s) = \frac{P(X > s + t)}{P(X > t)}$

$P(X > s + t | X > t) = \frac{1 - F_X(s + t)}{1 - F_X(t)} = = \frac{1 - P(X \leq s + t)}{1 - P(X \leq t)}$

With note of the following relation: 

$$F_X(s) = \int\limits_{0}^{s}{\frac{1}{\theta}e^\frac{-x}{\theta}dx} = (-e^{\frac{-s}{\theta}}) - (-e^{\frac{0}{\theta}}) = (-e^{\frac{-s}{\theta}}) - (-1) = 1 - e^{\frac{-s}{\theta}}$$
We then have: 

$$P(X > s + t | X > t) = \frac{1 - (1-\frac{1}{\theta} e^{-\frac{s + t}{\theta}})}{1 - (1-\frac{1}{\theta} e^{-\frac{t}{\theta}})}$$

Cancelling out (most) like terms gives us: 

$$P(X > s + t | X > t) = \frac{1 - F(s+t)}{1 - F(t)} = \frac{e^{-\frac{s + t}{\theta}}}{e^{-\frac{ t}{\theta}}} = e^{\frac{-(s+t)-(-t)}{\theta}} =e^{-\frac{s}{\theta}}$$

However, we know that this is exactly $P(X > s) = 1 - P(X \leq s) = 1 - (1 - e^{-\frac{s}{\theta}}) = e^{-\frac{s}{\theta}}$!, giving us: 

$P(X > s + t | X > t) = P(X > s)$

\newpage

# 3. 2.3: 
>> Q: Suppose X has the Geometrc pmf: 

$f_X(x) = \frac{1}{3}(\frac{2}{3})^x$, $x = 0, 1, 2, ...$
Determine the probability distribution of $Y = \frac{X}{X+1}$
Note that here X and Y are discrete random variables. To specify the probability distribution of Y, specify its pmf. 

>> A: 

$f_Y(y) = P(Y = y) = P(\frac{X}{X+1} = y)$

Using this relation we have: 
$y(X+1) = X \rightarrow yX + y = X \rightarrow y = X - yX \rightarrow y = X(1-y)$

Thus we have: 
$X = \frac{y}{1-y}$

Returning then to the original function for the pmf, we have: 

$f_Y(y) = P(X = \frac{y}{1-y}) = \frac{1}{3}(\frac{2}{3})^\frac{y}{1-y}$

We must then identify the support of Y given $x = 0, 1, 2, ...$

For the support of X as given, $x = 0, 1, 2, ... \rightarrow y = \frac{X}{X+1} = \frac{0}{1}, \frac{1}{2}, \frac{2}{3}, ...$

Thus we define the discrete random variable Y by its pmf and support respectively as: 

$f_y(y) = \frac{1}{3}(\frac{2}{3})^\frac{y}{1-y}$ for $y = 0, \frac{1}{2}, \frac{2}{3}, ...$

\newpage

# 4. 2.4: 
>> Q: 

Let $\lambda$ be a fixed positive constant, and define the function $f(x)$ by: 

$f(x) = \frac{1}{2}\lambda e^{-\lambda x}$ if $x \geq 0$ 
and $f(x) = \frac{1}{2}\lambda e^{\lambda x}$ if $x < 0$

(a): Verify that $f(x)$ is a pdf. 

(b): If X is a random variable with pdf given by $f(X)$, find $P(X < t)$ $\forall t$. Evaluate all integrals.

(c): Find $P(|X| < t)$ $\forall t$. Evaluate all integrals. 

>> A: 

(a): 
(1): $f(x)$ is a pdf so long as it is well defined, i.e. $f(x) \geq 0$ $\forall x \in \mathbb{X}$ 
(2): and so long as 
$\int\limits_{x \in \mathbb{X}}{f(x)dx} = 1$

Then $f(x)$ is a (proper) pdf

(1): $f(x)$ is well-defined, i.e. ever negative.  

For $x \geq 0$, $e^{-x} \geq 0$, so by including additional, fixed (positive!) constants such as $\lambda$, $f(x) \geq 0$ for $x \geq 0$. 

For $x < 0$, $f(x) = e^{\lambda x} \geq 0$, so by including additional, fixed positive constants such as $\lambda$, $f(x) \geq 0$ for $x < 0$

Taken collectively, $f(x) \geq 0$ for all $x \in \mathbb{X}$

(2):
$$\int\limits_{x \in \mathbb{X}}{f(x)dx} = \int\limits_{x < 0}{\frac{1}{2}\lambda e^{\lambda x}} + \int\limits_{x \geq 0}{\frac{1}{2}\lambda e^{-\lambda x}}$$ 

$$\int\limits_{x \in \mathbb{X}}{f(x)dx} = \int\limits_{-\infty}^{0}{\frac{1}{2}\lambda e^{\lambda x}} + \int\limits_{0}^{\infty}{\frac{1}{2}\lambda e^{-\lambda x}}$$

Note, we can factor out a constant term from both integrals, giving us: 

$$\int\limits_{x \in \mathbb{X}}{f(x)dx} = \frac{1}{2}\lambda(\int\limits_{-\infty}^{0}{e^{\lambda x}} + \int\limits_{0}^{\infty}{e^{-\lambda x}}) = \frac{1}{2}\lambda [\frac{e^{\lambda x}}{\lambda}\big|_{-\infty}^{0} + (-\frac{e^{-\lambda x}}{\lambda}\big|_{0}^{\infty})]$$

$$\int\limits_{x \in \mathbb{X}}{f(x)dx} = \frac{1}{2}\lambda(\frac{1}{\lambda} - (-\frac{1}{\lambda})) = \frac{1}{2}\lambda (\frac{2}{\lambda}) = 1$$

We may then conclude that $f(x)$ is a (proper) pdf. 

(b):

If X is a random variable with pdf given by $f(X)$, find $P(X < t)$ $\forall t$.

$$P(X < t) = 
\begin{cases}
  \int\limits_{-\infty}^{t}\frac{1}{2}\lambda{e^{\lambda x}}dx & t > 0 \\
  \int\limits_{-\infty}^{0}\frac{1}{2}\lambda{e^{\lambda x}}dx + \int\limits_{0}^{t}\frac{1}{2}\lambda{e^{-\lambda x}}dx & t \geq 0
\end{cases}$$

We then evaluate the integrals of each, giving: 

(1): 

$\int\limits_{-\infty}^{t}\frac{1}{2}\lambda{e^{\lambda x}}dx = \frac{1}{2}\lambda e^{\lambda t}\big|_{-\infty}^{t} = \frac{1}{2} e^{\lambda t} - 0 = \frac{1}{2} e^{\lambda t}$

(2)

$\int\limits_{0}^{t}\frac{1}{2}\lambda{e^{-\lambda x}}dx = {-\frac{1}{2}e^{-\lambda x}}\big|_{0}^{t} = \frac{1}{2} - \frac{1}{2}e^{-\lambda t}$ 

(3): 

$\int\limits_{-\infty}^{0}\frac{1}{2}\lambda{e^{\lambda x}}dx = {\frac{1}{2}e^{\lambda x}}\big|_{-\infty}^{0} = \frac{1}{2} - 0$

(4): For the case of (2) + (3), 

$$\frac{1}{2} + \frac{1}{2} - \frac{1}{2}e^{-\lambda t} = 1 - \frac{1}{2}e^{-\lambda t}$$

Thus we're left with:

$$P(X < t) = 
\begin{cases}
  \frac{1}{2}e^{\lambda t} & t > 0 \\
   1 - \frac{1}{2}e^{-\lambda t} & t \geq 0
\end{cases}$$


(c):

$P(|X| < t)$ $\forall t$, 

$$P(|X| < t) = P(-t < X < t)= \int\limits_{-t}^{0}\frac{1}{2}\lambda{e^{\lambda x}} + \int\limits_{0}^{t}\frac{1}{2}\lambda{e^{-\lambda x}}$$

$$P(|X| < t) = \frac{1}{2}[\frac{e^{\lambda x}}{\lambda}\big|_{-t}^{0} + (-\frac{e^{-\lambda x}}{\lambda}\big|_{0}^{t})] = \frac{1}{2} [(1 - e^{-\lambda t}) + (-e^{-\lambda t} + 1)] = \frac{1}{2}(2)(1 - e^{-\lambda t}) = 1-e^{-\lambda t}$$ 


\newpage

# 5. 2.6 (b, c): 
>> Q: In each of the following find the pdf of Y. (Do not need to verify the pdf/evaluate the integration, per Instructions).  

(b): $f_X(x) = \frac{3}{8}(x + 1)^2$, $-1 < x < 1$; $Y = 1 - X^2$

(c): $$f_X(x) = \frac{3}{8}(x + 1)^2$$, $-1 < x < 1$; $Y = 1 - X^2$ if $X \leq 0$ and $Y = 1 - X$ if $X > 0$

>> A: 

(b): 
$f_X(x) = \frac{3}{8}(x + 1)^2$, $-1 < x < 1$; $Y = 1 - X^2$

Then for the pdf of Y, we have: 

$$f_Y(y) =
\begin{cases}
  \sum \limits_{i=1}^{k} f_X(g^{-1}_i(y))|\frac{d}{dy}g_{i}^{-1}(y)| & y \in \mathbb{Y} \\
  0 & otherwise \\ 
\end{cases}$$

Let us consider the following to motivate our partitions of the sample space: 

```{r}
x <- seq(from = -1, to = 1, by = 0.01)
y <- (1 - x^2)
plot(x = c(-1, 1), y = c(0, 1), xlab = "x", ylab = "y")
lines(x, y)
abline(v = 0)
```

We see three distinct partitions to ensure monotone functions: 

$A_1 = (-1, 0)$ 
$A_2 = \{ 0 \}$
$A_3 = (0, 1)$ 

We then have their respective functions, $g_i(x)$ as follows: 

$g_1 = (1 - x^2)$
$g_2 = 0$
$g_3 = (1 - x^2)$

Then, with note from the results of the following theorem (2.1.8): 

$$f_Y(y) =
\begin{cases}
  \sum \limits_{i=1}^{k} f_X(g^{-1}_i(y))|\frac{d}{dy}g_{i}^{-1}(y)| & y \in \mathbb{Y} \\
  0 & otherwise \\ 
\end{cases}$$

We have, for $0<y<1$, 

$g_1(x) = g_3(x) = 1 - x^2 \rightarrow g^{-1}(y) = - (1-y)^{1/2}$

$\therefore |\frac{d}{dy}g_{1}^{-1}(y)| = \frac{1}{2(1-y)^{1/2}} = |\frac{d}{dy}g_{3}^{-1}(y)|$

Note however that we are dealing with two distinct functions, one positive and the other negative: 

(1):
$$f_X(g^{-1}_1(y))|\frac{d}{dy}g_{1}^{-1}(y)| = \frac{3}{8}(1 - (1-y)^{1/2})^2(\frac{1}{2(1-y)^{1/2}})$$

(2): 
$$f_X(g^{-1}_3(y))|\frac{d}{dy}g_{3}^{-1}(y)|= \frac{3}{8}(1 + (1-y)^{1/2})^2(\frac{1}{2(1-y)^{1/2}})$$

Such that we combine (1) and (2) together to get, for $0 < y < 1$:

$$f_Y(y) = \frac{3}{8}(1 - (1-y)^{1/2})^2(\frac{1}{2(1-y)^{1/2}}) + \frac{3}{8}(1 + (1-y)^{1/2})^2(\frac{1}{2(1-y)^{1/2}}) = \frac{3}{8}\frac{1}{2}(1-y)^{-1/2}[(1 - (1-y)^{1/2})^2 + (1 + (1-y)^{1/2})^2]$$

Notice the second term of the expansion between the two values will cancel each other out, leaving us (after much algebra and simplification):

$$f_Y(y) = 
\begin{cases}
  f_Y(y) = \frac{3}{8}(1-y)^{-1/2} + \frac{3}{8}(1-y)^{1/2} & 0 < y \leq 1 \\
  0 & \text{otherwise}
\end{cases}$$

(c): 
$$f_X(x) = \frac{3}{8}(x + 1)^2$$, $-1 < x < 1$; $Y = 1 - X^2$ if $X \leq 0$ and $Y = 1 - X$ if $X > 0$

Similar to part (b), we see three distinct partitions to ensure monotone functions: 

$A_1 = (-1, 0)$ 
$A_2 = \{ 0 \}$
$A_3 = (0, 1)$ 

We then have their respective functions, $g_i(x)$ as follows: 

$g_1 = (1 - x^2)$
$g_2 = 0$
$g_3 = (1 - x^2)$

Thus, with note of the relevant theorem:

$$f_Y(y) =
\begin{cases}
  \sum \limits_{i=1}^{k} f_X(g^{-1}_i(y))|\frac{d}{dy}g_{i}^{-1}(y)| & y \in \mathbb{Y} \\
  0 & otherwise \\ 
\end{cases}$$

Such that for $0 < y < 1$: 

$g_1(x) = 1 - x^2 \rightarrow g^{-1}(y) = (1-y)^{1/2}$

$\therefore |\frac{d}{dy}g_{1}^{-1}(y)| = \frac{1}{2(1-y)^{1/2}}$

$g_3(x)=1 - x \rightarrow g^{-1}(y) = 1 - y$

$\therefore |\frac{d}{dy}g_{i}^{-1}(y)| = |-1 = 1$

There are two relevant summations: 

(1): 
$$\frac{3}{8}(1- (1-y)^{1/2})^2\frac{1}{2}(1-y)^{-1/2} = \frac{3}{16}(1-(1-y)^{1/2})^2(1-y)^{-1/2}$$

(2): 
$$\frac{3}{8}((1-y) + 1)^2 = \frac{3}{8}(2 - Y)^2$$

Taken together, we have the sum of (1) and (2), written:

$$f_Y(y) = 
\begin{cases}
  f_Y(y) = \frac{3}{16}(1-(1-y)^{1/2})^2(1-y)^{-1/2} + \frac{3}{8}(2-y)^{2} & 0 < y \leq 1 \\
  0 & \text{otherwise}
\end{cases}$$

\newpage

# 6. 2.9: 
>> Q: If the random variable X has pdf: 
\[
f(x) = 
\begin{cases}
  \frac{x-1}{2} & 1 < x < 3 \\
  0 & \text{otherwise}
\end{cases}
\]

find a monotone function $u(x)$ such that the random variable $Y = u(X)$ has a Uniform(0,1) distribution.

>> A: 

We may take advantage of Thm 2.1.10, and let the random variable Y be defined as $Y = u(X) = F_x(x)$

Taking advantage of the fact that $u(x) = F_x(x) \rightarrow F_x(X)$ ~Uniform $(0, 1)$

That is to say define the random variable Y as the cdf of the random variable X. 

$F_x(x) = P(X \leq x) = \int\limits_{-\infty}^{x}{f(t)dt} = \int\limits_{-\infty}^{x} \frac{t-1}{2} = \int\limits_{-\infty}^{1} \frac{t-1}{2} + \int\limits_{1}^{x} \frac{t-1}{2} = 0 + \int\limits_{1}^{x} \frac{t-1}{2}$

$$F_x(x) = \int\limits_{1}^{x} \frac{t-1}{2} = \frac{(t-1)^2}{4}\big|_{1}^{x} = \frac{(x-1)^2}{4} - 0 = \frac{(x-1)^2}{4}$$

Such that we may define the monotone function $u(x)$ by: 

$$u(x) = 
\begin{cases}
  0 & x \leq 1 \\
  \frac{(x-1)^2}{4} & 1 < x < 3\\
  1 & x \geq 3
\end{cases}$$


\newpage

# 7. 2.22 (a, b): 
>> Q: Let X have the pdf: 

$$f(x) = \frac{4}{\beta^3\sqrt{\pi}}x^2e^{\frac{-x^2}{\beta^2}}$$, $0 < x < \infty$, $\beta>0$

(a): Verify that $f(x)$ is a pdf.  

(b): Find $E(X)$

>> A:

(a):

There are two conditions to verify that $f(x)$ is a pdf, the first is: 
(1): $f(x) \geq 0$, $\forall{x}$. This one is apparent under the conditions $0 < x < \infty$, $\beta>0$. We must then establish condition (2): 

(2): $\int_{\Omega} f(x)dx = 1$, or, the sum of the pdf over the sample space is 1 (note: this is for the continuous case, which we have). 

We thus have: 

$$\int_{\Omega} f(x)dx = \int\limits_{0}^{\infty}{\frac{4}{\beta^3\sqrt{\pi}}x^2e^{\frac{-x^2}{\beta^2}}} dx$$
Set $\frac{y}{\sqrt{2}} = \frac{x}{\beta} \rightarrow dx = \frac{\beta}{\sqrt{2}} dy$

And $x^2 = \frac{\beta^2 y^2}{{2}}$

Such that we may write:

$$\int_{\Omega} f(x)dx = \int\limits_{0}^{\infty}{\frac{4}{\beta^3\sqrt{\pi}}\frac{\beta^2 y^2}{{2}}e^{\frac{-y^2}{2}}} \frac{\beta}{\sqrt{2}} dy= \int\limits_{0}^{\infty} \frac{2}{\sqrt{2 \pi}} y^2 e^{-y^2/2} dy$$ 
We may then make use of our assumption/hint, namely: 

$$1 = \int\limits_{\infty}^{\infty}{\frac{1}{\sqrt{2\pi}}x^2 e^{\frac{-x^2}{2}}dx} = \frac{2}{\sqrt{2\pi}}\int\limits_{0}^{\infty}{x^2 e^{\frac{-x^2}{2}}dx} \rightarrow \sqrt{2 \pi}/2 = \int\limits_{0}^{\infty}{x^2 e^{\frac{-x^2}{2}}dx}$$
Incorporating this into the above relation on $f(x)$ gives us (taking out the constant term from the integral): 

$$\int_{\Omega} f(x)dx = \frac{2}{\sqrt{2 \pi}} \int\limits_{0}^{\infty} y^2 e^{-y^2/2} dy = \frac{2}{\sqrt{2 \pi}} \frac{\sqrt{2 \pi}}{2} = 1$$
We have shown then that conditions (1) and (2) hold, and as such, $f(x)$ is a pdf! 

(b): 

Q: Find $\mathbb{E}(X)$

Note: For the random variable X given from the prior $f(x)$, we have $\mathbb{E}(X) = \int_{\Omega} x f(x)dx$

We may calculate this as follows: 

$$\mathbb{E}(X) = \int\limits_{0}^{\infty}x{\frac{4}{\beta^3\sqrt{\pi}}x^2e^{\frac{-x^2}{\beta^2}}} dx$$

Let us take note of Integration by parts, that is: 

$$\int udv = uv - \int v du$$

For the above relation, let 
$$u = \frac{4x^2}{\beta^3 \sqrt{\pi}} \rightarrow du = \frac{8x}{\beta^3 \sqrt{\pi}}$$ 
and 
$$dv = x e^{-\frac{x^2}{\beta^2}} \rightarrow v = \int\limits_{0}^{\infty}{x e^{-\frac{x^2}{\beta^2}}}$$
Of interest is $uv$, which may be written: 
<!-- \biggl[_{0}^{\infty} \biggr] -->
$$uv = [\frac{4}{\beta^3 \sqrt{\pi}}x^2 (-\frac{\beta^2}{2} e^{-\frac{x^2}{\beta^2}})] \big|_{0}^{\infty}$$

Note: We have a number of constants, such that the above simplifies to: 

$$uv = \frac{4}{\beta^3\sqrt{\pi}} (-\frac{\beta^2}{2})[x^2 e^{-\frac{x^2}{\beta^2}}] \big|_{0}^{\infty}$$

And we note the following: 

$[x^2 e^{-\frac{x^2}{\beta^2}}] \big|_{0}^{\infty} = 0 - 0 = 0$

Such that our term $uv$ is equal to zero, leaving us with: 

$$\mathbb{E}(X) = 0 + \int\limits_{0}^{\infty}x{\frac{4}{\beta^3\sqrt{\pi}}x^2e^{\frac{-x^2}{\beta^2}}} dx = 0 + \frac{4}{\beta \sqrt{\pi}}\int\limits_{0}^{\infty}{x e^{-\frac{x^2}{\beta^2}}}dx$$

$$\mathbb{E}(X) = 0 + \frac{4}{\beta \sqrt{\pi}}(-\frac{1}{2}\beta^2 e^{-\frac{x^2}{\beta^2}} \big|_{0}^{\infty}) =  \frac{4}{\beta \sqrt{\pi}} \frac{\beta^2}{2} = \frac{2 \beta}{\sqrt{\pi}}$$

We conclude then:

$$\mathbb{E}(X) = \frac{2 \beta}{\sqrt{\pi}}$$

\newpage

# 8. 
>> Q: Suppose that a random variable U has a Uniform(0,1) distribution 

(i.e. pdf $f_U(u) = 1$ for 0 < u < 1)

(a): Suppose a random variable X has a cdf $F(x)$ which is strictly increasing and continuous on $x \in \mathbb{R}$; this implies that, for any real value of 0 < u < 1, there is an inverse $F^{-1}(u) = x \in \mathbb{R}$ so that $F(x) = F(F^{-1}(u)) = u$. Define a random variable $Y = F^{-1}(U)$ based on the random variable U. Show that X and Y have the same cdf (i.e. the same distributions).

Hint: Use that, because F is strictly increasing, $P(Y \leq y) = P(F(Y) \leq F(y))$ holds for any $y \in \mathbb{R}$, i.e., Y can be less than or equal to y if and only if F(Y) is less than or equal to $F(y)$. Noe that $F(y) \in (0, 1)$ for any real y. 

(b): If there is a computer program (i.e. random number generator) that produces numbers uniformly distributed between zero and one (i.e., according to the pdf $F_U(u)$), explain how these numbers could be  used to generate values distributed according to the pdf $f_Z(z) = \frac{e^{-|z|}}{2}$, $-\infty < z < \infty$. 

Hint: Use (a) where F now becomes the cdf of Z; you need to find $F^{-1}(u)$ for a given 0 < u < 1 by solving the expression $F(z) = u$ for $z \in \mathbb{R}$

>> A: 

(a): 

Let U and X be random variables. 

Define the following relations to hold: 

For any real value of 0 < u < 1, there is an inverse $F^{-1}(u) = x \in \mathbb{R}$ so that $F(x) = F(F^{-1}(u)) = u$. 

Let us then define a random variable Y as follows: $Y = F^{-1}(U)$

Note: F is strictly increasing, and $F^{-1}$ is also strictly increasing. 

Thus if we define $Y \leq y \rightarrow F(Y) \leq F(y)$. Similarly, if we define $F(Y) \leq F(y) \rightarrow F^{-1}(F(Y)) \leq F^{-1}(F(y)) \rightarrow Y \leq y$

Such that we have shown: 

$$Y \leq y \iff F(Y) \leq F(y)$$ for a strictly increasing function $F$. 

Then consider the cdf of the random variable X, and the following consequence of F being strictly increasing: 

$$F(x) = F(F^{-1}(u)) = u \rightarrow F^{-1}(u) = x$$

Given Our relations of the random variables Y and U, namely that F is strictly increasing, then the values the random variables take, y and u respectively may be written: 

$$Y = F^{-1}(U) \rightarrow y = F^{-1}(u) \rightarrow F(y) = u$$

such that the above relations give us: 

(1):

$$F(Y) =P(Y \leq y) = P(F^{-1}(U) \leq F^{-1}(u)) = P(F(F^{-1}(U)) \leq F(F^{-1}(u))) = P(U \leq u)$$

(2): 

$$F(X) = P(F(X) \leq u) = P(U \leq u)$$


And taking (1) and (2) together, we may conclude that the random variable X and Y have the same cdfs. 

(b): 

We are given the pdf of Z, so we derive its cdf as follows: 

$$F_Z(z) =
\begin{cases}
  \int\limits_{-\infty}^{z} \frac{e^t}{2} dt & z < 0 \\
  \int\limits_{-\infty}^{0} \frac{e^t}{2} dt + \int\limits_{0}^{z} \frac{e^-t}{2} dt  & z > 0
\end{cases}$$

We then evaluate the following integrals such that we have: 

$$\int\limits_{-\infty}^{z} \frac{e^t}{2} dt = \frac{e^z}{2}$$

$$\int\limits_{-\infty}^{0} \frac{e^t}{2} dt = \frac{1}{2}$$

$$\int\limits_{0}^{z} \frac{e^{-t}}{2} = \frac{1}{2} - \frac{1}{2}e^{-z}$$ 
Such that we have, for $z > 0$: 

$F_Z(z) = \frac{1}{2} + \frac{1}{2} - \frac{1}{2}e^{-z} = 1 - \frac{1}{2}e^{-z}$

Thus we have the cdf of Z as: 

$$F_Z(z) =
\begin{cases}
  \frac{e^z}{2} & z < 0 \\
   1 - \frac{1}{2}e^{-z}  & z > 0
\end{cases}$$

We then take the inverse $F^{-1}$ to transform this from the random variable Z to the random variable U: 

$F^{-1}(\frac{e^z}{2}) = ln(2u)$

$F^{-1}(1 - \frac{1}{2}e^{-z}) = \frac{1}{ln(2-2u)}$

We may then write $F^{-1}(u)$

$$F_Z^{-1}(u) = 
\begin{cases}
   ln(2)-ln(u) & 0 < u < 1/2 \\
   \frac{1}{ln(2-2u)}  & 1/2 < u < 1 
\end{cases}$$