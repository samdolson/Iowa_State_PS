---
title: "HW6"
output: pdf_document
date: "2024-10-26"
---

# Homework 6

# Outline: 
  Q1: DONE 
  Q2: DONE 
  Q3: DONE  
  Q4: DONE 
  Q5: DONE 
  Q6: WIP
  Q7: WIP

# Q1: 4.17, Casella & Berger

Let X be an exponential(1) random variable, and define Y to be the integer part of X+1, that is: 

$$Y = i + 1 \text{ iff } i \leq X < i+1, i = 0, 1, 2, ...$$

## (a) 

Find the distribution of Y. What well-known distribution does Y have? 

$$P(Y = i + 1) = \int\limits_{i}^{i+1}e^{-x} dx = -e^{-x} \big|_{x = i}^{i+1} = -e^{-(i+1)} + e^{-i} = e^{-i}(1-e^{-1})$$

This is a geometric distribution with $p = 1-e^{-1}$, such that

$Y \sim Geom(1-e^{-1})$

## (b) 

Find the conditional distribution of X - 4 given $Y \geq 5$

As defined, $Y = i + 1$, such that 

$$Y \geq 5 \iff i + 1 \geq 5 \iff X \geq 4$$
Utilizing the distributions as defined and found, we then have

$$
P(X - 4 \leq x \mid Y \geq 5) = P(X - 4 \leq x \mid X \geq 4) = P(X \leq x + 4 \mid X \geq 4)
$$

$$
P(X - 4 \leq x \mid Y \geq 5)  = P(X \leq x + 4 \mid X \geq 4) = 1 - P(X > x + 4 \mid X \geq 4) = 1 - P(X > x) = 1 - e^{-x}
$$

This sure looks like the memoryless property we observed previously! 

$$
P(X - 4 \leq x \mid Y \geq 5) = P(X \leq x) = 1 - e^{-x}
$$

\newpage

# Q2:  4.32(a), Casella & Berger

## (a) 

For a hierarchical model: 

$$Y | \Lambda \sim Poisson(\Lambda) \text{ and } \Lambda \sim Gamma(\alpha, \beta)$$

find the marginal distribution, mean, and variance of Y. Show that the marginal distribution of Y is a negative binomial if $\alpha$ is an integer. 

For y = 0, 1, ..., we may write the conditional distribution of Y = y as: 

$$P(Y = y | \lambda) = \sum\limits_{n=y}^{\infty} P(Y = y | N = n, \lambda) P(N = n | \lambda) = \sum\limits_{n=y}^{\infty} {n \choose y} p^{y} (1-p)^{n-y} \frac{e^{-\lambda} \lambda^n}{n!}$$

$$P(Y = y | \lambda) = \sum\limits_{n=y}^{\infty} \frac{1}{y!(n-y)!} (\frac{p}{1-p})^{y} [(1-p)\lambda]^{n}e^{-\lambda}$$

Define $m = n - y$, such that we may rewrite the above as: 

$$P(Y = y | \lambda) = \sum\limits_{n=y}^{\infty} \frac{e^{-\lambda}}{y!m!} (\frac{p}{1-p})^{y} [(1-p)\lambda)^{m}] = \sum\limits_{n=y}^{\infty} \frac{e^{-\lambda}}{y!} (\frac{p}{1-p})^{y} \frac{[(1-p)\lambda)^{m}]}{m!}$$

After gathering the terms, we see quite a lot of this does not depend on m, such that we may take it out of the summation and write: 

$$P(Y = y | \lambda) = \frac{e^{-\lambda}}{y!} (\frac{p}{1-p})^{y} \sum\limits_{n=y}^{\infty}  \frac{[(1-p)\lambda)^{m}]}{m!}$$

After simplifying, we then take advantage that 

$$\sum\limits_{n=y}^{\infty}  \frac{[(1-p)\lambda)^{m}]}{m!} = e^{(1-p)\lambda}$$

And may write: 

$$P(Y = y | \lambda) = e^{-\lambda}(p\lambda)^{y}e^{(1-p)\lambda}= \frac{(p\lambda)^{y}e^{-p\lambda}}{y!}$$

Note the above is a type of Poisson, specifically: 

$$Y | \Lambda \sim Poisson(p\lambda)$$

From this we may "extract" the pmf of Y (pmf as both the conditional of Y and $\Lambda$ are both Poisson distributed), specifically for $y = 0 , 1, ...$, 

$$f_Y(y) = \frac{1}{\Gamma(\alpha) y! (p\beta)^{\alpha}} \Gamma(y + \alpha) (\frac{p\beta}{1+p\beta})^{y + \alpha}$$

For a positive integer $\alpha$, the above provides a pmf for a negative binomial distribution, specifically: 

$$Y \sim NB(\alpha, \frac{1}{1 + p\beta}$$

\newpage

# Q3

Expectation

## (a) 

Show that any random variable X (with finite mean) has zero covariance with any real constant c, i.e. $Cov(X, c) = 0$

The covariance of X and c may be written: 

$$Cov(X, c) = E[(X - E[X])(c - E[c])] = E[(X - E[X])(c-c)] = E[(X - E[X])0] = E[0] = 0$$

Such that we conclude:

$$Cov(X, c) = 0$$

And we must have the condition that X has a finite mean as $\infty * 0$ is undefined. 

## (b) 

Using the definition of conditional expectation, show that 

$$E[g(X)h(Y) | X = x] = g(x)E[h(Y) | X = x]$$

for an x with pdf $f_X(x) > 0$ (You may also assume (X, Y) are jointly discrete). 

To show that 

$$
E[g(X)h(Y) \mid X = x] = g(x)E[h(Y) \mid X = x]
$$

For jointly discrete random variables X and Y, the conditional expectation of h(Y) | X = x is:

$$
E[h(Y) \mid X = x] = \sum_y h(y) P(Y = y \mid X = x)
$$

Similarly, the conditional expectation of g(X)h(Y) | X = x is:

$$
E[g(X)h(Y) \mid X = x] = \sum_y g(x)h(y) P(Y = y \mid X = x)
$$

We can simplify by recognizing there are terms in the above equations that do not depend on the index of the summation, specifically:

$$
E[g(X)h(Y) \mid X = x] = \sum_y g(x) h(y) P(Y = y \mid X = x) = g(x) \sum_y h(y) P(Y = y \mid X = x)
$$

However, this is a very familiar formula to us! 

$$
E[g(X)h(Y) \mid X = x] = g(x) \sum_y h(y) P(Y = y \mid X = x)
=  g(x) E[h(Y) \mid X = x]
$$

Note: 

The condition $f_X(x) > 0$ is necessary as it ensures that $P(Y = y \mid X = x)$ is defined, because: 

$$
P(Y = y \mid X = x) = \frac{P(X = x, Y = y)}{P(X = x)} \equiv \frac{P(X = x, Y = y)}{f_X(x)}
$$


\newpage

# Q4

Suppose that $X_i$ has mean $\mu_i$ and variance $\sigma_i^2$, for i = 1, 2, and that the covariance of $X_1$ and $X_2$ is $\sigma_{12}$. Compute the covariance between $X_1 - 2X_2 + 8$, and then compute the covariance of $3X_1 + X_2$. 

## (a) 

$X_1 - 2X_2 + 8$

$$
Var(X_1 - 2X_2 + 8) = Cov(X_1 - 2X_2 + 8, X_1 - 2X_2 + 8) = Cov(X_1 - 2X_2, X_1 - 2X_2)
$$

$$
Cov(X_1 - 2X_2 + 8, X_1 - 2X_2 + 8) = Cov(X_1, X_1) - 2Cov(X_1, X_2) - 2Cov(X_2, X_1) + 4Cov(X_2, X_2) = \sigma_1^2 - 4\sigma_{12} + 4\sigma_2^2
$$

$$
Cov(X_1 - 2X_2 + 8, X_1 - 2X_2 + 8) = \sigma_1^2 - 4\sigma_{12} + 4\sigma_2^2.
$$

## (b)  

$3X_1 + X_2$

$$
Cov(3X_1 + X_2, 3X_1 + X_2) = Cov(3X_1, 3X_1) + Cov(3X_1, X_2) + Cov(X_2, 3X_1) + Cov(X_2, X_2)
$$

$$
Cov(3X_1 + X_2, 3X_1 + X_2) = 9\sigma_1^2 + 3\sigma_{12} + 3\sigma_{12} + \sigma_2^2
$$

\newpage

# Q5

The joint distribution of X, Y is given by the joint pdf: 

$$f(x, y) = 3 (x + y) \text{ for } 0<x<1, 0<y<1, 0<x+y<1$$

## (a) 

Find the marginal distribution of $f_X(x)$

$$
f_X(x) = \int_{y \in Y} f(x, y) \, dy = \int_{0}^{1} f(x, y) \, dy = \int_{0}^{1} 3 (x + y) dy
$$

However, the bounds of the integral as given above are not correct, as:

$$
0 < x < 1, \, 0 < y < 1, \, 0 < x + y < 1
$$
So we actually have: 

$$
f_X(x) = \int_{0}^{1-x} 3 (x + y) \, dy = 3 \int_{0}^{1-x} (x + y) \, dy = 3 \left[ \int_{0}^{1-x} x \, dy + \int_{0}^{1-x} y \, dy \right]
$$

For the sake of simplifcation, these are: 
### i. 

$$\int_{0}^{1-x} x \, dy = x(1 - x).$$
### ii. 

$$\int_{0}^{1-x} y \, dy = \frac{(1 - x)^2}{2}.$$

Taken together, we have: 

$$
f_X(x) = 3 \left[ x(1 - x) + \frac{(1 - x)^2}{2} \right] = 3 \left[ (1 - x) \left( x + \frac{1 - x}{2} \right) \right] = 3 (1 - x) \left( \frac{x + 1}{2} \right) = \frac{3}{2} (1 - x)(x + 1)
$$

Thus, the marginal distribution of X is:

$$
f_X(x) = \frac{3}{2} (1 - x)(x + 1), \quad \text{ for } 0 < x < 1.
$$

## (b) 

Find the conditional pdf of Y | X = x, given some 0<x<1. 

Using the definition of the conditional pdf, we have: 

$$
f_{Y | X}(y | x) = \frac{f(x, y)}{f_X(x)} = \frac{3 (x + y)}{\frac{3}{2} (1 - x)(x + 1)}
$$

For 

$$
0 < x < 1, \, 0 < y < 1, \, 0 < x + y < 1
$$

Simplifying gives us:

$$
f_{Y | X}(y | x) = \frac{2 (x + y)}{(1 - x)(x + 1)} \text{ for } 0 < y < 1 - x
$$

## (c) 

Find $E[Y | X = x]$

Using what we derived in part (b), we have: 

$$
E[Y \mid X = x] = \int_{0}^{1-x} y \, f_{Y | X}(y | x) \, dy = \int_{0}^{1-x} y \frac{2 (x + y)}{(1 - x)(x + 1)} = \frac{2}{(1 - x)(x + 1)} \int_{0}^{1-x} y (x + y) \, dy = \frac{2}{(1 - x)(x + 1)} \int_{0}^{1-x} yx + y^2 dy
$$

$$
E[Y \mid X = x] = \frac{2}{(1 - x)(x + 1)} [\left( \int_{0}^{1-x} xy \, dy \right) + \left( \int_{0}^{1-x} y^2 \, dy \right)]
$$

### i. 

$$
\int_{0}^{1-x} xy \, dy = x \int_{0}^{1-x} y \, dy = x \left[ \frac{(1 - x)^2}{2} \right] = \frac{x (1 - x)^2}{2}
$$
### ii. 

$$
\int_{0}^{1-x} y^2 \, dy = \left[ \frac{(1 - x)^3}{3} \right]
$$

Combining the two parts above, we then have: 

$$
E[Y \mid X = x] = \frac{2}{(1 - x)(x + 1)} \left( \frac{x (1 - x)^2}{2} + \frac{(1 - x)^3}{3} \right) = \frac{2 (1 - x)^2}{(1 - x)(x + 1)} \left( \frac{x}{2} + \frac{1 - x}{3} \right)
$$

Simplify, simplify: 

$$
E[Y \mid X = x] = \frac{2 (1 - x)}{x + 1} \left( \frac{3x + 2 - 2x}{6} \right) = \frac{2 (1 - x)}{x + 1} \left( \frac{x + 2}{6} \right) = \frac{(1 - x)(x + 2)}{3 (x + 1)}
$$

## (d)

Given the results in (a), (b), and (c), explain how you know $E[X | Y = y]$ without any further calculation

Given the above results, we can take advantage of symmetry, since the joint pdf of X and Y involves a simple sum of x + y, and the support of each is the same, i.e.  

$$
f(x, y) = 3 (x + y), \quad \text{ for } 0 < x < 1, \, 0 < y < 1, \, 0 < x + y < 1
$$

So we can effective "swap" any "x" in the prior calculations with "y" (and similarly if we felt inclined to derive everything again we could/would swap the "y" in our calculations with "x"). 

Given from (c):

$$
E[Y \mid X = x] = \frac{(1 - x)(x + 2)}{3 (x + 1)}
$$

By symmetry, we know:

$$
E[X \mid Y = y] = \frac{(1 - y)(y + 2)}{3 (y + 1)}
$$

## (e) 

Find $E[E[2XY - Y|X]]$

From the parts above, we know most everything but $E[XY]$. 

$$
E[E[2XY - Y \mid X]] = E[2XY - Y] = 2E[XY] - E[Y]
$$

Taking advantage of the symmetry property used in part (d), we can easily find the marginal pdf of Y. Namely, as: 

$$
f_X(x) = \frac{3}{2} (1 - x)(x + 1), \quad \text{ for } 0 < x < 1
$$

Then: 

$$
f_Y(y) = \frac{3}{2} (1 - y)(y + 1), \quad \text{ for } 0 < y < 1
$$

However, due to symmetry:

$$
E[Y] = E[X]
$$

So if we calculate $E[X]$, we effectively get $E[Y]$. Let's do that! 

$$
E[X] = \int_0^1 x \cdot f_X(x) \, dx = \int_0^1 x \frac{3}{2} (1 - x)(x + 1) \, dx = \int_0^1 \frac{3x}{2} (1 - x^2) = \frac{3}{2} \int_0^1 x (1 - x^2) \, dx = \frac{3}{2} \left( \int_0^1 x \, dx - \int_0^1 x^3 \, dx \right)
$$
### o. 

$$
\int_0^1 x \, dx = \frac{1}{2}
$$ 

$$
\quad \int_0^1 x^3 \, dx = \frac{1}{4}
$$

Taking the above gives us: 

$$
E[X] = \frac{3}{2} \left( \frac{1}{2} - \frac{1}{4} \right) = \frac{3}{2} \cdot \frac{1}{4} = \frac{3}{8}
$$

And $E[Y] = \frac{3}{8}$ too.

Last part now, we need to evaluate $E[XY]$: 

$$
E[XY] = \int_0^1 \int_0^{1-x} xy f(x,y) dy dx = \int_0^1 \int_0^{1-x} xy  3(x + y) \, dy \, dx = 3 \int_0^1 \int_0^{1-x} xy (x + y) =  \, dy \, dx = 3 \int_0^1 \int_0^{1-x} (x^2 y + xy^2) \, dy \, dx
$$

Alright, back to it, separating the integrals: 

### i. 

$$
\int_0^1 \int_0^{1-x} x^2 y \, dy \, dx = \int_0^1 x^2 \left( \frac{(1-x)^2}{2} \right) \, dx = \frac{1}{2} \int_0^1 x^2 (1 - x)^2 \, dx = \frac{1}{2} \left( \int_0^1 x^2 \, dx - 2 \int_0^1 x^3 \, dx + \int_0^1 x^4 \, dx \right)
$$

Where: 

$$
\int_0^1 x^2 \, dx = \frac{1}{3}
$$

$$
\quad \int_0^1 x^3 \, dx = \frac{1}{4}
$$

$$
\quad \int_0^1 x^4 \, dx = \frac{1}{5}
$$

And our total for the "first" term is then

$$
\frac{1}{2} \left( \frac{1}{3} - 2(\frac{1}{4}) + \frac{1}{5} \right) = \frac{1}{2} \left( \frac{1}{3} - \frac{1}{2} + \frac{1}{5} \right) = \frac{1}{2} \left( \frac{10}{30} - \frac{15}{30} + \frac{6}{30} \right) = \frac{1}{2}(\frac{1}{30}) = \frac{1}{60}
$$

### ii. 

$$
\int_0^1 \int_0^{1-x} xy^2 \, dy \, dx = \int_0^1 x \left( \frac{(1-x)^3}{3} \right) \, dx = \frac{1}{3} \int_0^1 x (1 - x)^3 \, dx = \frac{1}{3} \int_0^1 (x - 3x^2 + 3x^3 - x^4) \, dx
$$

$$
\frac{1}{3} \left( \frac{1}{2} - 3 \cdot \frac{1}{3} + 3 \cdot \frac{1}{4} - \frac{1}{5} \right) = \frac{1}{3} \left( \frac{1}{2} - 1 + \frac{3}{4} - \frac{1}{5} \right) = \frac{1}{3} \left( \frac{30}{60} - \frac{60}{60} + \frac{45}{60} - \frac{12}{60} \right) = \frac{1}{3} \cdot \frac{3}{60} = \frac{1}{60}
$$

Crazy, $\frac{1}{60}$ again...symmetry? 

Taking the two parts above, we then have: 

$$
E[XY] = 3 \left( \frac{1}{60} + \frac{1}{60} \right) = \frac{3}{30} = \frac{1}{10}
$$

$$
2E[XY] = 2 (\frac{1}{10}) = \frac{1}{5}
$$
And with:

$$
E[Y] = \frac{3}{8}
$$
We may finally calculate the desired value as: 

$$
E[E[2XY - Y \mid X]] = \frac{1}{5} - \frac{3}{8} = \frac{8}{40} - \frac{15}{40} = -\frac{7}{40} = - \frac{7}{40}
$$

\newpage

# Q6

Suppose that $f(x, y) = e^{-y} \text{ for } 0 < x < y < \infty$

## (a) 

Find the joint moment generating function for (X, Y). 

The joint moment generating function $M_{X, Y}(t_1, t_2)$ may be defined:

$$
M_{X, Y}(t_1, t_2) = E\left[e^{t_1 X + t_2 Y}\right] = \int_0^\infty \int_0^y e^{t_1 x + t_2 y} e^{-y} \, dx \, dy = \int_0^\infty \int_0^y e^{t_1 x} e^{(t_2 - 1) y} \, dx \, dy
$$

First, integrate with respect to \( x \). The inner integral is:

$$
\int_0^y e^{t_1 x} \, dx = \frac{1}{t_1} \left( e^{t_1 y} - 1 \right),
$$

assuming \( t_1 \neq 0 \).

Substitute the result into the outer integral:

$$
M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \int_0^\infty \left( e^{(t_1 + t_2 - 1) y} - e^{(t_2 - 1) y} \right) \, dy.
$$

Now, integrate term by term:

For \( e^{(t_1 + t_2 - 1) y} \):

$$
\int_0^\infty e^{(t_1 + t_2 - 1) y} \, dy = \frac{1}{1 - t_1 - t_2} \quad \text{for} \, t_1 + t_2 < 1.
$$

For \( e^{(t_2 - 1) y} \):

$$
\int_0^\infty e^{(t_2 - 1) y} \, dy = \frac{1}{1 - t_2} \quad \text{for} \, t_2 < 1.
$$

Now, combine the two results:

$$
M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right).
$$

Thus, the joint moment generating function for \( (X, Y) \) is:

$$
M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right),
$$

valid for \( t_1 + t_2 < 1 \) and \( t_2 < 1 \).

## (b) 

Use the joint moment generating function to find the variance of X, the variance of Y, and the covariance of X and Y. 

To find the variances of \( X \), \( Y \), and the covariance between \( X \) and \( Y \) using the joint moment generating function (MGF), we will compute the necessary partial derivatives of the MGF.

The joint MGF we found is:

$$
M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right),
$$

valid for \( t_1 + t_2 < 1 \) and \( t_2 < 1 \).

To find the means of \( X \) and \( Y \), we use the following formulas for the partial derivatives of the MGF:

- \( \mathbb{E}[X] = \frac{\partial}{\partial t_1} M_{X, Y}(t_1, t_2) \Big|_{t_1 = 0, t_2 = 0} \),
- \( \mathbb{E}[Y] = \frac{\partial}{\partial t_2} M_{X, Y}(t_1, t_2) \Big|_{t_1 = 0, t_2 = 0} \).

First, we differentiate \( M_{X, Y}(t_1, t_2) \) with respect to \( t_1 \):

$$
\frac{\partial}{\partial t_1} M_{X, Y}(t_1, t_2) = \frac{-1}{t_1^2} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right) + \frac{1}{t_1} \cdot \frac{1}{(1 - t_1 - t_2)^2}.
$$

Taking the limit as \( t_1 \to 0 \) and \( t_2 \to 0 \), we get:

$$
\mathbb{E}[X] = \frac{\partial}{\partial t_1} M_{X, Y}(t_1, t_2) \Big|_{t_1 = 0, t_2 = 0} = \frac{1}{1^2} = 1.
$$

Now, we differentiate \( M_{X, Y}(t_1, t_2) \) with respect to \( t_2 \):

$$
\frac{\partial}{\partial t_2} M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \left( \frac{1}{(1 - t_1 - t_2)^2} - \frac{1}{(1 - t_2)^2} \right).
$$

Taking the limit as \( t_1 \to 0 \) and \( t_2 \to 0 \), we get:

$$
\mathbb{E}[Y] = \frac{\partial}{\partial t_2} M_{X, Y}(t_1, t_2) \Big|_{t_1 = 0, t_2 = 0} = 1.
$$

The variance of \( X \) is given by:

$$
\text{Var}(X) = \frac{\partial^2}{\partial t_1^2} M_{X, Y}(t_1, t_2) \Big|_{t_1 = 0, t_2 = 0}.
$$

From the first derivative:

$$
\frac{\partial}{\partial t_1} M_{X, Y}(t_1, t_2) = \frac{-1}{t_1^2} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right) + \frac{1}{t_1} \cdot \frac{1}{(1 - t_1 - t_2)^2}.
$$

The second derivative is:

$$
\frac{\partial^2}{\partial t_1^2} M_{X, Y}(t_1, t_2) = \frac{2}{t_1^3} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right) - \frac{2}{t_1^2} \cdot \frac{1}{(1 - t_1 - t_2)^2} + \frac{2}{t_1} \cdot \frac{1}{(1 - t_1 - t_2)^3}.
$$

Evaluating at \( t_1 = 0 \) and \( t_2 = 0 \), we get:

$$
\text{Var}(X) = 1.
$$

Similarly, the variance of \( Y \) is:

$$
\text{Var}(Y) = \frac{\partial^2}{\partial t_2^2} M_{X, Y}(t_1, t_2) \Big|_{t_1 = 0, t_2 = 0}.
$$

This is:

$$
\frac{\partial^2}{\partial t_2^2} M_{X, Y}(t_1, t_2) = \frac{2}{t_1} \left( \frac{1}{(1 - t_1 - t_2)^3} - \frac{1}{(1 - t_2)^3} \right).
$$

Evaluating at \( t_1 = 0 \) and \( t_2 = 0 \), we get:

$$
\text{Var}(Y) = 1.
$$

The covariance of \( X \) and \( Y \) is given by:

$$
\text{Cov}(X, Y) = \frac{\partial^2}{\partial t_1 \partial t_2} M_{X, Y}(t_1, t_2) \Big|_{t_1 = 0, t_2 = 0}.
$$

From the derivative:

$$
\frac{\partial}{\partial t_1} \frac{\partial}{\partial t_2} M_{X, Y}(t_1, t_2) = \frac{1}{(1 - t_1 - t_2)^2}.
$$

Evaluating at \( t_1 = 0 \) and \( t_2 = 0 \), we get:

$$
\text{Cov}(X, Y) = 1.
$$

Conclusions:
- \( \text{Var}(X) = 1 \),
- \( \text{Var}(Y) = 1 \),
- \( \text{Cov}(X, Y) = 1 \).

## (c)

Based on the joint moment generating function, identify the marginal distribution of X and the marginal distribution of Y. 

To find the marginal distributions of \( X \) and \( Y \) based on the joint moment generating function (MGF), we will extract the MGFs of \( X \) and \( Y \) by setting appropriate parameters in the joint MGF.

The joint moment generating function we found is:

$$
M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right),
$$

valid for \( t_1 + t_2 < 1 \) and \( t_2 < 1 \).

To find the marginal MGF of \( X \), we set \( t_2 = 0 \) in the joint MGF:

$$
M_X(t_1) = M_{X, Y}(t_1, 0) = \frac{1}{t_1} \left( \frac{1}{1 - t_1} - 1 \right).
$$

Simplifying:

$$
M_X(t_1) = \frac{1}{t_1} \left( \frac{1}{1 - t_1} - 1 \right) = \frac{1}{t_1} \left( \frac{1 - (1 - t_1)}{1 - t_1} \right) = \frac{t_1}{t_1(1 - t_1)} = \frac{1}{1 - t_1}.
$$

This is the MGF of an **Exponential(1)** distribution. Therefore, the marginal distribution of \( X \) is:

$$
X \sim \text{Exponential}(1).
$$

To find the marginal MGF of \( Y \), we set \( t_1 = 0 \) in the joint MGF:

$$
M_Y(t_2) = M_{X, Y}(0, t_2) = \frac{1}{0} \left( \frac{1}{1 - t_2} - \frac{1}{1 - t_2} \right),
$$

which simplifies directly to:

$$
M_Y(t_2) = \frac{1}{1 - t_2}.
$$

This is also the MGF of an **Exponential(1)** distribution. Therefore, the marginal distribution of \( Y \) is:

$$
Y \sim \text{Exponential}(1).
$$
- The marginal distribution of \( X \) is **Exponential(1)**.
- The marginal distribution of \( Y \) is **Exponential(1)**.

Both \( X \) and \( Y \) are independently distributed as **Exponential(1)** random variables.

\newpage

# Q7

Beta-Binomial model: Suppose that the conditional distribution X | P = p is Binomial(n, p) and Suppose P has a Beta($\alpha, \beta$) distribution. 

## (a) 

Using the EVVE formula, find Var(X) 

As we know the distribution of X | P = p, we know that its mean and variance are: 

$$
E[X | P = p] = np
$$

$$
Var(X | P = p) = np(1 - p).
$$

Since we also know the distribution of P, we know it has mean and variance:

$$
E[P] = \frac{\alpha}{\alpha + \beta}
$$

$$
Var(P) = \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}
$$


Thus, using EVVE, we know:

$$
Var(X) = E[Var(X | P)] + Var(E[X | P]) = E[np(1 - p)] = n E[p(1 - p)] = n\left( E[p] - E[p^2] \right)
$$

Using the properties of the Beta distribution (distribution of P):

$$
E[p] = \frac{\alpha}{\alpha + \beta}
$$

and

$$
E[p^2] = \frac{\alpha(\alpha + 1)}{(\alpha + \beta)(\alpha + \beta + 1)}.
$$

We we have: 

$$
E[p] - E[p^2] = \frac{\alpha}{\alpha + \beta} - \frac{\alpha(\alpha + 1)}{(\alpha + \beta)(\alpha + \beta + 1)} = \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}.
$$

Combining with our prior formulation of gives us: 

$$
E[Var(X | P)] = n \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}.
$$

$$
Var(E[X | P]) = Var(np) = n^2 Var(P) = n^2 \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}
$$

$$
Var(X) = n \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)} + n^2 \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)} = \frac{n \alpha \beta (n + 1)}{(\alpha + \beta)^2 (\alpha + \beta + 1)}
$$

## (b) 

Suppose that W has a Binomial(n, $\tilde{p}$) distribution having the same mean as X above. For n > 1, show that X has a larger variance than W by a multiplicative factor of: 

$$\frac{\alpha + \beta + n}{\alpha + \beta + 1} > 1$$

From the Beta-Binomial model, we have:

$$
E[X] = n E[P] = n \frac{\alpha}{\alpha + \beta}
$$

If $E[W] = E[X]$, then:

$$
n \tilde{p} = n \frac{\alpha}{\alpha + \beta}.
$$

And it follows that: 

$$
\tilde{p} = \frac{\alpha}{\alpha + \beta}
$$

Using this, we then have: 

$$
Var(W) = n \tilde{p}(1 - \tilde{p}) = n \left(\frac{\alpha}{\alpha + \beta}\right) \left(1 - \frac{\alpha}{\alpha + \beta}\right) = n \frac{\alpha}{\alpha + \beta} \frac{\beta}{\alpha + \beta} = n \frac{\alpha \beta}{(\alpha + \beta)^2}
$$

From part (a), we know Var(X) as: 

$$
Var(X) = \frac{n \alpha \beta (n + 1)}{(\alpha + \beta)^2 (\alpha + \beta + 1)}
$$

Comparing the two variances, we have: 

$$
\frac{Var(X)}{Var(W)} = \frac{\frac{n \alpha \beta (n + 1)}{(\alpha + \beta)^2 (\alpha + \beta + 1)}}{n \frac{\alpha \beta}{(\alpha + \beta)^2}} = \frac{(n + 1)}{\alpha + \beta + 1}
$$

Thus, the multiplicative factor by which \( X \) has a larger variance than \( W \) is:

$$
\frac{\alpha + \beta + n}{\alpha + \beta + 1}.
$$

Since \( n > 1 \), it follows that:

$$
\frac{\alpha + \beta + n}{\alpha + \beta + 1} > 1.
$$

This demonstrates that the variance of \( X \) is indeed larger than the variance of \( W \) by a factor of \( \frac{\alpha + \beta + n}{\alpha + \beta + 1} \).