---
title: "HW6"
output: pdf_document
date: "2024-10-26"
---

# Homework 6

# Q1: 4.17, Casella & Berger

Let X be an exponential(1) random variable, and define Y to be the integer part of X+1, that is: 

$$Y = i + 1 \text{ iff } i \leq X < i+1, i = 0, 1, 2, ...$$

## (a) 

Find the distribution of Y. What well-known distribution does Y have? 

$$P(Y = i + 1) = \int\limits_{i}^{i+1}e^{-x} dx = -e^{-x} \big|_{x = i}^{i+1} = -e^{-(i+1)} + e^{-i} = e^{-i}(1-e^{-1})$$

This is a geometric distribution with $p = 1-e^{-1}$, such that

$Y \sim Geom(1-e^{-1})$

## (b) 

Find the conditional distribution of X - 4 given $Y \geq 5$

As defined, $Y = i + 1$, such that 

$$Y \geq 5 \iff i + 1 \geq 5 \iff X \geq 4$$
Utilizing the distributions as defined and found, we then have

$$
P(X - 4 \leq x \mid Y \geq 5) = P(X - 4 \leq x \mid X \geq 4) = P(X \leq x + 4 \mid X \geq 4)
$$

$$
P(X - 4 \leq x \mid Y \geq 5)  = P(X \leq x + 4 \mid X \geq 4) = 1 - P(X > x + 4 \mid X \geq 4) = 1 - P(X > x) = 1 - e^{-x}
$$

This sure looks like the memoryless property we observed previously! 

$$
P(X - 4 \leq x \mid Y \geq 5) = P(X \leq x) = 1 - e^{-x}
$$

\newpage

# Q2:  4.32(a), Casella & Berger

## (a) 

For a hierarchical model: 

$$Y | \Lambda \sim Poisson(\Lambda) \text{ and } \Lambda \sim Gamma(\alpha, \beta)$$

find the marginal distribution, mean, and variance of Y. Show that the marginal distribution of Y is a negative binomial if $\alpha$ is an integer. 

For y = 0, 1, ..., we may write the conditional distribution of Y = y as: 

$$P(Y = y | \lambda) = \sum\limits_{n=y}^{\infty} P(Y = y | N = n, \lambda) P(N = n | \lambda) = \sum\limits_{n=y}^{\infty} {n \choose y} p^{y} (1-p)^{n-y} \frac{e^{-\lambda} \lambda^n}{n!}$$

$$P(Y = y | \lambda) = \sum\limits_{n=y}^{\infty} \frac{1}{y!(n-y)!} (\frac{p}{1-p})^{y} [(1-p)\lambda]^{n}e^{-\lambda}$$

Define $m = n - y$, such that we may rewrite the above as: 

$$P(Y = y | \lambda) = \sum\limits_{n=y}^{\infty} \frac{e^{-\lambda}}{y!m!} (\frac{p}{1-p})^{y} [(1-p)\lambda)^{m}] = \sum\limits_{n=y}^{\infty} \frac{e^{-\lambda}}{y!} (\frac{p}{1-p})^{y} \frac{[(1-p)\lambda)^{m}]}{m!}$$

After gathering the terms, we see quite a lot of this does not depend on m, such that we may take it out of the summation and write: 

$$P(Y = y | \lambda) = \frac{e^{-\lambda}}{y!} (\frac{p}{1-p})^{y} \sum\limits_{n=y}^{\infty}  \frac{[(1-p)\lambda)^{m}]}{m!}$$

After simplifying, we then take advantage that 

$$\sum\limits_{n=y}^{\infty}  \frac{[(1-p)\lambda)^{m}]}{m!} = e^{(1-p)\lambda}$$

And may write: 

$$P(Y = y | \lambda) = e^{-\lambda}(p\lambda)^{y}e^{(1-p)\lambda}= \frac{(p\lambda)^{y}e^{-p\lambda}}{y!}$$

Note the above is a type of Poisson, specifically: 

$$Y | \Lambda \sim Poisson(p\lambda)$$

From this we may "extract" the pmf of Y (pmf as both the conditional of Y and $\Lambda$ are both Poisson distributed), specifically for $y = 0 , 1, ...$, 

$$f_Y(y) = \frac{1}{\Gamma(\alpha) y! (p\beta)^{\alpha}} \Gamma(y + \alpha) (\frac{p\beta}{1+p\beta})^{y + \alpha}$$

For a positive integer $\alpha$, the above provides a pmf for a negative binomial distribution, specifically: 

$$Y \sim NB(\alpha, \frac{1}{1 + p\beta}$$

\newpage

# Q3

Expectation

## (a) 

Show that any random variable X (with finite mean) has zero covariance with any real constant c, i.e. $Cov(X, c) = 0$

The covariance of X and c may be written: 

$$Cov(X, c) = E[(X - E[X])(c - E[c])] = E[(X - E[X])(c-c)] = E[(X - E[X])0] = E[0] = 0$$

Such that we conclude:

$$Cov(X, c) = 0$$

And we must have the condition that X has a finite mean as $\infty * 0$ is undefined. 

## (b) 

Using the definition of conditional expectation, show that 

$$E[g(X)h(Y) | X = x] = g(x)E[h(Y) | X = x]$$

for an x with pdf $f_X(x) > 0$ (You may also assume (X, Y) are jointly discrete). 

To show that 

$$
E[g(X)h(Y) \mid X = x] = g(x)E[h(Y) \mid X = x]
$$

For jointly discrete random variables X and Y, the conditional expectation of h(Y) | X = x is:

$$
E[h(Y) \mid X = x] = \sum_y h(y) P(Y = y \mid X = x)
$$

Similarly, the conditional expectation of g(X)h(Y) | X = x is:

$$
E[g(X)h(Y) \mid X = x] = \sum_y g(x)h(y) P(Y = y \mid X = x)
$$

We can simplify by recognizing there are terms in the above equations that do not depend on the index of the summation, specifically:

$$
E[g(X)h(Y) \mid X = x] = \sum_y g(x) h(y) P(Y = y \mid X = x) = g(x) \sum_y h(y) P(Y = y \mid X = x)
$$

However, this is a very familiar formula to us! 

$$
E[g(X)h(Y) \mid X = x] = g(x) \sum_y h(y) P(Y = y \mid X = x)
=  g(x) E[h(Y) \mid X = x]
$$

Note: 

The condition $f_X(x) > 0$ is necessary as it ensures that $P(Y = y \mid X = x)$ is defined, because: 

$$
P(Y = y \mid X = x) = \frac{P(X = x, Y = y)}{P(X = x)} \equiv \frac{P(X = x, Y = y)}{f_X(x)}
$$


\newpage

# Q4

Suppose that $X_i$ has mean $\mu_i$ and variance $\sigma_i^2$, for i = 1, 2, and that the covariance of $X_1$ and $X_2$ is $\sigma_{12}$. Compute the covariance between $X_1 - 2X_2 + 8$, and then compute the covariance of $3X_1 + X_2$. 

## (a) 

$X_1 - 2X_2 + 8$

$$
Var(X_1 - 2X_2 + 8) = Cov(X_1 - 2X_2 + 8, X_1 - 2X_2 + 8) = Cov(X_1 - 2X_2, X_1 - 2X_2)
$$

$$
Cov(X_1 - 2X_2 + 8, X_1 - 2X_2 + 8) = Cov(X_1, X_1) - 2Cov(X_1, X_2) - 2Cov(X_2, X_1) + 4Cov(X_2, X_2) = \sigma_1^2 - 4\sigma_{12} + 4\sigma_2^2
$$

$$
Cov(X_1 - 2X_2 + 8, X_1 - 2X_2 + 8) = \sigma_1^2 - 4\sigma_{12} + 4\sigma_2^2.
$$

## (b)  

$3X_1 + X_2$

$$
Cov(3X_1 + X_2, 3X_1 + X_2) = Cov(3X_1, 3X_1) + Cov(3X_1, X_2) + Cov(X_2, 3X_1) + Cov(X_2, X_2)
$$

$$
Cov(3X_1 + X_2, 3X_1 + X_2) = 9\sigma_1^2 + 3\sigma_{12} + 3\sigma_{12} + \sigma_2^2
$$

\newpage

# Q5

The joint distribution of X, Y is given by the joint pdf: 

$$f(x, y) = 3 (x + y) \text{ for } 0<x<1, 0<y<1, 0<x+y<1$$

## (a) 

Find the marginal distribution of $f_X(x)$

$$
f_X(x) = \int_{y \in Y} f(x, y) \, dy = \int_{0}^{1} f(x, y) \, dy = \int_{0}^{1} 3 (x + y) dy
$$

However, the bounds of the integral as given above are not correct, as:

$$
0 < x < 1, \, 0 < y < 1, \, 0 < x + y < 1
$$
So we actually have: 

$$
f_X(x) = \int_{0}^{1-x} 3 (x + y) \, dy = 3 \int_{0}^{1-x} (x + y) \, dy = 3 \left[ \int_{0}^{1-x} x \, dy + \int_{0}^{1-x} y \, dy \right]
$$

For the sake of simplifcation, these are: 
### i. 

$$\int_{0}^{1-x} x \, dy = x(1 - x).$$
### ii. 

$$\int_{0}^{1-x} y \, dy = \frac{(1 - x)^2}{2}.$$

Taken together, we have: 

$$
f_X(x) = 3 \left[ x(1 - x) + \frac{(1 - x)^2}{2} \right] = 3 \left[ (1 - x) \left( x + \frac{1 - x}{2} \right) \right] = 3 (1 - x) \left( \frac{x + 1}{2} \right) = \frac{3}{2} (1 - x)(x + 1)
$$

Thus, the marginal distribution of X is:

$$
f_X(x) = \frac{3}{2} (1 - x)(x + 1), \quad \text{ for } 0 < x < 1.
$$

## (b) 

Find the conditional pdf of Y | X = x, given some 0<x<1. 

Using the definition of the conditional pdf, we have: 

$$
f_{Y | X}(y | x) = \frac{f(x, y)}{f_X(x)} = \frac{3 (x + y)}{\frac{3}{2} (1 - x)(x + 1)}
$$

For 

$$
0 < x < 1, \, 0 < y < 1, \, 0 < x + y < 1
$$

Simplifying gives us:

$$
f_{Y | X}(y | x) = \frac{2 (x + y)}{(1 - x)(x + 1)} \text{ for } 0 < y < 1 - x
$$

## (c) 

Find $E[Y | X = x]$

Using what we derived in part (b), we have: 

$$
E[Y \mid X = x] = \int_{0}^{1-x} y \, f_{Y | X}(y | x) \, dy = \int_{0}^{1-x} y \frac{2 (x + y)}{(1 - x)(x + 1)} = \frac{2}{(1 - x)(x + 1)} \int_{0}^{1-x} y (x + y) \, dy = \frac{2}{(1 - x)(x + 1)} \int_{0}^{1-x} yx + y^2 dy
$$

$$
E[Y \mid X = x] = \frac{2}{(1 - x)(x + 1)} [\left( \int_{0}^{1-x} xy \, dy \right) + \left( \int_{0}^{1-x} y^2 \, dy \right)]
$$

### i. 

$$
\int_{0}^{1-x} xy \, dy = x \int_{0}^{1-x} y \, dy = x \left[ \frac{(1 - x)^2}{2} \right] = \frac{x (1 - x)^2}{2}
$$
### ii. 

$$
\int_{0}^{1-x} y^2 \, dy = \left[ \frac{(1 - x)^3}{3} \right]
$$

Combining the two parts above, we then have: 

$$
E[Y \mid X = x] = \frac{2}{(1 - x)(x + 1)} \left( \frac{x (1 - x)^2}{2} + \frac{(1 - x)^3}{3} \right) = \frac{2 (1 - x)^2}{(1 - x)(x + 1)} \left( \frac{x}{2} + \frac{1 - x}{3} \right)
$$

Simplify, simplify: 

$$
E[Y \mid X = x] = \frac{2 (1 - x)}{x + 1} \left( \frac{3x + 2 - 2x}{6} \right) = \frac{2 (1 - x)}{x + 1} \left( \frac{x + 2}{6} \right) = \frac{(1 - x)(x + 2)}{3 (x + 1)}
$$

## (d)

Given the results in (a), (b), and (c), explain how you know $E[X | Y = y]$ without any further calculation

Given the above results, we can take advantage of symmetry, since the joint pdf of X and Y involves a simple sum of x + y, and the support of each is the same, i.e.  

$$
f(x, y) = 3 (x + y), \quad \text{ for } 0 < x < 1, \, 0 < y < 1, \, 0 < x + y < 1
$$

So we can effective "swap" any "x" in the prior calculations with "y" (and similarly if we felt inclined to derive everything again we could/would swap the "y" in our calculations with "x"). 

Given from (c):

$$
E[Y \mid X = x] = \frac{(1 - x)(x + 2)}{3 (x + 1)}
$$

By symmetry, we know:

$$
E[X \mid Y = y] = \frac{(1 - y)(y + 2)}{3 (y + 1)}
$$

## (e) 

Find $E[E[2XY - Y|X]]$

From the parts above, we know most everything but $E[XY]$. 

$$
E[E[2XY - Y \mid X]] = E[2XY - Y] = 2E[XY] - E[Y]
$$

Taking advantage of the symmetry property used in part (d), we can easily find the marginal pdf of Y. Namely, as: 

$$
f_X(x) = \frac{3}{2} (1 - x)(x + 1), \quad \text{ for } 0 < x < 1
$$

Then: 

$$
f_Y(y) = \frac{3}{2} (1 - y)(y + 1), \quad \text{ for } 0 < y < 1
$$

However, due to symmetry:

$$
E[Y] = E[X]
$$

So if we calculate $E[X]$, we effectively get $E[Y]$. Let's do that! 

$$
E[X] = \int_0^1 x \cdot f_X(x) \, dx = \int_0^1 x \frac{3}{2} (1 - x)(x + 1) \, dx = \int_0^1 \frac{3x}{2} (1 - x^2) = \frac{3}{2} \int_0^1 x (1 - x^2) \, dx = \frac{3}{2} \left( \int_0^1 x \, dx - \int_0^1 x^3 \, dx \right)
$$
### o. 

$$
\int_0^1 x \, dx = \frac{1}{2}
$$ 

$$
\quad \int_0^1 x^3 \, dx = \frac{1}{4}
$$

Taking the above gives us: 

$$
E[X] = \frac{3}{2} \left( \frac{1}{2} - \frac{1}{4} \right) = \frac{3}{2} \cdot \frac{1}{4} = \frac{3}{8}
$$

And $E[Y] = \frac{3}{8}$ too.

Last part now, we need to evaluate $E[XY]$: 

$$
E[XY] = \int_0^1 \int_0^{1-x} xy f(x,y) dy dx = \int_0^1 \int_0^{1-x} xy  3(x + y) \, dy \, dx = 3 \int_0^1 \int_0^{1-x} xy (x + y) =  \, dy \, dx = 3 \int_0^1 \int_0^{1-x} (x^2 y + xy^2) \, dy \, dx
$$

Alright, back to it, separating the integrals: 

### i. 

$$
\int_0^1 \int_0^{1-x} x^2 y \, dy \, dx = \int_0^1 x^2 \left( \frac{(1-x)^2}{2} \right) \, dx = \frac{1}{2} \int_0^1 x^2 (1 - x)^2 \, dx = \frac{1}{2} \left( \int_0^1 x^2 \, dx - 2 \int_0^1 x^3 \, dx + \int_0^1 x^4 \, dx \right)
$$

Where: 

$$
\int_0^1 x^2 \, dx = \frac{1}{3}
$$

$$
\quad \int_0^1 x^3 \, dx = \frac{1}{4}
$$

$$
\quad \int_0^1 x^4 \, dx = \frac{1}{5}
$$

And our total for the "first" term is then

$$
\frac{1}{2} \left( \frac{1}{3} - 2(\frac{1}{4}) + \frac{1}{5} \right) = \frac{1}{2} \left( \frac{1}{3} - \frac{1}{2} + \frac{1}{5} \right) = \frac{1}{2} \left( \frac{10}{30} - \frac{15}{30} + \frac{6}{30} \right) = \frac{1}{2}(\frac{1}{30}) = \frac{1}{60}
$$

### ii. 

$$
\int_0^1 \int_0^{1-x} xy^2 \, dy \, dx = \int_0^1 x \left( \frac{(1-x)^3}{3} \right) \, dx = \frac{1}{3} \int_0^1 x (1 - x)^3 \, dx = \frac{1}{3} \int_0^1 (x - 3x^2 + 3x^3 - x^4) \, dx
$$

$$
\frac{1}{3} \left( \frac{1}{2} - 3 \cdot \frac{1}{3} + 3 \cdot \frac{1}{4} - \frac{1}{5} \right) = \frac{1}{3} \left( \frac{1}{2} - 1 + \frac{3}{4} - \frac{1}{5} \right) = \frac{1}{3} \left( \frac{30}{60} - \frac{60}{60} + \frac{45}{60} - \frac{12}{60} \right) = \frac{1}{3} \cdot \frac{3}{60} = \frac{1}{60}
$$

Crazy, $\frac{1}{60}$ again...symmetry? 

Taking the two parts above, we then have: 

$$
E[XY] = 3 \left( \frac{1}{60} + \frac{1}{60} \right) = \frac{3}{30} = \frac{1}{10}
$$

$$
2E[XY] = 2 (\frac{1}{10}) = \frac{1}{5}
$$
And with:

$$
E[Y] = \frac{3}{8}
$$
We may finally calculate the desired value as: 

$$
E[E[2XY - Y \mid X]] = \frac{1}{5} - \frac{3}{8} = \frac{8}{40} - \frac{15}{40} = -\frac{7}{40} = - \frac{7}{40}
$$

\newpage

# Q6

Suppose that $f(x, y) = e^{-y} \text{ for } 0 < x < y < \infty$

## (a) 

Find the joint moment generating function for (X, Y). 

The joint moment generating function $M_{X, Y}(t_1, t_2)$ may be defined:

$$
M_{X, Y}(t_1, t_2) = E\left[e^{t_1 X + t_2 Y}\right] = \int_0^\infty \int_0^y e^{t_1 x + t_2 y} e^{-y} \, dx \, dy = \int_0^\infty \int_0^y e^{t_1 x} e^{(t_2 - 1) y} \, dx \, dy = \int_0^\infty e^{(t_2 - 1) y}\left( \int_0^y e^{t_1 x} \, dx \, \right) dy =  \int_0^\infty e^{(t_2 - 1) y} (\frac{1}{t_1}(e^{t_1y} - 1)) dy
$$


$$
M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \int_0^\infty e^{(t_2 - 1) y} (e^{t_1y} - 1) dy = \frac{1}{t_1} \int_0^\infty e^{(t_2 + t_1 - 1)y}  - e^{(t_2 - 1)y} dy
$$

$$
M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \int_0^\infty \left( e^{(t_1 + t_2 - 1) y} - e^{(t_2 - 1) y} \right) \, dy = \frac{1}{t_1} \left[ \int_0^\infty \left( e^{(t_1 + t_2 - 1) y} dy \right) - \int_0^\infty \left( e^{(t_2 - 1) y} \right) \right] dy
$$

### i. 

$$
\int_0^\infty e^{(t_1 + t_2 - 1) y} \, dy = \frac{1}{1 - t_1 - t_2} \quad \text{ for } \, t_1 + t_2 < 1
$$

### ii. 

$$
\int_0^\infty e^{(t_2 - 1) y} \, dy = \frac{1}{1 - t_2} \quad \text{ for } \, t_2 < 1
$$

Combining the two parts above gives us the joint mgf: 

$$
M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right)
$$

## (b) 

## Overview

From (c) we know: 

$X \sim \text{Exponential}(1) \rightarrow f_X(x) = e^{-x}$
$Y \sim \text{Gamma}(\alpha = 2, \beta = 1)$

So

$E[X] = 1, Var(X) = 1$

$E[Y] = 2, Var(Y) = 2$

This is here for my sanity because I keep messing up the calculations. 

## Calculations using joint mgf 

Use the joint moment generating function to find the variance of X, the variance of Y, and the covariance of X and Y. 

For a joint MGF of: 

$$
M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right)
$$

## E[X] 

To find the mean of X, we have: 

$$
M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right)
$$ 

$$
\frac{\partial}{\partial t_1} M_{X, Y}(t_1, t_2) = \frac{\partial}{\partial t_1} \left( \frac{1}{t_1} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right) \right) = \frac{-1}{t_1^2} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right) + \frac{1}{t_1}  \frac{1}{(1 - t_1 - t_2)^2}
$$

$$
\lim_{t_1 \to 0, t_2 \to 0} \frac{\partial}{\partial t_1} M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \left( \frac{-1}{(1 - t_1 - t_2)(1 - t_2)} + \frac{1}{(1 - t_1 - t_2)^2} \right) = (\frac{1}{t_1}) \frac{t_1}{(1 - t_1 - t_2)^2 (1 - t_2)} =  \frac{1}{(1 - t_1 - t_2)^2 (1 - t_2)} = \frac{1}{(1 - 0 - 0)^2 (1 - 0)} = \frac{1}{1^2 \cdot 1} = 1
$$

$$
E[X] = \lim_{t_1 \to 0, t_2 \to 0} \frac{\partial}{\partial t_1} M_{X, Y}(t_1, t_2) = 1
$$

## E[Y]

For the mean of Y, we have:

$$
M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right)
$$

$$
\frac{\partial}{\partial t_2} M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \left( \frac{1}{(1 - t_1 - t_2)^2} - \frac{1}{(1 - t_2)^2} \right)
$$

Similar as before, as we cannot directly evaluate $(t_1, t_2) = (0, 0)$, we must then evaluate the limit as $t_1 \rightarrow \infty$, $t_2 \rightarrow \infty$: 

$$
\frac{\partial}{\partial t_2} M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \left( \frac{1}{(1 - t_1 - t_2)^2} - \frac{1}{(1 - t_2)^2} \right) = (\frac{1}{t_1}) \frac{2t_1}{(1 - t_1 - t_2)^2 (1 - t_2)^2} = \frac{2}{(1 - t_1 - t_2)^2 (1 - t_2)^2}
$$

$$
E[Y] = \lim_{t_1 \to 0, t_2 \to 0} \frac{2}{(1 - t_1 - t_2)^2 (1 - t_2)^2} = 2
$$

## Var(X)

We then need to calculate the variance of both X and Y. Starting with X, we have: 

$$
E[X^2] = \frac{\partial^2}{\partial t_1^2} M_{X, Y}(t_1, t_2) = \frac{2}{t_1^3} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right) - \frac{2}{t_1^2}  \frac{1}{(1 - t_1 - t_2)^2} + \frac{2}{t_1}  \frac{1}{(1 - t_1 - t_2)^3}
$$

$$
E[X^2] = \frac{\partial^2}{\partial t_1^2} M_{X, Y}(t_1, t_2) \Big|_{t_1 = 0, t_2 = 0} = \frac{2}{t_1^3} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right) - \frac{2}{t_1^2}  \frac{1}{(1 - t_1 - t_2)^2} + \frac{2}{t_1}  \frac{1}{(1 - t_1 - t_2)^3}
$$

Similar as before, as we cannot directly evaluate $(t_1, t_2) = (0, 0)$, we must then evaluate the limit as $t_1 \rightarrow \infty$, $t_2 \rightarrow \infty$: 

$$
E[X^2] = \frac{\partial^2}{\partial t_1^2} M_{X, Y}(t_1, t_2) = \frac{\partial}{\partial t_1} \left( \frac{1}{(1 - t_1 - t_2)^2 (1 - t_2)} \right) = \frac{2}{(1 - t_1 - t_2)^3 (1 - t_2)}
$$

$$
E[X^2] = \frac{\partial^2}{\partial t_1^2} M_{X, Y}(t_1, t_2) \Big|_{t_1 = 0, t_2 = 0} \frac{2}{(1 - t_1 - t_2)^3 (1 - t_2)} = \frac{2}{(1 - 0 - 0)^3 (1 - 0)} = 2
$$

Then for Var(X): 

$$
\text{Var}(X) = E(X^2) - [E(X)]^2 = 2 - 1 = 1
$$

## Var(Y) 

For the variance of Y we then have: 

$$
M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right)
$$

$$
E[Y^2] = \frac{\partial^2}{\partial t_2^2} M_{X, Y}(t_1, t_2) \Big|_{t_1 = 0, t_2 = 0} \frac{2}{t_1} \left( \frac{1}{(1 - t_1 - t_2)^3} - \frac{1}{(1 - t_2)^3} \right)
$$

Similar as before, as we cannot directly evaluate $(t_1, t_2) = (0, 0)$, we must then evaluate the limit as $t_1 \rightarrow \infty$, $t_2 \rightarrow \infty$: 

$$
E[Y^2] = \frac{\partial^2}{\partial t_2^2} M_{X, Y}(t_1, t_2) \Big|_{t_1 = 0, t_2 = 0} \frac{2}{t_1} \left( \frac{(1 - t_2)^3 - (1 - t_1 - t_2)^3}{(1 - t_1 - t_2)^3 (1 - t_2)^3} \right) = \frac{2}{t_1} \left( \frac{3t_1}{(1 - t_1 - t_2)^3 (1 - t_2)^3}\right) = \frac{6}{(1 - t_1 - t_2)^3 (1 - t_2)^3}
$$

$$
E[Y^2] =\lim_{t_1 \to 0, t_2 \to 0} \frac{6}{(1 - 0 - 0)^3 (1 - 0)^3} = \frac{6}{1} = 6
$$

$$
Var(Y) = E(Y^2) - [E(Y)]^2 = 6 - 2^2 = 2
$$

## Cov(X, Y)

To find Cov(X, Y), we have: 

$$
M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right)
$$

$$
E[XY] = \frac{\partial^2}{\partial t_1 \partial t_2} M_{X, Y}(t_1, t_2) \Big|_{t_1 = 0, t_2 = 0}
$$

$$
\frac{\partial}{\partial t_1} M_{X, Y}(t_1, t_2) = \frac{-1}{t_1^2} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right) + (\frac{1}{t_1}) \frac{1}{(1 - t_1 - t_2)^2} = \frac{1}{(1 - t_1 - t_2)^2 (1 - t_2)}
$$

Uno mas wrt $t_2$: 

$$
\frac{\partial^2}{\partial t_1 \partial t_2} M_{X, Y}(t_1, t_2) = \frac{\partial}{\partial t_2} \left( \frac{1}{(1 - t_1 - t_2)^2 (1 - t_2)} \right) = \frac{2}{(1 - t_1 - t_2)^3 (1 - t_2)} + \frac{1}{(1 - t_1 - t_2)^2 (1 - t_2)^2}
$$

$$
E[XY] = \frac{\partial^2}{\partial t_1 \partial t_2} M_{X, Y}(t_1, t_2) \Big|_{t_1 = 0, t_2 = 0} \frac{2}{(1 - t_1 - t_2)^3 (1 - t_2)} + \frac{1}{(1 - t_1 - t_2)^2 (1 - t_2)^2} = \frac{2}{1} + \frac{1}{1} = 3
$$



$$
Cov(X, Y) = E[XY] - E[X]E[Y] = 3 - 1(2) = 1
$$

So, $Var(X) = 1, Var(Y) = 2, \text{ and } Cov(X, Y) = 1$.

## (c)

Based on the joint moment generating function, identify the marginal distribution of X and the marginal distribution of Y. 

Given the joint moment generating function:

$$
M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right)
$$

For the marginal MGF of X, we set $(t_1, t_2) = (t_1, 0)$:

$$
M_X(t_1) = M_{X, Y}(t_1, 0) = \frac{1}{t_1} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right) = \frac{1}{t_1} \left( \frac{1}{1 - t_1} - 1 \right) = \frac{1}{t_1} \left( \frac{1}{1 - t_1} - 1 \right) = \frac{1}{t_1} \left( \frac{1 - (1 - t_1)}{1 - t_1} \right) = \frac{t_1}{t_1(1 - t_1)} = \frac{1}{1 - t_1}
$$

This is the MGF of an Exponential(1) distribution, such that: 

$$
X \sim \text{Exponential}(1) \rightarrow f_X(x) = e^{-x}
$$

Similarly, for the marginal MGF of Y, we set $(t_1, t_2) = (0, t_2)$:

$$
M_Y(t_2) = M_{X, Y}(0, t_2) = \frac{1}{t_1} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right) = \frac{1}{0} \left( \frac{1}{1 - t_2} - \frac{1}{1 - t_2} \right)
$$

Similar as before, as we cannot directly evaluate $(0, t_2)$, we must then evaluate the limit as $t_1 \rightarrow \infty$: 

$$
M_Y(t_2) = M_{X, Y}(0, t_2) = \frac{1}{t_1} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right)
$$ 

$$
M_{X, Y}(t_1, t_2) =\frac{1}{t_1} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} = \frac{(1 - t_2) - (1 - t_1 - t_2)}{(1 - t_1 - t_2)(1 - t_2)} \right) = \frac{1}{t_1} \left( \frac{1 - t_2 - 1 + t_1 + t_2}{(1 - t_1 - t_2)(1 - t_2)} \right) = \frac{1}{t_1} \frac{t_1}{(1 - t_1 - t_2)(1 - t_2)} = \frac{1}{(1 - t_1 - t_2)(1 - t_2)}
$$

$$
f_Y(y) = \lim_{t_1 \to 0} M_Y(t_2) = \lim_{t_1 \to 0} \frac{1}{(1 - t_1 - t_2)(1 - t_2)} = \frac{1}{(1 - t_2)(1 - t_2)} = \frac{1}{(1 - t_2)^2}
$$

Which is very similar to a Gamma distribution MGF, specifically: 

$$
Y \sim \text{Gamma}(\alpha = 2, \beta = 1)
$$

$X \sim \text{Exponential}(1) \rightarrow f_X(x) = e^{-x}$
$Y \sim \text{Gamma}(\alpha = 2, \beta = 1) \rightarrow f_Y(y) = \frac{1}{\Gamma(2)} y e^{-y} = y e^{-y}$

\newpage

# Q7

Beta-Binomial model: Suppose that the conditional distribution X | P = p is Binomial(n, p) and Suppose P has a Beta($\alpha, \beta$) distribution. 

## (a) 

Using the EVVE formula, find Var(X) 

As we know the distribution of X | P = p, we know that its mean and variance are: 

$$
E[X | P = p] = np
$$

$$
Var(X | P = p) = np(1 - p).
$$

Since we also know the distribution of P, we know it has mean and variance:

$$
E[P] = \frac{\alpha}{\alpha + \beta}
$$

$$
Var(P) = \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}
$$


Thus, using EVVE, we know:

$$
Var(X) = E[Var(X | P)] + Var(E[X | P])
$$

But both of these values will need to be evaluated. To that end: 

$$
E(\text{Var}(X | P)) = E(np(1 - p)) = n E(p(1 - p)) = n \left[ E(p) - E(p^2) \right]
$$
For a Beta distribution, we know: 

$$
E(p) = \frac{\alpha}{\alpha + \beta} \quad \text{and} \quad \text{Var}(p) = \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}
$$

$$
E(p^2) = \frac{\alpha(\alpha + 1)}{(\alpha + \beta)(\alpha + \beta + 1)}
$$



Additionally, as $E(p(1 - p)) = E(p) - E(p^2)$ we have:

$$
E(\text{Var}(X | P)) = n \left( \frac{\alpha}{\alpha + \beta} - \frac{\alpha(\alpha + 1)}{(\alpha + \beta)(\alpha + \beta + 1)} \right) = n \left( \frac{\alpha \beta}{(\alpha + \beta) (\alpha + \beta + 1)} \right) 
$$

$$
\text{Var}(E(X | P)) = \text{Var}(np) = n^2 \text{Var}(p) = n^2 \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}
$$

Combining the two parts, we have: 

$$
\text{Var}(X) = n \left( \frac{\alpha \beta}{(\alpha + \beta) (\alpha + \beta + 1)} \right)  + n^2 \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)} = \frac{n\alpha \beta (\alpha + \beta + n)}{(\alpha + \beta)^2 (\alpha + \beta + 1)}
$$
## (b) 

Suppose that W has a Binomial(n, $\tilde{p}$) distribution having the same mean as X above. For n > 1, show that X has a larger variance than W by a multiplicative factor of: 

$$\frac{\alpha + \beta + n}{\alpha + \beta + 1} > 1$$

From the Beta-Binomial model, we have:

$$
\mathbb{E}(X) = n \cdot E(P) = n \cdot \frac{\alpha}{\alpha + \beta}
$$

If the RV W has the same mean as X, then:

$$
E(W) = n \tilde{p} = n \cdot \frac{\alpha}{\alpha + \beta}
$$

Which means that 

$$
\tilde{p} = \frac{\alpha}{\alpha + \beta}
$$

Furthermore: 

$$
\text{Var}(W) = n \tilde{p} (1 - \tilde{p}) = n \frac{\alpha}{\alpha + \beta}(1 - \frac{\alpha}{\alpha + \beta}) = n (\frac{\alpha}{\alpha + \beta}) (\frac{\beta}{\alpha + \beta}) = \frac{n \alpha \beta}{(\alpha + \beta)^2}
$$

From (a), we have: 

$$
\text{Var}(X) = \frac{n \alpha \beta (\alpha + \beta + n)}{(\alpha + \beta)^2 (\alpha + \beta + 1)}
$$

Looking at the ratio of X/W: 

$$
\frac{\text{Var}(X)}{\text{Var}(W)} = \frac{\frac{n \alpha \beta (\alpha + \beta + n)}{(\alpha + \beta)^2 (\alpha + \beta + 1)}}{\frac{n \alpha \beta}{(\alpha + \beta)^2}} = \frac{\alpha + \beta + n}{\alpha + \beta + 1}
$$

Assuming n > 1 , we have:

$$
( \alpha + \beta + n > \alpha + \beta + 1 )
$$

Such that X has a larger variance than W by a multiplicative factor of: 

$$\frac{\alpha + \beta + n}{\alpha + \beta + 1} > 1$$