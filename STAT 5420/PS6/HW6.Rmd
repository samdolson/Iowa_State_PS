---
title: "HW6"
output: pdf_document
date: "2024-10-26"
---

# Homework 6

# Outline: 
  Q1: DONE 
  Q2: DONE 
  Q3: DONE  
  Q4: DONE 
  Q5: Started 
  Q6: WIP
  Q7: Started

# Q1: 4.17, Casella & Berger

Let X be an exponential(1) random variable, and define Y to be the integer part of X+1, that is: 

$$Y = i + 1 \text{ iff } i \leq X < i+1, i = 0, 1, 2, ...$$

## (a) 

Find the distribution of Y. What well-known distribution does Y have? 

$$P(Y = i + 1) = \int\limits_{i}^{i+1}e^{-x} dx = -e^{-x} \big|_{x = i}^{i+1} = -e^{-(i+1)} + e^{-i} = e^{-i}(1-e^{-1})$$

This is a geometric distribution with $p = 1-e^{-1}$, such that

$Y \sim Geom(1-e^{-1})$

## (b) 

Find the conditional distribution of X - 4 given $Y \geq 5$

As defined, $Y = i + 1$, such that 

$$Y \geq 5 \iff i + 1 \geq 5 \iff X \geq 4$$
Utilizing the distributions as defined and found, we then have

$$
P(X - 4 \leq x \mid Y \geq 5) = P(X - 4 \leq x \mid X \geq 4) = P(X \leq x + 4 \mid X \geq 4)
$$

$$
P(X - 4 \leq x \mid Y \geq 5)  = P(X \leq x + 4 \mid X \geq 4) = 1 - P(X > x + 4 \mid X \geq 4) = 1 - P(X > x) = 1 - e^{-x}
$$

This sure looks like the memoryless property we observed previously! 

$$
P(X - 4 \leq x \mid Y \geq 5) = P(X \leq x) = 1 - e^{-x}
$$

\newpage

# Q2:  4.32(a), Casella & Berger

## (a) 

For a hierarchical model: 

$$Y | \Lambda \sim Poisson(\Lambda) \text{ and } \Lambda \sim Gamma(\alpha, \beta)$$

find the marginal distribution, mean, and variance of Y. Show that the marginal distribution of Y is a negative binomial if $\alpha$ is an integer. 

For y = 0, 1, ..., we may write the conditional distribution of Y = y as: 

$$P(Y = y | \lambda) = \sum\limits_{n=y}^{\infty} P(Y = y | N = n, \lambda) P(N = n | \lambda) = \sum\limits_{n=y}^{\infty} {n \choose y} p^{y} (1-p)^{n-y} \frac{e^{-\lambda} \lambda^n}{n!}$$

$$P(Y = y | \lambda) = \sum\limits_{n=y}^{\infty} \frac{1}{y!(n-y)!} (\frac{p}{1-p})^{y} [(1-p)\lambda]^{n}e^{-\lambda}$$

Define $m = n - y$, such that we may rewrite the above as: 

$$P(Y = y | \lambda) = \sum\limits_{n=y}^{\infty} \frac{e^{-\lambda}}{y!m!} (\frac{p}{1-p})^{y} [(1-p)\lambda)^{m}] = \sum\limits_{n=y}^{\infty} \frac{e^{-\lambda}}{y!} (\frac{p}{1-p})^{y} \frac{[(1-p)\lambda)^{m}]}{m!}$$

After gathering the terms, we see quite a lot of this does not depend on m, such that we may take it out of the summation and write: 

$$P(Y = y | \lambda) = \frac{e^{-\lambda}}{y!} (\frac{p}{1-p})^{y} \sum\limits_{n=y}^{\infty}  \frac{[(1-p)\lambda)^{m}]}{m!}$$

After simplifying, we then take advantage that 

$$\sum\limits_{n=y}^{\infty}  \frac{[(1-p)\lambda)^{m}]}{m!} = e^{(1-p)\lambda}$$

And may write: 

$$P(Y = y | \lambda) = e^{-\lambda}(p\lambda)^{y}e^{(1-p)\lambda}= \frac{(p\lambda)^{y}e^{-p\lambda}}{y!}$$

Note the above is a type of Poisson, specifically: 

$$Y | \Lambda \sim Poisson(p\lambda)$$

From this we may "extract" the pmf of Y (pmf as both the conditional of Y and $\Lambda$ are both Poisson distributed), specifically for $y = 0 , 1, ...$, 

$$f_Y(y) = \frac{1}{\Gamma(\alpha) y! (p\beta)^{\alpha}} \Gamma(y + \alpha) (\frac{p\beta}{1+p\beta})^{y + \alpha}$$

For a positive integer $\alpha$, the above provides a pmf for a negative binomial distribution, specifically: 

$$Y \sim NB(\alpha, \frac{1}{1 + p\beta}$$

\newpage

# Q3

Expectation

## (a) 

Show that any random variable X (with finite mean) has zero covariance with any real constant c, i.e. $Cov(X, c) = 0$

The covariance of X and c may be written: 

$$Cov(X, c) = E[(X - E[X])(c - E[c])] = E[(X - E[X])(c-c)] = E[(X - E[X])0] = E[0] = 0$$

Such that we conclude:

$$Cov(X, c) = 0$$

And we must have the condition that X has a finite mean as $\infty * 0$ is undefined. 

## (b) 

Using the definition of conditional expectation, show that 

$$E[g(X)h(Y) | X = x] = g(x)E[h(Y) | X = x]$$

for an x with pdf $f_X(x) > 0$ (You may also assume (X, Y) are jointly discrete). 

To show that 

$$
E[g(X)h(Y) \mid X = x] = g(x)E[h(Y) \mid X = x]
$$

For jointly discrete random variables X and Y, the conditional expectation of h(Y) | X = x is:

$$
E[h(Y) \mid X = x] = \sum_y h(y) P(Y = y \mid X = x)
$$

Similarly, the conditional expectation of g(X)h(Y) | X = x is:

$$
E[g(X)h(Y) \mid X = x] = \sum_y g(x)h(y) P(Y = y \mid X = x)
$$

We can simplify by recognizing there are terms in the above equations that do not depend on the index of the summation, specifically:

$$
E[g(X)h(Y) \mid X = x] = \sum_y g(x) h(y) P(Y = y \mid X = x) = g(x) \sum_y h(y) P(Y = y \mid X = x)
$$

However, this is a very familiar formula to us! 

$$
E[g(X)h(Y) \mid X = x] = g(x) \sum_y h(y) P(Y = y \mid X = x)
=  g(x) E[h(Y) \mid X = x]
$$

Note: 

The condition $f_X(x) > 0$ is necessary as it ensures that $P(Y = y \mid X = x)$ is defined, because: 

$$
P(Y = y \mid X = x) = \frac{P(X = x, Y = y)}{P(X = x)} \equiv \frac{P(X = x, Y = y)}{f_X(x)}
$$


\newpage

# Q4

Suppose that $X_i$ has mean $\mu_i$ and variance $\sigma_i^2$, for i = 1, 2, and that the covariance of $X_1$ and $X_2$ is $\sigma_{12}$. Compute the covariance between $X_1 - 2X_2 + 8$, and then compute the covariance of $3X_1 + X_2$. 

## (a) 

$X_1 - 2X_2 + 8$

$$
Var(X_1 - 2X_2 + 8) = Cov(X_1 - 2X_2 + 8, X_1 - 2X_2 + 8) = Cov(X_1 - 2X_2, X_1 - 2X_2)
$$

$$
Cov(X_1 - 2X_2 + 8, X_1 - 2X_2 + 8) = Cov(X_1, X_1) - 2Cov(X_1, X_2) - 2Cov(X_2, X_1) + 4Cov(X_2, X_2) = \sigma_1^2 - 4\sigma_{12} + 4\sigma_2^2
$$

$$
Cov(X_1 - 2X_2 + 8, X_1 - 2X_2 + 8) = \sigma_1^2 - 4\sigma_{12} + 4\sigma_2^2.
$$

## (b)  

$3X_1 + X_2$

$$
Cov(3X_1 + X_2, 3X_1 + X_2) = Cov(3X_1, 3X_1) + Cov(3X_1, X_2) + Cov(X_2, 3X_1) + Cov(X_2, X_2)
$$

$$
Cov(3X_1 + X_2, 3X_1 + X_2) = 9\sigma_1^2 + 3\sigma_{12} + 3\sigma_{12} + \sigma_2^2
$$

\newpage

# Q5

The joint distribution of X, Y is given by the joint pdf: 

$$f(x, y) = 3 (x + y) \text{ for } 0<x<1, 0<y<1, 0<x+y<1$$

## (a) 

Find the marginal distribution of $f_X(x)$

To find the marginal distribution \( f_X(x) \), we need to integrate the joint probability density function \( f(x, y) \) with respect to \( y \):

$$
f_X(x) = \int_{0}^{1} f(x, y) \, dy
$$

Given the joint pdf:

$$
f(x, y) = 3 (x + y) \quad \text{for } 0 < x < 1, \, 0 < y < 1, \, 0 < x + y < 1,
$$

the region where the pdf is nonzero is bounded by \( 0 < x < 1 \), \( 0 < y < 1 \), and \( 0 < x + y < 1 \). We need to integrate within these bounds.

For a fixed \( x \), \( y \) must satisfy \( 0 < y < 1 - x \) to ensure \( 0 < x + y < 1 \).

The marginal distribution \( f_X(x) \) is given by:

$$
f_X(x) = \int_{0}^{1-x} 3 (x + y) \, dy.
$$

$$
f_X(x) = 3 \int_{0}^{1-x} (x + y) \, dy = 3 \left[ \int_{0}^{1-x} x \, dy + \int_{0}^{1-x} y \, dy \right].
$$

Evaluating these integrals:

1. $$\int_{0}^{1-x} x \, dy = x(1 - x).$$
2. $$\int_{0}^{1-x} y \, dy = \frac{(1 - x)^2}{2}.$$

So, we have:

$$
f_X(x) = 3 \left[ x(1 - x) + \frac{(1 - x)^2}{2} \right] = 3 \left[ (1 - x) \left( x + \frac{1 - x}{2} \right) \right].
$$

Simplifying further:

$$
f_X(x) = 3 (1 - x) \left( \frac{2x + 1 - x}{2} \right) = 3 (1 - x) \left( \frac{x + 1}{2} \right) = \frac{3}{2} (1 - x)(x + 1).
$$

Thus, the marginal distribution is:

$$
f_X(x) = \frac{3}{2} (1 - x)(x + 1), \quad \text{for } 0 < x < 1.
$$

## (b) 

Find the conditional pdf of Y | X = x, given some 0<x<1. 

To find the conditional probability density function of \( Y \) given \( X = x \), we use the definition:

$$
f_{Y | X}(y | x) = \frac{f(x, y)}{f_X(x)},
$$

where \( f(x, y) \) is the joint pdf and \( f_X(x) \) is the marginal pdf of \( X \).

The joint pdf is:

$$
f(x, y) = 3 (x + y), \quad \text{for } 0 < x < 1, \, 0 < y < 1, \, 0 < x + y < 1.
$$

We found that:

$$
f_X(x) = \frac{3}{2} (1 - x)(x + 1), \quad \text{for } 0 < x < 1.
$$

The conditional pdf is given by:

$$
f_{Y | X}(y | x) = \frac{f(x, y)}{f_X(x)} = \frac{3 (x + y)}{\frac{3}{2} (1 - x)(x + 1)}.
$$

Simplifying the expression:

$$
f_{Y | X}(y | x) = \frac{2 (x + y)}{(1 - x)(x + 1)}.
$$

Given \( 0 < x < 1 \), the support for \( y \) is \( 0 < y < 1 - x \) to satisfy \( 0 < x + y < 1 \).

Thus, the conditional pdf of \( Y \) given \( X = x \) is:

$$
f_{Y | X}(y | x) = \frac{2 (x + y)}{(1 - x)(x + 1)}, \quad \text{for } 0 < y < 1 - x.
$$

## (c) 

Find $E[Y | X = x]$

To find the conditional expectation \( E[Y \mid X = x] \), we use the conditional probability density function \( f_{Y | X}(y | x) \):

$$
E[Y \mid X = x] = \int_{0}^{1-x} y \, f_{Y | X}(y | x) \, dy.
$$

From the previous result, the conditional pdf is:

$$
f_{Y | X}(y | x) = \frac{2 (x + y)}{(1 - x)(x + 1)}, \quad \text{for } 0 < y < 1 - x.
$$

The conditional expectation becomes:

$$
E[Y \mid X = x] = \int_{0}^{1-x} y \left( \frac{2 (x + y)}{(1 - x)(x + 1)} \right) \, dy.
$$

We have:

$$
E[Y \mid X = x] = \frac{2}{(1 - x)(x + 1)} \int_{0}^{1-x} y (x + y) \, dy.
$$

Expanding \( y(x + y) \), we get:

$$
y(x + y) = xy + y^2.
$$

So the integral becomes:

$$
E[Y \mid X = x] = \frac{2}{(1 - x)(x + 1)} \left( \int_{0}^{1-x} xy \, dy + \int_{0}^{1-x} y^2 \, dy \right).
$$

1. Evaluate \( \int_{0}^{1-x} xy \, dy \):

$$
\int_{0}^{1-x} xy \, dy = x \int_{0}^{1-x} y \, dy = x \left[ \frac{(1 - x)^2}{2} \right] = \frac{x (1 - x)^2}{2}.
$$

2. Evaluate \( \int_{0}^{1-x} y^2 \, dy \):

$$
\int_{0}^{1-x} y^2 \, dy = \left[ \frac{(1 - x)^3}{3} \right].
$$

The conditional expectation is:

$$
E[Y \mid X = x] = \frac{2}{(1 - x)(x + 1)} \left( \frac{x (1 - x)^2}{2} + \frac{(1 - x)^3}{3} \right).
$$

Factor out \( (1 - x)^2 \):

$$
E[Y \mid X = x] = \frac{2 (1 - x)^2}{(1 - x)(x + 1)} \left( \frac{x}{2} + \frac{1 - x}{3} \right).
$$

Simplify further:

$$
E[Y \mid X = x] = \frac{2 (1 - x)}{x + 1} \left( \frac{3x + 2 - 2x}{6} \right) = \frac{2 (1 - x)}{x + 1} \left( \frac{x + 2}{6} \right).
$$

Thus, the conditional expectation is:

$$
E[Y \mid X = x] = \frac{(1 - x)(x + 2)}{3 (x + 1)}.
$$

## (d)

Given the results in (a), (b), and (c), explain how you know $E[X | Y = y]$ without any further calculation

We can determine \( E[X \mid Y = y] \) using the symmetry of the joint distribution \( f(x, y) \).

The given joint pdf is:

$$
f(x, y) = 3 (x + y), \quad \text{for } 0 < x < 1, \, 0 < y < 1, \, 0 < x + y < 1.
$$

This joint distribution is symmetric in \( x \) and \( y \), meaning that if we interchange \( x \) and \( y \), the form of the joint pdf remains unchanged. Specifically, since \( f(x, y) \) depends only on the sum \( x + y \), it treats \( x \) and \( y \) symmetrically within the valid region.

Because of this symmetry, the roles of \( X \) and \( Y \) are interchangeable. Thus, the conditional expectation \( E[X \mid Y = y] \) should have the same form as \( E[Y \mid X = x] \), with \( x \) replaced by \( y \).

Given that:

$$
E[Y \mid X = x] = \frac{(1 - x)(x + 2)}{3 (x + 1)},
$$

by symmetry, we can immediately conclude that:

$$
E[X \mid Y = y] = \frac{(1 - y)(y + 2)}{3 (y + 1)}.
$$

This conclusion follows without any further calculation because the joint distribution's symmetry ensures that the conditional expectation expressions for \( X \) and \( Y \) will be identical, with the variables swapped.

## (e) 

Find $E[E[2XY - Y|X]]$

To find \( E[E[2XY - Y \mid X]] \), we use the law of iterated expectations, which states that:

$$
E[E[Z \mid X]] = E[Z],
$$

where \( Z = 2XY - Y \).

According to the law of iterated expectations, we can rewrite \( E[E[2XY - Y \mid X]] \) as:

$$
E[E[2XY - Y \mid X]] = E[2XY - Y].
$$

Using the linearity of expectation, we get:

$$
E[2XY - Y] = 2E[XY] - E[Y].
$$

The marginal pdf of \( Y \), \( f_Y(y) \), is symmetric to the marginal pdf of \( X \), so it can be derived analogously. We previously found that the marginal pdf of \( X \) is:

$$
f_X(x) = \frac{3}{2} (1 - x)(x + 1), \quad \text{for } 0 < x < 1.
$$

Thus, \( E[Y] \) can be obtained by integrating \( y \) with respect to the marginal pdf of \( Y \), but given the symmetry of the joint pdf, we can directly conclude that:

$$
E[Y] = E[X].
$$

Now, we calculate \( E[X] \):

$$
E[X] = \int_0^1 x \cdot f_X(x) \, dx = \int_0^1 x \cdot \frac{3}{2} (1 - x)(x + 1) \, dx.
$$

Expanding \( (1 - x)(x + 1) \), we get:

$$
(1 - x)(x + 1) = 1 + x - x - x^2 = 1 - x^2.
$$

Thus,

$$
E[X] = \frac{3}{2} \int_0^1 x (1 - x^2) \, dx = \frac{3}{2} \left( \int_0^1 x \, dx - \int_0^1 x^3 \, dx \right).
$$

Evaluating these integrals:

$$
\int_0^1 x \, dx = \frac{1}{2}, \quad \int_0^1 x^3 \, dx = \frac{1}{4}.
$$

So,

$$
E[X] = \frac{3}{2} \left( \frac{1}{2} - \frac{1}{4} \right) = \frac{3}{2} \cdot \frac{1}{4} = \frac{3}{8}.
$$

Therefore, \( E[Y] = \frac{3}{8} \).

To find \( E[XY] \), we integrate \( xy \cdot f(x, y) \) over the region \( 0 < x < 1 \), \( 0 < y < 1 \), and \( 0 < x + y < 1 \):

$$
E[XY] = \int_0^1 \int_0^{1-x} xy \cdot 3(x + y) \, dy \, dx.
$$

Expanding this integral will give the value of \( E[XY] \).

After evaluating \( E[XY] \) and \( E[Y] \), we can find \( E[2XY - Y] = 2E[XY] - E[Y] \), which gives us the final result.

Let's proceed to find the final result for \( E[E[2XY - Y \mid X]] = E[2XY - Y] \), which requires calculating \( 2E[XY] - E[Y] \).

In the previous steps, we found:

$$
E[Y] = \frac{3}{8}.
$$

We need to evaluate:

$$
E[XY] = \int_0^1 \int_0^{1-x} xy \cdot 3(x + y) \, dy \, dx.
$$

Expanding this expression:

$$
E[XY] = 3 \int_0^1 \int_0^{1-x} xy (x + y) \, dy \, dx.
$$

Expanding \( xy(x + y) \), we get:

$$
xy(x + y) = x^2 y + xy^2.
$$

Thus, the integral becomes:

$$
E[XY] = 3 \int_0^1 \int_0^{1-x} (x^2 y + xy^2) \, dy \, dx.
$$

We will now evaluate these integrals separately.

1. **Evaluate \( \int_0^1 \int_0^{1-x} x^2 y \, dy \, dx \):**

$$
\int_0^1 \int_0^{1-x} x^2 y \, dy \, dx = \int_0^1 x^2 \left( \frac{(1-x)^2}{2} \right) \, dx = \frac{1}{2} \int_0^1 x^2 (1 - x)^2 \, dx.
$$

Expanding \( (1 - x)^2 = 1 - 2x + x^2 \), we get:

$$
\frac{1}{2} \int_0^1 x^2 (1 - 2x + x^2) \, dx = \frac{1}{2} \left( \int_0^1 x^2 \, dx - 2 \int_0^1 x^3 \, dx + \int_0^1 x^4 \, dx \right).
$$

Evaluating these integrals:

$$
\int_0^1 x^2 \, dx = \frac{1}{3}, \quad \int_0^1 x^3 \, dx = \frac{1}{4}, \quad \int_0^1 x^4 \, dx = \frac{1}{5}.
$$

Thus,

$$
\frac{1}{2} \left( \frac{1}{3} - 2 \cdot \frac{1}{4} + \frac{1}{5} \right) = \frac{1}{2} \left( \frac{1}{3} - \frac{1}{2} + \frac{1}{5} \right) = \frac{1}{2} \left( \frac{10}{30} - \frac{15}{30} + \frac{6}{30} \right) = \frac{1}{2} \cdot \frac{1}{30} = \frac{1}{60}.
$$

2. **Evaluate \( \int_0^1 \int_0^{1-x} xy^2 \, dy \, dx \):**

$$
\int_0^1 \int_0^{1-x} xy^2 \, dy \, dx = \int_0^1 x \left( \frac{(1-x)^3}{3} \right) \, dx = \frac{1}{3} \int_0^1 x (1 - x)^3 \, dx.
$$

Expanding \( (1 - x)^3 = 1 - 3x + 3x^2 - x^3 \), we get:

$$
\frac{1}{3} \int_0^1 (x - 3x^2 + 3x^3 - x^4) \, dx = \frac{1}{3} \left( \frac{1}{2} - 3 \cdot \frac{1}{3} + 3 \cdot \frac{1}{4} - \frac{1}{5} \right).
$$

Evaluating these terms:

$$
\frac{1}{3} \left( \frac{1}{2} - 1 + \frac{3}{4} - \frac{1}{5} \right) = \frac{1}{3} \left( \frac{30}{60} - \frac{60}{60} + \frac{45}{60} - \frac{12}{60} \right) = \frac{1}{3} \cdot \frac{3}{60} = \frac{1}{60}.
$$

Adding the results, we obtain:

$$
E[XY] = 3 \left( \frac{1}{60} + \frac{1}{60} \right) = \frac{3}{30} = \frac{1}{10}.
$$

Now, we have:

$$
2E[XY] = 2 \cdot \frac{1}{10} = \frac{1}{5},
$$

and

$$
E[Y] = \frac{3}{8}.
$$

Thus,

$$
2E[XY] - E[Y] = \frac{1}{5} - \frac{3}{8}.
$$

Converting to a common denominator (40):

$$
2E[XY] - E[Y] = \frac{8}{40} - \frac{15}{40} = -\frac{7}{40}.
$$

Therefore, the final result is:

$$
E[E[2XY - Y \mid X]] = -\frac{7}{40}.
$$

\newpage

# Q6

Suppose that $f(x, y) = e^{-y} \text{ for } 0 < x < y < \infty$

## (a) 

Find the joint moment generating function for (X, Y). 

The joint moment generating function $M_{X, Y}(t_1, t_2)$ may be defined:

$$
M_{X, Y}(t_1, t_2) = E\left[e^{t_1 X + t_2 Y}\right] = \int_0^\infty \int_0^y e^{t_1 x + t_2 y} e^{-y} \, dx \, dy = \int_0^\infty \int_0^y e^{t_1 x} e^{(t_2 - 1) y} \, dx \, dy
$$

First, integrate with respect to \( x \). The inner integral is:

$$
\int_0^y e^{t_1 x} \, dx = \frac{1}{t_1} \left( e^{t_1 y} - 1 \right),
$$

assuming \( t_1 \neq 0 \).

Substitute the result into the outer integral:

$$
M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \int_0^\infty \left( e^{(t_1 + t_2 - 1) y} - e^{(t_2 - 1) y} \right) \, dy.
$$

Now, integrate term by term:

For \( e^{(t_1 + t_2 - 1) y} \):

$$
\int_0^\infty e^{(t_1 + t_2 - 1) y} \, dy = \frac{1}{1 - t_1 - t_2} \quad \text{for} \, t_1 + t_2 < 1.
$$

For \( e^{(t_2 - 1) y} \):

$$
\int_0^\infty e^{(t_2 - 1) y} \, dy = \frac{1}{1 - t_2} \quad \text{for} \, t_2 < 1.
$$

Now, combine the two results:

$$
M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right).
$$

Thus, the joint moment generating function for \( (X, Y) \) is:

$$
M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right),
$$

valid for \( t_1 + t_2 < 1 \) and \( t_2 < 1 \).

## (b) 

Use the joint moment generating function to find the variance of X, the variance of Y, and the covariance of X and Y. 

To find the variances of \( X \), \( Y \), and the covariance between \( X \) and \( Y \) using the joint moment generating function (MGF), we will compute the necessary partial derivatives of the MGF.

The joint MGF we found is:

$$
M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right),
$$

valid for \( t_1 + t_2 < 1 \) and \( t_2 < 1 \).

To find the means of \( X \) and \( Y \), we use the following formulas for the partial derivatives of the MGF:

- \( \mathbb{E}[X] = \frac{\partial}{\partial t_1} M_{X, Y}(t_1, t_2) \Big|_{t_1 = 0, t_2 = 0} \),
- \( \mathbb{E}[Y] = \frac{\partial}{\partial t_2} M_{X, Y}(t_1, t_2) \Big|_{t_1 = 0, t_2 = 0} \).

First, we differentiate \( M_{X, Y}(t_1, t_2) \) with respect to \( t_1 \):

$$
\frac{\partial}{\partial t_1} M_{X, Y}(t_1, t_2) = \frac{-1}{t_1^2} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right) + \frac{1}{t_1} \cdot \frac{1}{(1 - t_1 - t_2)^2}.
$$

Taking the limit as \( t_1 \to 0 \) and \( t_2 \to 0 \), we get:

$$
\mathbb{E}[X] = \frac{\partial}{\partial t_1} M_{X, Y}(t_1, t_2) \Big|_{t_1 = 0, t_2 = 0} = \frac{1}{1^2} = 1.
$$

Now, we differentiate \( M_{X, Y}(t_1, t_2) \) with respect to \( t_2 \):

$$
\frac{\partial}{\partial t_2} M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \left( \frac{1}{(1 - t_1 - t_2)^2} - \frac{1}{(1 - t_2)^2} \right).
$$

Taking the limit as \( t_1 \to 0 \) and \( t_2 \to 0 \), we get:

$$
\mathbb{E}[Y] = \frac{\partial}{\partial t_2} M_{X, Y}(t_1, t_2) \Big|_{t_1 = 0, t_2 = 0} = 1.
$$

The variance of \( X \) is given by:

$$
\text{Var}(X) = \frac{\partial^2}{\partial t_1^2} M_{X, Y}(t_1, t_2) \Big|_{t_1 = 0, t_2 = 0}.
$$

From the first derivative:

$$
\frac{\partial}{\partial t_1} M_{X, Y}(t_1, t_2) = \frac{-1}{t_1^2} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right) + \frac{1}{t_1} \cdot \frac{1}{(1 - t_1 - t_2)^2}.
$$

The second derivative is:

$$
\frac{\partial^2}{\partial t_1^2} M_{X, Y}(t_1, t_2) = \frac{2}{t_1^3} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right) - \frac{2}{t_1^2} \cdot \frac{1}{(1 - t_1 - t_2)^2} + \frac{2}{t_1} \cdot \frac{1}{(1 - t_1 - t_2)^3}.
$$

Evaluating at \( t_1 = 0 \) and \( t_2 = 0 \), we get:

$$
\text{Var}(X) = 1.
$$

Similarly, the variance of \( Y \) is:

$$
\text{Var}(Y) = \frac{\partial^2}{\partial t_2^2} M_{X, Y}(t_1, t_2) \Big|_{t_1 = 0, t_2 = 0}.
$$

This is:

$$
\frac{\partial^2}{\partial t_2^2} M_{X, Y}(t_1, t_2) = \frac{2}{t_1} \left( \frac{1}{(1 - t_1 - t_2)^3} - \frac{1}{(1 - t_2)^3} \right).
$$

Evaluating at \( t_1 = 0 \) and \( t_2 = 0 \), we get:

$$
\text{Var}(Y) = 1.
$$

The covariance of \( X \) and \( Y \) is given by:

$$
\text{Cov}(X, Y) = \frac{\partial^2}{\partial t_1 \partial t_2} M_{X, Y}(t_1, t_2) \Big|_{t_1 = 0, t_2 = 0}.
$$

From the derivative:

$$
\frac{\partial}{\partial t_1} \frac{\partial}{\partial t_2} M_{X, Y}(t_1, t_2) = \frac{1}{(1 - t_1 - t_2)^2}.
$$

Evaluating at \( t_1 = 0 \) and \( t_2 = 0 \), we get:

$$
\text{Cov}(X, Y) = 1.
$$

Conclusions:
- \( \text{Var}(X) = 1 \),
- \( \text{Var}(Y) = 1 \),
- \( \text{Cov}(X, Y) = 1 \).

## (c)

Based on the joint moment generating function, identify the marginal distribution of X and the marginal distribution of Y. 

To find the marginal distributions of \( X \) and \( Y \) based on the joint moment generating function (MGF), we will extract the MGFs of \( X \) and \( Y \) by setting appropriate parameters in the joint MGF.

The joint moment generating function we found is:

$$
M_{X, Y}(t_1, t_2) = \frac{1}{t_1} \left( \frac{1}{1 - t_1 - t_2} - \frac{1}{1 - t_2} \right),
$$

valid for \( t_1 + t_2 < 1 \) and \( t_2 < 1 \).

To find the marginal MGF of \( X \), we set \( t_2 = 0 \) in the joint MGF:

$$
M_X(t_1) = M_{X, Y}(t_1, 0) = \frac{1}{t_1} \left( \frac{1}{1 - t_1} - 1 \right).
$$

Simplifying:

$$
M_X(t_1) = \frac{1}{t_1} \left( \frac{1}{1 - t_1} - 1 \right) = \frac{1}{t_1} \left( \frac{1 - (1 - t_1)}{1 - t_1} \right) = \frac{t_1}{t_1(1 - t_1)} = \frac{1}{1 - t_1}.
$$

This is the MGF of an **Exponential(1)** distribution. Therefore, the marginal distribution of \( X \) is:

$$
X \sim \text{Exponential}(1).
$$

To find the marginal MGF of \( Y \), we set \( t_1 = 0 \) in the joint MGF:

$$
M_Y(t_2) = M_{X, Y}(0, t_2) = \frac{1}{0} \left( \frac{1}{1 - t_2} - \frac{1}{1 - t_2} \right),
$$

which simplifies directly to:

$$
M_Y(t_2) = \frac{1}{1 - t_2}.
$$

This is also the MGF of an **Exponential(1)** distribution. Therefore, the marginal distribution of \( Y \) is:

$$
Y \sim \text{Exponential}(1).
$$
- The marginal distribution of \( X \) is **Exponential(1)**.
- The marginal distribution of \( Y \) is **Exponential(1)**.

Both \( X \) and \( Y \) are independently distributed as **Exponential(1)** random variables.

\newpage

# Q7

Beta-Binomial model: Suppose that the conditional distribution X | P = p is Binomial(n, p) and Suppose P has a Beta($\alpha, \beta$) distribution. 

## (a) 

Using the EVVE formula, find Var(X) 

Given \(X | P = p \sim \text{Binomial}(n, p)\), the conditional distribution of \(X\) given \(P = p\) has mean and variance:

$$
E[X | P = p] = np
$$

$$
Var(X | P = p) = np(1 - p).
$$

The prior distribution for \(P\) is \(P \sim \text{Beta}(\alpha, \beta)\), which has mean and variance:

$$
E[P] = \frac{\alpha}{\alpha + \beta}
$$

$$
Var(P) = \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}.
$$

The EVVE formula states:

$$
Var(X) = E[Var(X | P)] + Var(E[X | P]).
$$

Given \(Var(X | P = p) = np(1 - p)\), the expectation of this variance is:

$$
E[Var(X | P)] = E[np(1 - p)] = n E[p(1 - p)].
$$

$$
E[p(1 - p)] = E[p] - E[p^2].
$$

Using the properties of the Beta distribution:

$$
E[p] = \frac{\alpha}{\alpha + \beta}
$$

and

$$
E[p^2] = \frac{\alpha(\alpha + 1)}{(\alpha + \beta)(\alpha + \beta + 1)}.
$$

Thus,

$$
E[p(1 - p)] = \frac{\alpha}{\alpha + \beta} - \frac{\alpha(\alpha + 1)}{(\alpha + \beta)(\alpha + \beta + 1)} = \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}.
$$

Therefore,

$$
E[Var(X | P)] = n \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}.
$$

Given \(E[X | P = p] = np\), we need to find the variance:

$$
Var(E[X | P]) = Var(np) = n^2 Var(P).
$$

Since \(Var(P) = \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}\), we have:

$$
Var(E[X | P]) = n^2 \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}.
$$

$$
Var(X) = n \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)} + n^2 \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}.
$$

Combining the terms gives:

$$
Var(X) = \frac{n \alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)} (1 + n).
$$

Thus, the variance of \(X\) is:

$$
Var(X) = \frac{n \alpha \beta (n + 1)}{(\alpha + \beta)^2 (\alpha + \beta + 1)}.
$$

## (b) 

Suppose that W has a Binomial(n, $\tilde{p}$) distribution having the same mean as X above. For n > 1, show that X has a larger variance than W by a multiplicative factor of: 

$$\frac{\alpha + \beta + n}{\alpha + \beta + 1} > 1$$

From the Beta-Binomial model, we have:

- \( X | P = p \sim \text{Binomial}(n, p) \), where \( P \sim \text{Beta}(\alpha, \beta) \).
- The mean of \( X \) is:

$$
E[X] = n E[P] = n \frac{\alpha}{\alpha + \beta}.
$$

We want the mean of \( W \), given by \( n \tilde{p} \), to be equal to the mean of \( X \):

$$
n \tilde{p} = n \frac{\alpha}{\alpha + \beta}.
$$

Thus, we set:

$$
\tilde{p} = \frac{\alpha}{\alpha + \beta}.
$$

The variance of a Binomial random variable \( W \) is given by:

$$
Var(W) = n \tilde{p}(1 - \tilde{p}).
$$

Substitute \( \tilde{p} = \frac{\alpha}{\alpha + \beta} \):

$$
Var(W) = n \left(\frac{\alpha}{\alpha + \beta}\right) \left(1 - \frac{\alpha}{\alpha + \beta}\right) = n \frac{\alpha}{\alpha + \beta} \frac{\beta}{\alpha + \beta}.
$$

This simplifies to:

$$
Var(W) = n \frac{\alpha \beta}{(\alpha + \beta)^2}.
$$

The variance of \( X \) in the Beta-Binomial model, as derived earlier, is:

$$
Var(X) = \frac{n \alpha \beta (n + 1)}{(\alpha + \beta)^2 (\alpha + \beta + 1)}.
$$

To show that \( X \) has a larger variance than \( W \), we compare \( Var(X) \) with \( Var(W) \):

$$
\frac{Var(X)}{Var(W)} = \frac{\frac{n \alpha \beta (n + 1)}{(\alpha + \beta)^2 (\alpha + \beta + 1)}}{n \frac{\alpha \beta}{(\alpha + \beta)^2}}.
$$

Simplifying the expression:

$$
\frac{Var(X)}{Var(W)} = \frac{(n + 1)}{\alpha + \beta + 1}.
$$

Thus, the multiplicative factor by which \( X \) has a larger variance than \( W \) is:

$$
\frac{\alpha + \beta + n}{\alpha + \beta + 1}.
$$

Since \( n > 1 \), it follows that:

$$
\frac{\alpha + \beta + n}{\alpha + \beta + 1} > 1.
$$

This demonstrates that the variance of \( X \) is indeed larger than the variance of \( W \) by a factor of \( \frac{\alpha + \beta + n}{\alpha + \beta + 1} \).