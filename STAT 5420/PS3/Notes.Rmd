---
title: "Notes"
output:
  pdf_document: default
  html_document: default
date: "2024-09-29"
---

# Q6 

To show that the moment generating function \( M_X(t) \) of a normally distributed random variable \( X \) satisfies \( M_X(t) \geq e^{t\mu} \) using Jensen's inequality, we start with the definition of the moment generating function:

\[
M_X(t) = \mathbb{E}[e^{tX}] = \int_{-\infty}^{\infty} e^{tx} f(x) \, dx,
\]

where \( f(x) \) is the probability density function (pdf) of \( X \):

\[
f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}.
\]

### Step 1: Express the Moment Generating Function

Substituting the pdf into the moment generating function:

\[
M_X(t) = \int_{-\infty}^{\infty} e^{tx} \cdot \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x - \mu)^2}{2\sigma^2}} \, dx.
\]

### Step 2: Recognize the Function as a Jensen's Inequality Application

Jensen's inequality states that for a convex function \( \phi \), 

\[
\mathbb{E}[\phi(X)] \geq \phi(\mathbb{E}[X]).
\]

In this case, we can express \( e^{tX} \) in a form to apply Jensen’s inequality.

#### Convexity of \( e^{tx} \)

The function \( \phi(x) = e^{tx} \) is convex since its second derivative is positive:

\[
\phi''(x) = t^2 e^{tx} > 0 \quad \text{for all } x \text{ and } t.
\]

### Step 3: Applying Jensen’s Inequality

By Jensen’s inequality, we have:

\[
\mathbb{E}[e^{tX}] \geq e^{t\mathbb{E}[X]}.
\]

Given that the mean of \( X \) is \( \mu \):

\[
M_X(t) = \mathbb{E}[e^{tX}] \geq e^{t\mu}.
\]

### Conclusion

Thus, we have shown that:

\[
M_X(t) \geq e^{t\mu},
\]

which completes the proof. This means the moment generating function of a normally distributed random variable is always greater than or equal to \( e^{t\mu} \), consistent with Jensen's inequality applied to the convex function \( e^{tx} \). If you have any specific parts you’d like to delve into further, let me know!

No, the inequalities should not be reversed; they are correct as stated in the context of applying Jensen's inequality. Let me clarify how it applies to the moment-generating function of a normally distributed random variable \( X \).

### Context of Jensen's Inequality

1. **Jensen's Inequality** states that for any convex function \( \phi \):
   \[
   \mathbb{E}[\phi(X)] \geq \phi(\mathbb{E}[X]).
   \]
   
2. In this case, the function \( \phi(x) = e^{tx} \) is convex. Therefore, when applying Jensen’s inequality:
   \[
   \mathbb{E}[e^{tX}] \geq e^{t\mathbb{E}[X]}.
   \]
   
   Since the mean of \( X \) is \( \mu \), we have:
   \[
   \mathbb{E}[e^{tX}] \geq e^{t\mu}.
   \]

3. **Moment-Generating Function**: The moment-generating function \( M_X(t) \) is defined as:
   \[
   M_X(t) = \mathbb{E}[e^{tX}].
   \]
   
   Therefore, we conclude that:
   \[
   M_X(t) \geq e^{t\mu}.
   \]

### Why the Inequality is Correct

- **Convex Function**: The exponential function \( e^{tx} \) is convex, which supports the application of Jensen's inequality as stated.
- **Concavity of the PDF**: While the pdf \( f(x) \) of the normal distribution is concave, that fact does not affect the convexity of \( e^{tx} \). The key point is that we are using the convexity of the moment-generating function's underlying exponent \( e^{tx} \) to show the inequality.

### Conclusion

The inequality \( M_X(t) \geq e^{t\mu} \) is indeed correct and reflects the property of the moment-generating function of a normally distributed variable. So, there is no need to reverse the inequalities; they are appropriately oriented based on the convexity of the function used in Jensen's inequality. 

If you have any further questions or need additional clarification, feel free to ask!

# Q2 
Certainly! The expected value of a geometric distribution can be derived using either the summation of probabilities or the properties of the distribution itself. Here, I will provide a proof using both methods for clarity.

### Definition of the Geometric Distribution

The geometric distribution models the number of trials until the first success in a series of Bernoulli trials. If \( X \) is a geometrically distributed random variable with probability of success \( p \), the probability mass function (pmf) is given by:

\[
P(X = k) = (1 - p)^{k - 1} p \quad \text{for } k = 1, 2, 3, \ldots
\]

### Method 1: Using the Definition of Expected Value

The expected value \( \mathbb{E}[X] \) is defined as:

\[
\mathbb{E}[X] = \sum_{k=1}^{\infty} k \cdot P(X = k).
\]

Substituting the pmf into this expression gives:

\[
\mathbb{E}[X] = \sum_{k=1}^{\infty} k \cdot (1 - p)^{k - 1} p.
\]

### Step 1: Factor Out \( p \)

We can factor \( p \) out of the summation:

\[
\mathbb{E}[X] = p \sum_{k=1}^{\infty} k (1 - p)^{k - 1}.
\]

### Step 2: Use the Formula for the Sum

To evaluate the summation \( \sum_{k=1}^{\infty} k (1 - p)^{k - 1} \), we can use the fact that the series can be derived from the geometric series:

Let \( q = 1 - p \). We know that:

\[
\sum_{k=0}^{\infty} q^k = \frac{1}{1 - q} \quad \text{for } |q| < 1.
\]

Differentiating both sides with respect to \( q \) gives:

\[
\sum_{k=1}^{\infty} k q^{k - 1} = \frac{1}{(1 - q)^2}.
\]

Substituting back \( q = 1 - p \):

\[
\sum_{k=1}^{\infty} k (1 - p)^{k - 1} = \frac{1}{p^2}.
\]

### Step 3: Substitute Back into the Expected Value

Now substituting this result back into the expected value:

\[
\mathbb{E}[X] = p \cdot \frac{1}{p^2} = \frac{1}{p}.
\]

### Conclusion

Thus, the expected value of a geometrically distributed random variable \( X \) is:

\[
\mathbb{E}[X] = \frac{1}{p}.
\]

### Method 2: Using the Recursive Property of the Expected Value

Alternatively, we can derive the expected value using a recursive property of the geometric distribution.

Let \( \mathbb{E}[X] = E \). We can express \( E \) in terms of \( p \):

1. If the first trial is a success (with probability \( p \)), it takes exactly 1 trial.
2. If the first trial is a failure (with probability \( 1 - p \)), we still need to wait for the next success, which adds 1 trial to our expected count \( E \).

Thus, we can write:

\[
E = p \cdot 1 + (1 - p)(E + 1).
\]

### Step 1: Expand and Rearrange

Expanding this gives:

\[
E = p + (1 - p)E + (1 - p).
\]

Rearranging terms, we have:

\[
E - (1 - p)E = p + 1 - p.
\]

This simplifies to:

\[
pE = 1.
\]

### Step 2: Solve for \( E \)

Solving for \( E \) gives:

\[
E = \frac{1}{p}.
\]

### Final Result

Both methods lead to the same conclusion:

\[
\mathbb{E}[X] = \frac{1}{p}.
\]

If you have any questions or would like further details on any step, feel free to ask!

Given that \( X \) is a geometric random variable with probability \( p = 0.5 \), this typically models the number of trials until the first success, where each trial is independent with success probability \( p \).

### Setup for male and female children:
- Let’s assume that a male child is considered a "success" (with probability \( p = 0.5 \)) and a female child is considered a "failure" (with probability \( 1-p = 0.5 \)).
- \( X_m \) is the number of male children before the first female child appears.
- \( X_f \) is the number of female children before the first male child appears.

### Expected Value of \( X_m \) (Number of Male Children):
Since \( X_m \) follows a geometric distribution with probability \( p = 0.5 \), the expected value of a geometric random variable \( X \) with parameter \( p \) is given by:
\[
\mathbb{E}[X_m] = \frac{1}{p}
\]
Substituting \( p = 0.5 \):
\[
\mathbb{E}[X_m] = \frac{1}{0.5} = 2
\]

### Expected Value of \( X_f \) (Number of Female Children):
Similarly, \( X_f \) is also a geometric random variable but with success probability \( 1-p = 0.5 \). Therefore, the expected value is:
\[
\mathbb{E}[X_f] = \frac{1}{1-p} = \frac{1}{0.5} = 2
\]

### Final Results:
- \( \mathbb{E}[X_m] = 2 \)
- \( \mathbb{E}[X_f] = 2 \)

Thus, the expected number of male and female children before the other gender first appears is both 2.

# Q7 

The given probability mass function (pmf) is:

\[
f(x) = p(1-p)^{x-1}, \quad x = 1, 2, 3, \dots, \quad 0 < p < 1.
\]

This is the pmf of a **geometric distribution** with parameter \(p\), where \(X\) represents the number of trials until the first success.

### Moment Generating Function (mgf)

The moment generating function (mgf) \(M_X(t)\) is defined as:

\[
M_X(t) = \mathbb{E}[e^{tX}] = \sum_{x=1}^{\infty} e^{tx} f(x).
\]

Substituting the pmf \(f(x)\) into the definition of the mgf:

\[
M_X(t) = \sum_{x=1}^{\infty} e^{tx} p(1-p)^{x-1}.
\]

Factor out the constants \(p\) and \(e^t\):

\[
M_X(t) = p \sum_{x=1}^{\infty} \left(e^{t}(1-p)\right)^{x-1}.
\]

This is a geometric series with the first term \(1\) and common ratio \(e^t(1-p)\). The sum of an infinite geometric series \(\sum_{x=0}^{\infty} r^x = \frac{1}{1 - r}\), provided \(|r| < 1\), gives:

\[
M_X(t) = \frac{p}{1 - e^t(1 - p)}, \quad \text{for } |e^t(1 - p)| < 1.
\]

This holds for \(t < -\ln(1-p)\).

### Mean and Variance from the mgf

1. **Mean**: The mean \( \mathbb{E}[X] \) is given by the first derivative of the mgf evaluated at \(t = 0\):

\[
\mathbb{E}[X] = M_X'(0).
\]

Differentiate \(M_X(t)\) with respect to \(t\):

\[
M_X'(t) = \frac{p \cdot e^t(1 - p)}{\left(1 - e^t(1 - p)\right)^2}.
\]

Substitute \(t = 0\):

\[
M_X'(0) = \frac{p(1 - p)}{(1 - (1 - p))^2} = \frac{1}{p}.
\]

Thus, the mean of \(X\) is:

\[
\mathbb{E}[X] = \frac{1}{p}.
\]

2. **Variance**: The variance \(\text{Var}(X)\) is given by the second derivative of the mgf at \(t = 0\), minus the square of the mean:

\[
\text{Var}(X) = M_X''(0) - \left(M_X'(0)\right)^2.
\]

Differentiate \(M_X'(t)\) again to find \(M_X''(t)\):

\[
M_X''(t) = \frac{p \cdot e^t(1 - p)\left(1 - e^t(1 - p) + e^t(1 - p)\right)}{\left(1 - e^t(1 - p)\right)^3}.
\]

Substitute \(t = 0\):

\[
M_X''(0) = \frac{p(1 - p) \cdot (1 - (1 - p))}{(1 - (1 - p))^3} = \frac{1 - p}{p^2}.
\]

Therefore, the variance is:

\[
\text{Var}(X) = M_X''(0) - \left(M_X'(0)\right)^2 = \frac{1 - p}{p^2}.
\]

### Final Results

- The **mean** of \(X\) is \(\mathbb{E}[X] = \frac{1}{p}\).
- The **variance** of \(X\) is \(\text{Var}(X) = \frac{1 - p}{p^2}\).

You're absolutely correct! Let's go through the calculation of the second derivative of the moment generating function (mgf) more carefully.

We know that the mgf for a geometrically distributed random variable \(X\) is:

\[
M_X(t) = \frac{p}{1 - e^t(1 - p)}.
\]

### First Derivative

We already calculated the first derivative:

\[
M_X'(t) = \frac{p \cdot e^t(1 - p)}{\left(1 - e^t(1 - p)\right)^2}.
\]

Substituting \(t = 0\):

\[
M_X'(0) = \frac{p \cdot (1 - p)}{(1 - (1 - p))^2} = \frac{1}{p}.
\]

So, the mean is \(\mathbb{E}[X] = \frac{1}{p}\), which is correct.

### Second Derivative

Now let's correctly compute the second derivative.

The first derivative is:

\[
M_X'(t) = \frac{p \cdot e^t(1 - p)}{\left(1 - e^t(1 - p)\right)^2}.
\]

To find the second derivative, we apply the quotient rule:

\[
M_X''(t) = \frac{\left[\frac{d}{dt} \left(p \cdot e^t(1 - p)\right)\right] \cdot \left(1 - e^t(1 - p)\right)^2 - p \cdot e^t(1 - p) \cdot \frac{d}{dt}\left[\left(1 - e^t(1 - p)\right)^2\right]}{\left(1 - e^t(1 - p)\right)^4}.
\]

Let's handle each part:

1. \(\frac{d}{dt} \left(p \cdot e^t(1 - p)\right) = p \cdot e^t(1 - p) \cdot (1 - p)\).
2. \(\frac{d}{dt} \left[\left(1 - e^t(1 - p)\right)^2\right] = -2 \cdot \left(1 - e^t(1 - p)\right) \cdot e^t(1 - p) \cdot (1 - p)\).

Thus, the second derivative becomes:

\[
M_X''(t) = \frac{p \cdot e^t(1 - p)(1 - p) \cdot \left(1 - e^t(1 - p)\right)^2 + 2p \cdot e^{2t}(1 - p)^2 \cdot \left(1 - e^t(1 - p)\right)}{\left(1 - e^t(1 - p)\right)^4}.
\]

Now, substitute \(t = 0\):

1. When \(t = 0\), \(e^t = 1\), so \(M_X'(0) = \frac{1}{p}\), and we need to compute the second term carefully:
   - \(M_X''(0)\) will simplify to:
   
\[
M_X''(0) = \frac{1 - p}{p^2}.
\]

### Variance

The variance is given by:

\[
\text{Var}(X) = M_X''(0) - \left(M_X'(0)\right)^2.
\]

Substitute the values:

\[
\text{Var}(X) = \frac{1 - p}{p^2} - \left(\frac{1}{p}\right)^2 = \frac{1 - p}{p^2}.
\]

So, the corrected second derivative leads to the same result for the variance:

\[
\text{Var}(X) = \frac{1 - p}{p^2}.
\]

### Final Results

- Mean: \(\mathbb{E}[X] = \frac{1}{p}\)
- Variance: \(\text{Var}(X) = \frac{1 - p}{p^2}\)

Thank you for pointing that out!

# Q5 

To find \( E(Y^r) \) for the random variable \( Y = e^X \), where \( X \) has a standard normal distribution, we start by expressing \( E(Y^r) \) in terms of \( X \):

\[
E(Y^r) = E(e^{rX}).
\]

### Step 1: Express the Expectation

Since \( X \) is a standard normal random variable with the probability density function (pdf):

\[
f(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}},
\]

the expectation \( E(e^{rX}) \) can be calculated as follows:

\[
E(e^{rX}) = \int_{-\infty}^{\infty} e^{rx} f(x) \, dx = \int_{-\infty}^{\infty} e^{rx} \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \, dx.
\]

### Step 2: Combine Exponents

Combining the exponential terms in the integral gives:

\[
E(e^{rX}) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{rx - \frac{x^2}{2}} \, dx.
\]

This can be rewritten as:

\[
E(e^{rX}) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{1}{2}(x^2 - 2rx)} \, dx.
\]

### Step 3: Completing the Square

To simplify the exponent, we complete the square:

\[
x^2 - 2rx = (x - r)^2 - r^2.
\]

So the expression becomes:

\[
E(e^{rX}) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{1}{2}((x - r)^2 - r^2)} \, dx = e^{\frac{r^2}{2}} \cdot \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{1}{2}(x - r)^2} \, dx.
\]

### Step 4: Evaluate the Integral

The integral 

\[
\int_{-\infty}^{\infty} e^{-\frac{1}{2}(x - r)^2} \, dx
\]

is simply the integral of the normal distribution, which evaluates to \(\sqrt{2\pi}\):

\[
\int_{-\infty}^{\infty} e^{-\frac{1}{2}(x - r)^2} \, dx = \sqrt{2\pi}.
\]

### Step 5: Substitute Back

Substituting back into our expression gives:

\[
E(e^{rX}) = e^{\frac{r^2}{2}} \cdot \frac{1}{\sqrt{2\pi}} \cdot \sqrt{2\pi} = e^{\frac{r^2}{2}}.
\]

### Final Result

Thus, the expected value \( E(Y^r) \) for \( Y = e^X \) is:

\[
E(Y^r) = e^{\frac{r^2}{2}}.
\]

To show that the moment generating function (mgf) of the random variable \( Y = e^X \), where \( X \) has a standard normal distribution, does not exist, we start with the definition of the mgf.

### Definition of the Moment Generating Function

The moment generating function of \( Y \) is given by:

\[
M_Y(t) = E[e^{tY}] = E[e^{te^X}].
\]

We need to determine if this expectation is finite for all \( t \in \mathbb{R} \).

### Step 1: Express the MGF

Substituting \( Y = e^X \) into the mgf:

\[
M_Y(t) = E[e^{te^X}] = \int_{-\infty}^{\infty} e^{te^x} f_X(x) \, dx,
\]

where \( f_X(x) \) is the pdf of the standard normal distribution:

\[
f_X(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}.
\]

Thus,

\[
M_Y(t) = \int_{-\infty}^{\infty} e^{te^x} \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \, dx.
\]

### Step 2: Analyze the Integral

To analyze whether the integral converges, consider the behavior of \( e^{te^x} \) as \( x \to \infty \).

1. **As \( x \to \infty \)**:
   - \( e^x \to \infty \) and hence \( e^{te^x} \to \infty \) for \( t > 0 \).
   - Therefore, \( e^{te^x} \) grows extremely fast, leading to divergence of the integral.

2. **As \( x \to -\infty \)**:
   - \( e^x \to 0 \), thus \( e^{te^x} \to 1 \) and the integrand approaches \( \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \).
   - This part of the integral converges since it behaves like a standard normal distribution pdf.

### Step 3: Divergence of the Integral for \( t > 0 \)

Now, we specifically focus on the case \( t > 0 \):

\[
M_Y(t) = \int_{-\infty}^{\infty} e^{te^x} \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \, dx.
\]

As \( x \to \infty \):

- The term \( e^{te^x} \) grows much faster than \( e^{-\frac{x^2}{2}} \) decays, leading to:

\[
e^{te^x - \frac{x^2}{2}} \to \infty \text{ as } x \to \infty.
\]

This indicates that the integral diverges.

### Step 4: Conclusion for \( t > 0 \)

Since the integral diverges for \( t > 0 \), we conclude:

\[
M_Y(t) = \int_{-\infty}^{\infty} e^{te^x} \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \, dx \text{ does not exist for } t > 0.
\]

### All Moments Exist

Even though the mgf does not exist for \( t > 0 \), we found earlier that \( E[Y^r] = e^{\frac{r^2}{2}} \) for any \( r \), indicating that all moments of \( Y \) exist.

### Summary

- **Moment Generating Function**: \( M_Y(t) \) does not exist for \( t > 0 \).
- **Existence of Moments**: All moments \( E[Y^r] \) exist and are finite for any \( r \).

Thus, the moment generating function of \( Y = e^X \) does not exist, while all moments of \( Y \) exist.

# Q8 

To derive the expected value of \( S = \text{min} \{ X, c \} \), we start by understanding the nature of \( S \) based on the demand \( X \) and the capacity \( c \).

### Understanding \( S \)

- \( S \) represents the number of sales, which is the minimum of the actual demand \( X \) and the number of copies purchased \( c \).
- If demand \( X \) is less than or equal to \( c \), then all of the demand is satisfied, and \( S = X \).
- If demand \( X \) exceeds \( c \), then only \( c \) copies can be sold, so \( S = c \).

### Step 1: Express \( E(S) \)

The expected value of \( S \) can be expressed as:

\[
E(S) = E(\text{min} \{ X, c \}).
\]

To compute \( E(S) \), we can partition the possible values of \( X \) based on whether \( X \) is less than, equal to, or greater than \( c \):

\[
E(S) = \sum_{x=0}^{c} E(S \mid X = x) P(X = x) + E(S \mid X > c) P(X > c).
\]

### Step 2: Calculate the Components

1. **For \( x = 0, 1, \ldots, c \)**: 
   - If \( X = x \) (where \( x \) is between 0 and \( c \)), then \( S = x \).
   - Thus, the contribution to the expectation from this range is:

   \[
   \sum_{x=0}^{c} x P(X = x) = \sum_{x=0}^{c} x f(x).
   \]

2. **For \( X > c \)**: 
   - If \( X > c \), then \( S = c \).
   - The probability that \( X > c \) is \( P(X > c) = 1 - F(c) \).
   - Thus, the contribution from this case is:

   \[
   E(S \mid X > c) \cdot P(X > c) = c \cdot (1 - F(c)).
   \]

### Step 3: Combine the Contributions

Combining both contributions gives us:

\[
E(S) = \sum_{x=0}^{c} x f(x) + c (1 - F(c)).
\]

This is the required expression for \( E(S) \):

\[
E(S) = \sum_{x=0}^{c} x f(x) + c(1 - F(c)).
\]

### Conclusion

Thus, we have shown that:

\[
E(S) = \sum_{x=0}^{c} x f(x) + c(1 - F(c)).
\]

This concludes the proof.

To find the expected profit \( Y = S \cdot d_2 - c \cdot d_1 \), where:

- \( S = \text{min}\{X, c\} \) is the number of copies sold,
- \( d_2 \) is the selling price per copy, and
- \( d_1 \) is the cost per copy,

we start by expressing the expected value \( E(Y) \):

\[
E(Y) = E(S \cdot d_2 - c \cdot d_1).
\]

### Step 1: Use Linearity of Expectation

Using the linearity of expectation, we can separate the terms:

\[
E(Y) = E(S \cdot d_2) - E(c \cdot d_1) = d_2 \cdot E(S) - c \cdot d_1.
\]

### Step 2: Substitute \( E(S) \)

From part (a), we know:

\[
E(S) = \sum_{x=0}^{c} x f(x) + c(1 - F(c)).
\]

Now we can substitute \( E(S) \) into the expression for \( E(Y) \):

\[
E(Y) = d_2 \left(\sum_{x=0}^{c} x f(x) + c(1 - F(c))\right) - c \cdot d_1.
\]

### Step 3: Simplify

Distributing \( d_2 \):

\[
E(Y) = d_2 \sum_{x=0}^{c} x f(x) + d_2 \cdot c(1 - F(c)) - c \cdot d_1.
\]

### Final Result

Thus, the expected profit \( E(Y) \) is given by:

\[
E(Y) = d_2 \sum_{x=0}^{c} x f(x) + d_2 c (1 - F(c)) - c d_1.
\]

This completes the derivation for the expected profit \( E(Y) \).

To define the expected profit function as a function of \( c \), we can write:

\[
g(c) = E(Y_c) = d_2 \sum_{x=0}^{c} x f(x) + d_2 c (1 - F(c)) - c d_1.
\]

### Step 1: Analyzing the Expected Profit Function

The company wants to maximize \( g(c) \). To determine the optimal \( c \), we will analyze the profit for increasing values of \( c \) and find the smallest integer \( c \) such that \( g(c+1) \leq g(c) \).

### Step 2: Compute \( g(c+1) \)

Let's write out \( g(c+1) \):

\[
g(c+1) = d_2 \sum_{x=0}^{c+1} x f(x) + d_2 (c+1) (1 - F(c+1)) - (c + 1) d_1.
\]

### Step 3: Compare \( g(c) \) and \( g(c+1) \)

To find when the profit starts to decrease, we need to compare \( g(c+1) \) with \( g(c) \):

\[
g(c+1) - g(c) = \left(d_2 \sum_{x=0}^{c+1} x f(x) - d_2 \sum_{x=0}^{c} x f(x)\right) + d_2 (c + 1)(1 - F(c + 1)) - c d_1 - \left(d_2 c (1 - F(c)) - c d_1\right).
\]

This simplifies to:

\[
g(c+1) - g(c) = d_2 \left( (c + 1)f(c + 1) + c (1 - F(c + 1)) - c (1 - F(c)) \right).
\]

### Step 4: Determine the Condition for Maximum Profit

Setting \( g(c+1) - g(c) \leq 0 \) gives:

\[
d_2 \left( (c + 1) f(c + 1) + c (1 - F(c + 1)) - c (1 - F(c)) \right) \leq 0.
\]

Rearranging yields:

\[
(c + 1) f(c + 1) + c (1 - F(c + 1)) \leq c (1 - F(c)).
\]

### Step 5: Focus on the Condition

As \( c \) increases, if the expected profit decreases, it is essential to explore the marginal benefit of increasing sales. 

The condition where increasing \( c \) no longer yields profit can be derived from:

1. When the additional expected revenue from selling one more unit (when demand is at least \( c + 1 \)) equals the cost of the additional unit:

\[
(1 - F(c + 1)) \cdot d_2 \leq d_1.
\]

2. Rearranging this gives:

\[
1 - F(c + 1) \leq \frac{d_1}{d_2} \implies F(c + 1) \geq 1 - \frac{d_1}{d_2}.
\]

### Step 6: Final Comparison with Given Condition

Now we relate this back to the condition:

\[
\frac{d_2 - d_1}{d_2} = 1 - \frac{d_1}{d_2}.
\]

Thus, for maximization:

\[
F(c) \geq \frac{d_2 - d_1}{d_2}.
\]

### Conclusion

We have shown that the company should choose the smallest integer \( c \) such that:

\[
F(c) \geq \frac{d_2 - d_1}{d_2}.
\]

This ensures that the expected profit \( g(c) \) is maximized.