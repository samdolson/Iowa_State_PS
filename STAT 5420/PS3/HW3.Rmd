---
title: "HW3"
output: pdf_document
date: "2024-09-28"
---
# HW 3
[Name: Sam Olson]{.smallcaps} \
[Collaborators: **Tne Hatman**]{.smallcaps} \

# Overview 
  - 1: DONE
  - 2: DONE, but (c) may need a touch-up
  - 3: Part (c) left TO-DO
  - 4: DONE
  - 5: 
  - 6: DONE
  - 7: DONE, but can tidy up a bit
  - 8: 

> 1. 2.23(b) 

## Question 1
Let X have the pdf

$$f(x) = \frac{1}{2}(1+x)$$, $-1<x<1$

Define the random variable Y by $Y = X^2$

(b): Find $E(Y)$ and Var(Y). 

## Answer 1  

(b): 

(From prior HW) Note from the results of theorem 2.1.8: 

$$f_Y(y) =
\begin{cases}
  \sum \limits_{i=1}^{k} f_X(g^{-1}_i(y))|\frac{d}{dy}g_{i}^{-1}(y)| & y \in \mathbb{Y} \\
  0 & otherwise \\ 
\end{cases}$$

Note: For $X \in [-1, 1]$, we have $Y = X^2 \in [0, 1]$

Over the following partitions, we have monotonicity, 

$A_1 = (-1, 0) \rightarrow X = -\sqrt{y}$, as $g_1(x) = x^2$, and 
 
$A_2 = (0, 1) \rightarrow X = \sqrt{y}$, as $g_2(x) = x^2$

Taking the absolute value of the derivatives, $|\frac{d}{dy}g_{i}^{-1}(y)|$, we have: 

$|\frac{d}{dy}g_{1}^{-1}(y)| = |\frac{d}{dy}g_{2}^{-1}(y)| = \frac{1}{2}y^{-1/2}$ 

Thus we have 

$f_Y(y) = \frac{1}{2}y^{-1/2} \frac{1}{2}[(1 + \sqrt{y}) + (1 - \sqrt{y})] = \frac{1}{4}y^{-1/2} [2] = \frac{1}{2} y^{-1/2}$

Such that we have the pdf of Y as:

$f_Y(y) = \frac{1}{2} y^{-1/2}$, $0 < y < 1$

Using this for our Expected value calculation we have: 

$E(Y) = \int\limits_{y \in \mathbb{Y}} yf(y)dy = \int\limits_{y=0}^{1} y(\frac{1}{2\sqrt y})dy$

$$E(Y) = \int\limits_{y=0}^{1} \sqrt y (\frac{1}{2})dy = \frac{1}{2} \frac{2}{3} y^{3/2} \big|_{y=0}^{y=1} = \frac{1}{2} \frac{2}{3} (1) - 0 = \frac{1}{3}$$

To calculate Var(Y), let us consider $E(Y^2)$, 

$E(Y^2) = \int\limits_{y \in \mathbb{Y}} y^2f(y)dy = \int\limits_{y=0}^{1} y^2(\frac{1}{2\sqrt y})dy$

$$E(Y^2) = \int\limits_{y=0}^{1} y^{3/2} \frac{1}{2} dy = \frac{2}{5} (\frac{1}{2}) y^{5/2} \big|_{y=1}^{y=1} = \frac{2}{5} (\frac{1}{2}) (1) - 0 = \frac{2}{10} = \frac{1}{5}$$
Taking Var(Y) = $E(Y^2) - (E(Y))^2$, then, 

$Var(Y) = \frac{1}{5} - (\frac{1}{3})^2 = \frac{1}{5} - \frac{1}{9} = \frac{9}{45} - \frac{5}{45} = \frac{4}{45}$

\newpage

> 2. 

## Question 2
A family continues to have children until they have one female child. Suppose, for each birth, a single child is born and the child is equally likely to be male or female. The gender outcomes are independent across births. 

(a): Let X be a random variable representing the number of children born to this family. Find the distribution of X. 

(b): Find the expected value $E(X)$

(c): Let $X_m$ denote the number of male children in this family and let $X_f$ denote the number of female children. Find the expected value of $X_m$ and the expected value of $X_f$

## Answer 2

(a): We can frame X as the number of children until the family has their first (one) female child. So we can think of X as a Geometric distribution with probability $p=0.5$ since it is equally likely that they have a male/female for each birth. 

Notation-wise we write this as: 

$X \sim \text{Geometric}(p = 0.5)$

(b): 

Knowing the distribution of X, we know its pmf (discrete!) is given by: 

For X number of children, $k=1, 2, ...$, we have: 

$f_X(x) = P(X = x) = p(1-p)^{x-1}$

$$E(X) = \sum\limits_{x=1}^{\infty} x P(X = x) = \sum\limits_{k=x}^{\infty} x (p(1-p)^{x-1}) = p \sum\limits_{x=1}^{\infty} x ((1-p)^{k-1})$$

Note, for the infinite geometric series we have, for $|r| < 1$, k some positive integer, the following holds: 
$$\sum\limits_{k = 1}^{\infty} r^{k} = \frac{1}{1-r}$$

Note: Let $q = 1-p$ for simplicity. 
As $0 < p < 1 \rightarrow 0 < 1-p < 1 \rightarrow 0 < q < 1$.
For our purposes, we have $|q| < 1$, such that the above relation holds for an infinite geometric series: 

$$\sum\limits_{x = 1}^{\infty} q^{x} = \frac{1}{1-q}$$

Note then: 

$$\frac{d}{dq} (\sum\limits_{x = 1}^{\infty} q^{x}) = \sum\limits_{x = 1}^{\infty} (\frac{d}{dq} q^{x}) = \sum\limits_{x = 1}^{\infty} x q^{x-1}$$
Additionally, 

$$\frac{d}{dq} (\frac{1}{1-q}) = \frac{d}{dq}[(1-q)^{-1}] = \frac{1}{(1-q)^2} = (1-q)^{-2}$$

Thus we have: 
$$E(X) = p (1-q)^{-2} = p ( 1 - (1 - p) ) ^ {-2} = p ( p)^{-2} = p^{-1} = \frac{1}{p}$$

For $p=0.5$, we have: 

$$E(X) = \frac{1}{p} = \frac{1}{0.5} = 2$$

Or, on average they would have two children before they have their first female. 

(c): 

Note: $X_f$ and $X_m$ are subsets of the random variable $X$. 

Both $X_f$ and $X_m$ are Geometrically distributed random variables with parameter p, where $p = 0.5$. Given the derivation in part (b), we have: 

$E(X_f) = \frac{1}{p} = \frac{1}{0.5} = 2$

and 

$E(X_m) = \frac{1}{p} = \frac{1}{0.5} = 2$

Which we may interpret as meaning the expected number of males/females born before having a child belonging to the other respective gender is 2. 

\newpage

> 3. 2.30 (a), (b), (c)

## Question 3
Find the moment generating function corresponding to: 

(a): $f(x) = \frac{1}{c}$, $0<x<c$

(b): $f(x) = \frac{2x}{c^2}$, $0<x<c$

(c): $f(x) = \frac{1}{2\beta}e^{\frac{-|x-\alpha|}{\beta}}$, $-\infty < x < \infty$, $-\infty < \alpha < \infty$, $\beta > 0$

## Answer 3

Note, for a continuous random variable X, we may write the moment generating funciton as: 

$$M_X(t) = \int\limits_{-\infty}^{\infty}{e^{tx} f_X(x) dx}$$

Using this method, we then calculate the following:

(a):

$$M_X(t) = \int\limits_{-\infty}^{\infty}{e^{tx} f_X(x) dx} = \int\limits_{0}^{c}{e^{tx} \frac{1}{c} dx} = \frac{1}{ct}e^{tx} \big|_{x=0}^{x=c} = \frac{1}{ct}e^{tc} - \frac{1}{ct}(1)$$

$$M_X(t) = \frac{1}{ct}e^{tc} - \frac{1}{ct}(1) = \frac{1}{ct}(e^{tc} -1)$$

(b): 

$$M_X(t) = \int\limits_{-\infty}^{\infty}{e^{tx} f_X(x) dx} = \int\limits_{0}^{c}{e^{tx}} \frac{2x}{c^2} dx = \frac{2}{c^2t^2}e^{tx}(tx-1) \big|_{x=0}^{x=c}$$

$$M_X(t) = \frac{2}{c^2t^2}e^{tc}(tc-1) - (\frac{2}{c^2t^2}1(-1)) = \frac{2}{c^2t^2}(tce^{tc} - e^{tc} + 1)$$

(c): 

\newpage

> 4. 2.31

## Question 4
Does a distribution exist for which $M_X(t) = \frac{t}{(1-t)}$, $|t| < 1$? If yes, find it. If no, prove it. 

## Answer 4

Let us suppose that the distribution exists. 

Then by the definition of a(n) mfg: 

$M_X(t) = E(e^{tX})$

We know for $t=0$ that the relation $|t| = | 0 | = 0 < 1$ holds. 

Thus we know the 0-th moment is defined, as: 

$M_X(0) = E(e^{0X}) = E(e^0) = E(1) = 1$

However, if we evaluate $M_X(t)$ directly using the mgf as given, for $t=0$ as given, we have: 

$M_X(t) = \frac{t}{(1-t)} = \frac{0}{1-0} = 0$

And we arrive at a contradiction. Thus we must conclude that such a distribution does not exist. 

\newpage

> 5. 

## Question 5
Suppose that X has the standard normal distribution with pdf: 

$$f(x) = \frac{1}{\sqrt{2\pi}} e^{\frac{-x^2}{2}}$$, $-\infty < x < \infty$

Then the random variable Y, $Y=e^{X}$ has a log-normal distribution. 

(a): Find $E(Y^r)$ for any r. 

(b): Show the moment generating function of Y does not exist (even though all moments of Y exist). 

## Answer 5

(a):

(b): 

\newpage

> 6. 

## Question 6 
Suppose that X has a normal distribution with pdf: 

$$f(x) \frac{1}{\sigma\sqrt{2\pi}} e^{\frac{-(x-\mu)^2}{\sigma^22}}$$, $-\infty < x < \infty$

The mean of X is $\mu$. Show that the moment generating function of X satisfies $M_X(t) \geq e^{t\mu}$

## Answer 6

With note of Jensen's Inequality, we have, for a convex function g, (avoiding confusion of the usage of f with the above pdf), 

Let us then consider the moment generation function of X, 

$M_X(t) = E(e^{tX})$

Consider then the function $e^{tX}$, specifically its second derivative: 

$$\frac{d^2}{dx^2} e^tX = t^2 e^{tX} > 0$$, $\forall x, t$

We may then note that the mgf of X is convex since its second derivative is positive. 

This is advantagous to our purposes, as we know when applying Jensen's inequality that we have a convex function, such that we may write: 

Since $E(X) = \mu$,

$$M_X(t) = E[e^{tX}] \geq e^{tE(X)} = e^{t\mu}$$
And we conclude 

$M_X(t) \geq e^{t\mu}$

\newpage

> 7. 

## Question 7
Suppose that $X$ has pmf $f(x) = p(1-p)^{x-1}$, for $x = 1, 2, 3, ...$ where $0<p<1$. Find the mgf $M_X(t)$ and use this to derive the mean and variance of X. 

## Answer 7

For deriving the mean and variance of X, we will need to first define the mgf of X as: 

$$M_X(t) = E(e^{tX}) = \sum\limits_{x=1}^{\infty} e^{tx} f(x) = \sum\limits_{x=1}^{\infty} e^{tx} p(1-p)^{x-1}$$

The mean $\mu = E(X)$ is equal to the first derivative of the mgf evaluated at $t = 0$:

$E(X) = M_X'(0)$

$$M_X'(t) = \frac{p e^t(1 - p)}{\left(1 - e^t(1 - p)\right)^2}$$

$$M_X'(0) = \frac{p(1 - p)}{(1 - (1 - p))^2} = \frac{1}{p}$$

$$E(X) = \frac{1}{p}$$

We then derive the variance of X. We know the typical variance formula as: 
$\text{Var}(X) = E(X^2) - (E(X))^2$

However, we just calculated $E(X)$! Additionally, we know that $E(X^2)$ is equal to the second derivative of the mgf at $t=0$. As such we write: 

$$M_X''(t) = \frac{p e^t(1 - p)\left(1 - e^t(1 - p) + e^t(1 - p)\right)}{\left(1 - e^t(1 - p)\right)^3}$$ 

To make computation easier, let $q = 1 - p$. Then, for $t = 0$, 

$$M_X''(0) = \frac{p-pq^2}{(1-q)^4} = \frac{p(1-q^2)}{(1-q)^4} = \frac{p(1+q)}{(1-q)^3} = \frac{1+q}{p^2}$$

Taking this calculation minus the square of the mean gives us: 

$$\text{Var}(X) = M_X''(0) - \left(M_X'(0)\right)^2 = \frac{1+q}{p^2} - \frac{1}{p^2} = \frac{q}{p^2} = \frac{1-p}{p^2}$$

And we conclude then

$$\text{Var}(X) =\frac{1-p}{p^2}$$


\newpage

> 8.

## Question 8
Suppose for one month a company purfchases c copies of a software package at a cost of $d_1$ dollars per copy. The packages are sold to customers for $d_2$ dollars per copy; any unsold copies are destroyed at the end of the month. Let $X$ represent the demand for this software package in the month. Assume that $X$ is a discrete random variable with pmf $f(x)$ and cdf $F(x)$. 

(a): Let $s = \text{min} \{ X, c\}$ represent the number of sales during the month. Show that:

$$E(S) = \sum\limits_{x=0}^{c} xf(x) + c(1-F(c))$$

(b): Let $Y = S * d_2 - cd_1$ represent the profit for the company, the total income from sales minus the total cost of all copies. Find $E(Y)$

(c): As $Y \equiv Y_c$ depends on integer $c \geq 0$, write the expected profit function as $g(c) \equiv E(Y_c)$ from part (b). The company should choose the value of c which maximizes $g(c)$; that is, choose the smallest c such that $g(c+1)$ is less than or equal to $g(c)$. Show that such $c \geq 0$ is the smallest integer with $F(c) \geq \frac{d_2 - d_1}{d_2}$

## Answer 8

(a): 

(b): 

(c): 
