---
title: "HW3"
output: pdf_document
date: "2024-09-28"
---
# HW 3
[Name: Sam Olson]{.smallcaps} \
[Collaborators: **Tne Hatman**]{.smallcaps} \

# Overview 
  - 1: DONE
  - 2: DONE
  - 3: DONE
  - 4: DONE
  - 5: DONE
  - 6: DONE
  - 7: DONE
  - 8: 

> 1. 2.23(b) 

## Question 1
Let X have the pdf

$$f(x) = \frac{1}{2}(1+x)$$, $-1<x<1$

Define the random variable Y by $Y = X^2$

(b): Find $E(Y)$ and Var(Y). 

## Answer 1  

(b): 

(From prior HW) Note from the results of theorem 2.1.8: 

$$f_Y(y) =
\begin{cases}
  \sum \limits_{i=1}^{k} f_X(g^{-1}_i(y))|\frac{d}{dy}g_{i}^{-1}(y)| & y \in \mathbb{Y} \\
  0 & otherwise \\ 
\end{cases}$$

Note: For $X \in [-1, 1]$, we have $Y = X^2 \in [0, 1]$

Over the following partitions, we have monotonicity, 

$A_1 = (-1, 0) \rightarrow X = -\sqrt{y}$, as $g_1(x) = x^2$, and 
 
$A_2 = (0, 1) \rightarrow X = \sqrt{y}$, as $g_2(x) = x^2$

Taking the absolute value of the derivatives, $|\frac{d}{dy}g_{i}^{-1}(y)|$, we have: 

$|\frac{d}{dy}g_{1}^{-1}(y)| = |\frac{d}{dy}g_{2}^{-1}(y)| = \frac{1}{2}y^{-1/2}$ 

Thus we have 

$f_Y(y) = \frac{1}{2}y^{-1/2} \frac{1}{2}[(1 + \sqrt{y}) + (1 - \sqrt{y})] = \frac{1}{4}y^{-1/2} [2] = \frac{1}{2} y^{-1/2}$

Such that we have the pdf of Y as:

$f_Y(y) = \frac{1}{2} y^{-1/2}$, $0 < y < 1$

Using this for our Expected value calculation we have: 

$E(Y) = \int\limits_{y \in \mathbb{Y}} yf(y)dy = \int\limits_{y=0}^{1} y(\frac{1}{2\sqrt y})dy$

$$E(Y) = \int\limits_{y=0}^{1} \sqrt y (\frac{1}{2})dy = \frac{1}{2} \frac{2}{3} y^{3/2} \big|_{y=0}^{y=1} = \frac{1}{2} \frac{2}{3} (1) - 0 = \frac{1}{3}$$

To calculate Var(Y), let us consider $E(Y^2)$, 

$E(Y^2) = \int\limits_{y \in \mathbb{Y}} y^2f(y)dy = \int\limits_{y=0}^{1} y^2(\frac{1}{2\sqrt y})dy$

$$E(Y^2) = \int\limits_{y=0}^{1} y^{3/2} \frac{1}{2} dy = \frac{2}{5} (\frac{1}{2}) y^{5/2} \big|_{y=1}^{y=1} = \frac{2}{5} (\frac{1}{2}) (1) - 0 = \frac{2}{10} = \frac{1}{5}$$
Taking Var(Y) = $E(Y^2) - (E(Y))^2$, then, 

$Var(Y) = \frac{1}{5} - (\frac{1}{3})^2 = \frac{1}{5} - \frac{1}{9} = \frac{9}{45} - \frac{5}{45} = \frac{4}{45}$

\newpage

> 2. 

## Question 2
A family continues to have children until they have one female child. Suppose, for each birth, a single child is born and the child is equally likely to be male or female. The gender outcomes are independent across births. 

(a): Let X be a random variable representing the number of children born to this family. Find the distribution of X. 

(b): Find the expected value $E(X)$

(c): Let $X_m$ denote the number of male children in this family and let $X_f$ denote the number of female children. Find the expected value of $X_m$ and the expected value of $X_f$

## Answer 2

(a): We can frame X as the number of children until the family has their first (one) female child. So we can think of X as a Geometric distribution with probability $p=0.5$ since it is equally likely that they have a male/female for each birth. 

Notation-wise we write this as: 

$X \sim \text{Geometric}(p = 0.5)$

(b): 

Knowing the distribution of X, we know its pmf (discrete!) is given by: 

For X number of children, $k=1, 2, ...$, we have: 

$f_X(x) = P(X = x) = p(1-p)^{x-1}$

$$E(X) = \sum\limits_{x=1}^{\infty} x P(X = x) = \sum\limits_{k=x}^{\infty} x (p(1-p)^{x-1}) = p \sum\limits_{x=1}^{\infty} x ((1-p)^{k-1})$$

Note, for the infinite geometric series we have, for $|r| < 1$, k some positive integer, the following holds: 
$$\sum\limits_{k = 1}^{\infty} r^{k} = \frac{1}{1-r}$$

Note: Let $q = 1-p$ for simplicity. 
As $0 < p < 1 \rightarrow 0 < 1-p < 1 \rightarrow 0 < q < 1$.
For our purposes, we have $|q| < 1$, such that the above relation holds for an infinite geometric series: 

$$\sum\limits_{x = 1}^{\infty} q^{x} = \frac{1}{1-q}$$

Note then: 

$$\frac{d}{dq} (\sum\limits_{x = 1}^{\infty} q^{x}) = \sum\limits_{x = 1}^{\infty} (\frac{d}{dq} q^{x}) = \sum\limits_{x = 1}^{\infty} x q^{x-1}$$
Additionally, 

$$\frac{d}{dq} (\frac{1}{1-q}) = \frac{d}{dq}[(1-q)^{-1}] = \frac{1}{(1-q)^2} = (1-q)^{-2}$$

Thus we have: 
$$E(X) = p (1-q)^{-2} = p ( 1 - (1 - p) ) ^ {-2} = p ( p)^{-2} = p^{-1} = \frac{1}{p}$$

For $p=0.5$, we have: 

$$E(X) = \frac{1}{p} = \frac{1}{0.5} = 2$$

Or, on average they would have two children before they have their first female. 

(c): 

Note: $X_f$ and $X_m$ are subsets of the random variable $X$, specifically $X_f + X_m = X$. 

We stop the "experiment" at the first female child, so we will only ever have 1 female child, meaning: 

$E(X_f) = 1$

The number of male children then is the number of children we expect to have minus the number of female children, which is:

$E(X_m) = E(X) - E(X_f) = 2 - 1 = 1$

We expect to have two children, one child is female and the other is male. 

\newpage

> 3. 2.30 (a), (b), (c)

## Question 3
Find the moment generating function corresponding to: 

(a): $f(x) = \frac{1}{c}$, $0<x<c$

(b): $f(x) = \frac{2x}{c^2}$, $0<x<c$

(c): $f(x) = \frac{1}{2\beta}e^{\frac{-|x-\alpha|}{\beta}}$, $-\infty < x < \infty$, $-\infty < \alpha < \infty$, $\beta > 0$

## Answer 3

Note, for a continuous random variable X, we may write the moment generating funciton as: 

$$M_X(t) = \int\limits_{-\infty}^{\infty}{e^{tx} f_X(x) dx}$$

Using this method, we then calculate the following:

(a):

$$M_X(t) = \int\limits_{-\infty}^{\infty}{e^{tx} f_X(x) dx} = \int\limits_{0}^{c}{e^{tx} \frac{1}{c} dx} = \frac{1}{ct}e^{tx} \big|_{x=0}^{x=c} = \frac{1}{ct}e^{tc} - \frac{1}{ct}(1)$$

$$M_X(t) = \frac{1}{ct}e^{tc} - \frac{1}{ct}(1) = \frac{1}{ct}(e^{tc} -1)$$

Note: For $t = 0$, $\frac{1}{ct}$ is not defined, so the above mgf is defined for $t \neq 0$.

(b): 

$$M_X(t) = \int\limits_{-\infty}^{\infty}{e^{tx} f_X(x) dx} = \int\limits_{0}^{c}{e^{tx}} \frac{2x}{c^2} dx$$
Via integration by parts, let $u = x, du = 1$ such that $dv = \frac{2e^{tx}}{c^2} = \rightarrow v = \frac{2e^{tx}}{tc^2}$

So our formula now is 

$$M_X(t) = \int u dv = uv - \int vdu = x\frac{2e^{tx}}{tc^2} - \frac{2e^{tx}}{t^2c^2} = xt \frac{2e^{tx}}{t^2c^2} - \frac{2e^{tx}}{t^2c^2}$$

Simplifying, we then evaluate over the original range (support of X), giving us: 

$$M_X(t) = \frac{2}{c^2t^2}e^{tx}(tx-1) \big|_{x=0}^{x=c}$$

$$M_X(t) = \frac{2}{c^2t^2}e^{tc}(tc-1) - (\frac{2}{c^2t^2}1(-1)) = \frac{2}{c^2t^2}(tce^{tc} - e^{tc} + 1)$$

Note: For $t = 0$, $\frac{1}{c^2t^2}$ is not defined, so the above mgf is defined for $t \neq 0$.

(c): 

$$M_X(t) = \int\limits_{-\infty}^{\infty}{e^{tx} f_X(x) dx} = \int\limits_{-\infty}^{\infty}{e^{tx} \frac{1}{2\beta}e^{\frac{-|x-\alpha|}{\beta}} dx}$$

With regards to the usage of $- |x - \alpha|$, when $x < \alpha \rightarrow (x - \alpha) < 0$ and when $x \geq \alpha \rightarrow (x - \alpha) \geq 0$

Thus, we may break the above integration into two parts, namely: 

$$M_X(t) = \int\limits_{-\infty}^{\alpha}{e^{tx} \frac{1}{2\beta}e^{\frac{(x-\alpha)}{\beta}} dx} + \int\limits_{\alpha}^{\infty}{e^{tx} \frac{1}{2\beta}e^{\frac{-(x-\alpha)}{\beta}} dx}$$

> 1. 

$$\int\limits_{-\infty}^{\alpha}{e^{tx} \frac{1}{2\beta}e^{\frac{(x-\alpha)}{\beta}} dx} = \frac{e^{tx + (x-\alpha)/\beta}}{2(t \beta + 1)} \big|_{x=-\infty}^{\alpha} = \frac{e^{tx}}{2(t \beta + 1)} - 0 = \frac{e^{t\alpha}}{2(t\beta + 1)}$$ 

> 2. 

$$\int\limits_{\alpha}^{\infty}{e^{tx} \frac{1}{2\beta}e^{\frac{-(x-\alpha)}{\beta}} dx} = \frac{e^{tx - (x-\alpha)/\beta}}{2(t \beta - 1)} \big|_{x = \alpha}^{\infty}  = 0 - \frac{e^{tx}}{2(t \beta - 1)}= - \frac{e^{t\alpha}}{2(t \beta - 1)}$$ 

> 3. 

Combining the above (1.) and (2.) together we then have: 

$$M_X(t) = \frac{e^{t\alpha}}{2(t\beta + 1)} - \frac{e^{t\alpha}}{2(t \beta - 1)} = \frac{4e^{\alpha t}}{4 - \beta^2t^2}$$

Note, we need to ensure the above Mgf of X evaluates, so we need to specify the conditions where the denominator is not equal to 0 (divide by zero error!). 

For a fixed $\beta$, consider: $4 - \beta^2t^2 = 0 \rightarrow 4 = \beta^2 t^2 \rightarrow 4/\beta^2 = t^2$

So the denominator is equal to 0 when $t = \sqrt{4/\beta^2} = \pm 2 / \beta$

However, we also know that $M_X(t) \geq 0$, (as we assume f(x) is a pdf, hence $f(x) \geq 0$) $\forall t$, so we actually have bounds for $t$, namely: 

The above mgf is defined for $- 2/ \beta < t < 2 / \beta$, where $\beta > 0$. 

\newpage

> 4. 2.31

## Question 4
Does a distribution exist for which $M_X(t) = \frac{t}{(1-t)}$, $|t| < 1$? If yes, find it. If no, prove it. 

## Answer 4

Let us suppose that the distribution exists. 

Then by the definition of a(n) mfg, using the existence of the distribution we have: 

$M_X(t) = E(e^{tX})$

We know for $t=0$ that the relation $|t| = | 0 | = 0 < 1$ holds. 

Thus we know the 0-th moment is defined: 

$M_X(0) = E(e^{0X}) = E(e^0) = E(1) = 1$

However, if we evaluate $M_X(t)$ directly using the mgf as given, for $t=0$ as given, we have: 

$M_X(t) = \frac{t}{(1-t)} = \frac{0}{1-0} = 0$

And we arrive at a contradiction. Thus we must conclude that such a distribution does not exist. 

\newpage

> 5. 

## Question 5
Suppose that X has the standard normal distribution with pdf: 

$$f(x) = \frac{1}{\sqrt{2\pi}} e^{\frac{-x^2}{2}}$$, $-\infty < x < \infty$

Then the random variable Y, $Y=e^{X}$ has a log-normal distribution. 

(a): Find $E(Y^r)$ for any r.

(b): Show the moment generating function of Y does not exist (even though all moments of Y exist). 

## Answer 5

(a):

$$E(Y^r) = E((e^{X})^r) = E(e^{rX})$$

$$E(Y^r) = \int_{-\infty}^{\infty} e^{rx} f(x) dx = \int_{-\infty}^{\infty} e^{rx} \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} dx$$

$$E(Y^r) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{rx - \frac{x^2}{2}} dx = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{1}{2}(x^2 - 2rx)} dx$$

$$E(Y^r) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{1}{2}((x - r)^2 - r^2)} dx = e^{\frac{r^2}{2}}  \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{1}{2}(x - r)^2} dx$$

The below is the normal distribution, which evaluates to $\sqrt{2\pi}$

$$\int_{-\infty}^{\infty} e^{-\frac{1}{2}(x - r)^2} = \sqrt{2\pi} $$

So we have: 

$$E(Y^r)  = \sqrt{2\pi} e^{\frac{r^2}{2}}  \frac{1}{\sqrt{2\pi}} = e^{r^2/2}$$

(b): 

Part (a) shows that all moments of Y exist. We must then show that the moment generating function of Y does not exist.

To that end, let us consider the moment generating function of Y: 

$M_Y(t) = E(e^tY) = E(e^{te^X})$

$$M_Y(t) = \int\limits_{-\infty}^{\infty} e^{te^{x}} \frac{1}{\sqrt{2\pi}} e^{\frac{-x^2}{2}}$$

$$M_Y(t) = \frac{1}{\sqrt{2\pi}} \int\limits_{-\infty}^{\infty} e^{te^{x}} e^{\frac{-x^2}{2}} dx = \frac{1}{\sqrt{2\pi}} \int\limits_{-\infty}^{\infty} e^{te^{x} - \frac{x^2}{2}} dx$$

Note: For $x>0$, define some positive real $c$, $c > 0$

There exists a sufficiently large $x_0$ such that for $x > x_0$: 

$te^{x} - \frac{x^2}{2} > 0$

Meaning: $te^{x} - \frac{x^2}{2} \geq c > 0$

Note the exponential function is a positive monotonic transformation, such that the following holds: 

$e^{te^{x} - \frac{x^2}{2}} > e^c > 0$, such that: 

With note that we are working with a non-negative function, 

$$M_Y(t) = \frac{1}{\sqrt{2\pi}} \int\limits_{-\infty}^{\infty} e^{te^{x} - \frac{x^2}{2}} dx \geq \frac{1}{\sqrt{2\pi}} \int\limits_{x_0}^{\infty} e^{te^{x} - \frac{x^2}{2}} dx \geq \frac{1}{\sqrt{2\pi}} \int\limits_{x_0}^{\infty} e^{c}dx = \infty$$

As the integral does not converge to a finite value, we say the moment generating function does not exist for positive real $t$. 

\newpage

> 6. 

## Question 6 
Suppose that X has a normal distribution with pdf: 

$$f(x) \frac{1}{\sigma\sqrt{2\pi}} e^{\frac{-(x-\mu)^2}{\sigma^22}}$$, $-\infty < x < \infty$

The mean of X is $\mu$. Show that the moment generating function of X satisfies $M_X(t) \geq e^{t\mu}$

## Answer 6

With note of Jensen's Inequality, we have, for a convex function g, (avoiding confusion of the usage of f with the above pdf), 

Let us then consider the moment generation function of X, 

$M_X(t) = E(e^{tX})$

Consider then the function $e^{tX}$, specifically its second derivative: 

$$\frac{d^2}{dx^2} e^tX = t^2 e^{tX} > 0$$, $\forall x, t$

We may then note that the mgf of X is convex since its second derivative is positive. 

This is advantagous to our purposes, as we know when applying Jensen's inequality that we have a convex function, such that we may write: 

Since $E(X) = \mu$,

$$M_X(t) = E[e^{tX}] \geq e^{tE(X)} = e^{t\mu}$$
And we conclude 

$M_X(t) \geq e^{t\mu}$

\newpage

> 7. 

## Question 7
Suppose that $X$ has pmf $f(x) = p(1-p)^{x-1}$, for $x = 1, 2, 3, ...$ where $0<p<1$. Find the mgf $M_X(t)$ and use this to derive the mean and variance of X. 

## Answer 7

For deriving the mean and variance of X, we will need to first define the mgf of X as: 

$$M_X(t) = E(e^{tX}) = \sum\limits_{x=1}^{\infty} e^{tx} f(x) = \sum\limits_{x=1}^{\infty} e^{tx} p(1-p)^{x-1}$$

The mean $\mu = E(X)$ is equal to the first derivative of the mgf evaluated at $t = 0$:

$E(X) = M_X'(0)$

$$M_X'(t) = \frac{p e^t(1 - p)}{\left(1 - e^t(1 - p)\right)^2}$$

$$M_X'(0) = \frac{p(1 - p)}{(1 - (1 - p))^2} = \frac{1}{p}$$

$$E(X) = \frac{1}{p}$$

We then derive the variance of X. We know the typical variance formula as: 
$\text{Var}(X) = E(X^2) - (E(X))^2$

However, we just calculated $E(X)$! Additionally, we know that $E(X^2)$ is equal to the second derivative of the mgf at $t=0$. As such we write: 

$$M_X''(t) = \frac{p e^t(1 - p)\left(1 - e^t(1 - p) + e^t(1 - p)\right)}{\left(1 - e^t(1 - p)\right)^3}$$ 

To make computation easier, let $q = 1 - p$. Then, for $t = 0$, 

$$M_X''(0) = \frac{p-pq^2}{(1-q)^4} = \frac{p(1-q^2)}{(1-q)^4} = \frac{p(1+q)}{(1-q)^3} = \frac{1+q}{p^2}$$

Taking this calculation minus the square of the mean gives us: 

$$\text{Var}(X) = M_X''(0) - \left(M_X'(0)\right)^2 = \frac{1+q}{p^2} - \frac{1}{p^2} = \frac{q}{p^2} = \frac{1-p}{p^2}$$

And we conclude then

$$\text{Var}(X) =\frac{1-p}{p^2}$$


\newpage

> 8.

## Question 8
Suppose for one month a company purchases c copies of a software package at a cost of $d_1$ dollars per copy. The packages are sold to customers for $d_2$ dollars per copy; any unsold copies are destroyed at the end of the month. Let $X$ represent the demand for this software package in the month. Assume that $X$ is a discrete random variable with pmf $f(x)$ and cdf $F(x)$. 

(a): Let $s = \text{min} \{ X, c\}$ represent the number of sales during the month. Show that:

$$E(S) = \sum\limits_{x=0}^{c} xf(x) + c(1-F(c))$$

(b): Let $Y = S * d_2 - cd_1$ represent the profit for the company, the total income from sales minus the total cost of all copies. Find $E(Y)$

(c): As $Y \equiv Y_c$ depends on integer $c \geq 0$, write the expected profit function as $g(c) \equiv E(Y_c)$ from part (b). The company should choose the value of c which maximizes $g(c)$; that is, choose the smallest c such that $g(c+1)$ is less than or equal to $g(c)$. Show that such $c \geq 0$ is the smallest integer with $F(c) \geq \frac{d_2 - d_1}{d_2}$

## Answer 8

(a): 

Consider two cases: (1): $x < c$, (2): $x > c$ (demand is either greater than or less than or equal to the number of copies sold)

> 1. 

$$E(X) = \sum\limits_{x=0}^{c}x P(X = x) = \sum\limits_{x=0}^{c}xf(x)$$

> 2. 

$$E(c) = \sum\limits_{x=0}^{c} c P(X > c) = \sum\limits_{x=0}^{c} c  (1 - F(c))$$

As $F(c) = P(X \leq c) \rightarrow P(X \leq c) + P(X > c) = 1$

> 3.

We combine parts (1) and (2) together then, for: 

$$E(S) = E(X) + E(c) = \sum\limits_{x=0}^{c}xf(x) + \sum\limits_{x=0}^{c} c  (1 - F(c)) = \sum\limits_{x=0}^{c}xf(x) + c(1-F(c))$$

(b): 

$Y = S d_2 - cd_1 \rightarrow E(Y) = E(S d_2 - cd_1)$

Given linearity of Expectation, we may rewrite this expectation as: 

$E(Y) = E(S d_2) - E(c d_1) = d_2 \cdot E(S) - c \cdot d_1$

$$E(Y) = d_2 \left(\sum_{x=0}^{c} x f(x) + c(1 - F(c))\right) - c d_1$$

$$E(Y) = d_2 \left(\sum_{x=0}^{c} x f(x) \right) + d_2 c(1 - F(c)) - c d_1$$

$$E(Y) = d_2 \left( \sum\limits_{x=0}^{c} x f(x) \right) + c(d_2 - d_1 - F(c))$$

(c): 

Using the above calculation for $E(Y)$, we may write the expected profit function as: 

$$g(c) = E(Y_c) = d_2 \left( \sum\limits_{x=0}^{c} x f(x) \right) + c(d_2 - d_1 - F(c))$$

$$g(c+1) = E(Y_c) = d_2 \left( \sum\limits_{x=0}^{c+1} x f(x) \right) + (c+1)(d_2 - d_1 - F(c+1))$$
Gathering like terms for simplicity: 

$$g(c+1) - g(c) = d_2 \left( \sum\limits_{x=0}^{c+1} x f(x) \right) - d_2 \left( \sum\limits_{x=0}^{c} x f(x) \right) +  (c+1)(d_2 - d_1 - F(c+1)) - c(d_2 - d_1 - F(c))$$

$$g(c+1) - g(c) = d_2 (c+1) f(c+1) + (d_2 - d_1 + F(c) - F(c+1))$$

> Hazy From Here 

$$d_2 (c+1) f(c+1) + (d_2 - d_1 + F(c) - F(c+1)) \geq 0$$

$g(c+1) \leq g(c) \rightarrow g(c+1) - g(c) \leq 0$

> Ending Answer 

$$F(c) \geq \frac{d_2 - d_1}{d_2}$$