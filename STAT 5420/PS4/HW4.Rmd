---
title: "HW4"
output:
  pdf_document: default
  html_document: default
date: "2024-09-29"
---

# Status 
  - Q1: DONE
  - Q2: DONE 
  - Q3: DONE 
  - Q4: DONE 
  - Q5: DONE
  - Q6: DONE
  - Q7: WIP 
  
# Homework 4 
Due October 13 

## Q1 
> Question: 3.6 (a), (b) Casella & Berger

A large number of insects are expected to be attracted to a certain variety of rose plant. A commercial insecticide is advertised as being 99% effective. Suppose 2,000 insects infest a rose garden where the insecticide has been applied and let $X =$ number of surviving insects. 

>> (a)

What probability distribution might provide a reasonable model for this experiment?

>> (b) 

Write down, but do not evaluate, an expression for the probability that fewer than 100 insects survive, suing the model in part (a) 

> Answer 
>> (a)

We may interpret X as the number of "failures" given an effective rate of 99%, or $p=1 - 0.99 = 0.01$ (1% chance of failure). As we are counting the number of failures, we have a dicrete random variable. We know $n=2,000$, or our total number of "trials" for the insecticide. 

Taken together, we have X as a Binomial distributed random variable, or: 

$X \sim Binomial(n = 2,000, p = 0.01)$

It's worth noting that we can also represent this as a Poisson distributed random variable with parameter $\lambda = np = 2000 (0.01) = 20$

>> (b) 

$P(X < 100) = P(X \leq 99)$

$\sum\limits_{x=0}^{99} P(X=x) = \sum\limits_{x=0}^{99} f(x) = \sum\limits_{x=0}^{99} {2,000 \choose x}(0.01)^x (0.99)^{2000-x}$

\newpage 
## Q2 
> Question: 3.13 (a) Casella & Berger

A truncated discrete distribution is one in which a particular class cannot be observed and is eliminated from the sample space. In particular, if X has range 0, 1, 2, ... and the 0 class cannot be observed (as is usually the case), the 0-truncated random variable $X_T$ has pmf: 

$$P(X_T = x) = \frac{P(X=x)}{P(X>0)}$$

for $x = 1, 2, ...$

Find the pmf, mean, and variance of the 0-truncated random variable starting from: 

(a)

$X \sim Poisson(\lambda)$

> Answer 
>> (a) 

The pmf of a Poisson distribution is: 

$P(X = x) = \frac{\lambda^x e^{-\lambda}}{x!}$

Also:

$P(X > 0) = 1 - P(X = 0) = 1 - \frac{\lambda^0 e^{-\lambda}}{0!} = 1 - e^{-\lambda}$

Combining these together gives us the truncated pmf: 

$$P(X_T = x) = \frac{P(X=x)}{P(X>0)} = \frac{\lambda^x e^{-\lambda}}{x!} / 1 - e^{-\lambda} = \frac{\lambda^x e^{-\lambda}}{x!( 1 - e^{-\lambda})}$$
For $x = 1, 2, ...$

Using the above pmf, we may find the mean as: 

$$E(X_T) = \sum\limits_{x=1}^{\infty} x P(X_T = x) = \sum\limits_{x=1}^{\infty} x \frac{\lambda^x e^{-\lambda}}{x!( 1 - e^{-\lambda})} = \frac{\lambda e^{-\lambda}}{(1 - e^{-\lambda})} \sum\limits_{x \geq 1} \frac{\lambda^{x-1}}{(x-1)!}$$

Let $y = x - 1$, such that we may rewrite the above as: 

$$E(X_T) = \frac{\lambda e^{-\lambda}}{(1 - e^{-\lambda})} \sum\limits_{y \geq 0} \frac{\lambda^{y}}{(y)!}$$

Using the infinite summation for Euler, namely: 

$$e^\lambda = \sum\limits_{y \geq 0} \lambda^y / y! $$

We may then evaluate this as: 

$$E(X_T) = \frac{\lambda e^{-\lambda}}{(1 - e^{-\lambda})} e^{\lambda} = \frac{\lambda}{(1 - e^{-\lambda})}$$

To then find the variance, let us consider $E(X_T ^2)$

$$E(X_T^2) = \sum\limits_{x \geq 1} x^2 P(X_T = x) = \sum\limits_{x\geq 1} x^2  \frac{\lambda^x e^{-\lambda}}{x!( 1 - e^{-\lambda})} = \frac{\lambda e^{-\lambda}}{(1-e^{-\lambda})} \sum\limits_{x\geq 1} x  \frac{\lambda^x}{(x-1)!}$$

$$E(X_T^2) = \frac{\lambda e^{-\lambda}}{(1-e^{-\lambda})} 
\big(\sum\limits_{x\geq 1} (x-1)  \frac{\lambda^{x-1}}{(x-1)!} + 
\sum\limits_{x\geq 1}   \frac{\lambda^{x-1}}{(x-1)!} \big) = 
\frac{\lambda e^{-\lambda}}{(1-e^{-\lambda})} 
\big(\lambda \sum\limits_{x\geq 2}  \frac{\lambda^{x-2}}{(x-2)!} + 
\sum\limits_{x\geq 1}   \frac{\lambda^{x-1}}{(x-1)!} \big)$$

Then, let $y = x-2$, $z= x-1$, we have: 

$$E(X_T^2) = \frac{\lambda e^{-\lambda}}{(1-e^{-\lambda})} 
\big(\lambda \sum\limits_{y\geq 0}  \frac{\lambda^{y}}{(y)!} + 
\sum\limits_{z \geq 0}   \frac{\lambda^{z}}{(z)!} \big) = 
\frac{\lambda e^{-\lambda}}{(1-e^{-\lambda})}
\big(\lambda e^{\lambda} + e^\lambda \big)$$

Thus: 

$$E(X_T^2) = \frac{\lambda^2 + \lambda}{(1-e^{-\lambda})}$$

$$Var(E_T) = E(X_T^2) - (E(X_T))^2 = \frac{\lambda^2 + \lambda}{(1-e^{-\lambda})} - (\frac{\lambda}{(1-e^{-\lambda})})^2$$

\newpage 
## Q3 
> Question: 3.17 Casella & Berger

Establish a formula similar to (3.3.18) for the gamma distribution. If $X \sim Gamma (\alpha, \beta)$, then for any positive constant v, 

$EX^v = \frac{\beta^v \Gamma(v + \alpha)}{\Gamma(\alpha)}$

> Answer 

Formula 3.3.18: 

$$EX^n = \frac{B(\alpha + n, \beta)}{B(\alpha, \beta)} = \frac{\Gamma(\alpha + n)\Gamma(\alpha + \beta)}{\Gamma(\alpha + \beta + n)\Gamma(\alpha)}$$

$$EX^v = \int\limits_{x = 0}^{\infty} x^v \frac{1}{\Gamma(\alpha) \beta^\alpha}x^{\alpha - 1}e^{-x/\beta}dx = \frac{1}{\Gamma(\alpha) \beta^\alpha} \int\limits_{x = 0}^{\infty} x^{(v + \alpha - 1)} e^{-x/\beta}dx$$ 
$$EX^v = \frac{\Gamma(v + \alpha) \beta ^ {v+\alpha}}{\Gamma(\alpha) \beta^\alpha} \int\limits_{x = 0}^{\infty} \frac{1}{\Gamma(v + \alpha) \beta ^ {v+\alpha}} x^{(v + \alpha - 1)} e^{-x/\beta}dx = \frac{\Gamma(v + \alpha) \beta ^ {v+\alpha}}{\Gamma(\alpha) \beta^\alpha} (1)$$
With note of a cheeky: $\frac{\Gamma(v + \alpha) \beta ^ {v+\alpha}}{\Gamma(v + \alpha) \beta ^ {v+\alpha}} = 1$

Simplifying gives us: 

$$EX^v = \frac{\Gamma(v + \alpha) \beta ^ {v+\alpha}}{\Gamma(\alpha) \beta^\alpha} = \frac{\Gamma(v + \alpha) \beta ^ {v}}{\Gamma(\alpha)}$$
$\forall v > -\alpha$ as The Gamma function is only defined for positive values. 

\newpage 
## Q4
> Question: 3.19 Casella & Berger 

Show that: 

$$\int\limits_{x}^{\infty} \frac{1}{\Gamma(\alpha)} z^{\alpha-1} e^{-z} dz = \sum\limits_{y=0}^{\alpha-1} \frac{x^y e^{-x}}{y!}$$

For $\alpha = 1, 2, 3, ...$

Hint: Use integration by parts. Express this formula as a probabilistic relationship between Poisson and Gamma random variables. 

> Answer 

Via Integration by Parts, let us take out the constant $\frac{1}{\Gamma(\alpha)}$, and let $\alpha = n$, such that we have: 

$$u = z^{\alpha-1}$$

$$du = (\alpha - 1)z^{\alpha-2}$$

$$dv = e^{-z}dz$$

$$v = -e^{-z}$$

Thus, we have :

$$\int u dv = uv - \int vdu$$

Giving us: 


$$\int\limits_{z = x}^{\infty} z^{n-1}z^{-z} dz = x^{n-1} e^{-x} - \int\limits_{z = x}^{\infty} z^{n-2}(n-2)(-z^{-z}) dz$$

Simplifying and adding back in the $\frac{1}{\Gamma(n)}$ term gives us: 

$$\frac{1}{\Gamma(n)} \int\limits_{z = x}^{\infty} z^{n-1}z^{-z} dz = \frac{1}{\Gamma(n)} \big[ x^{n-1} e^{-x} + \int\limits_{z = x}^{\infty} (n-1)z^{n-2}z^{-z} dz\big]$$

We then note the expansion of $\Gamma(n) = (n-1) \Gamma (n-1) = (n-1)(n-2)\Gamma(n-2) = ... = (n-1)!\Gamma(1) = (n-1)! (1) = (n-1)!$. Using this relation above gives us: 

$$\frac{1}{\Gamma(n)} \int\limits_{z = x}^{\infty} z^{n-1}z^{-z} dz = \frac{x^{n-1} e^{-x}}{(n-1)!} + \int\limits_{z = x}^{\infty} \frac{1}{(n-1)\Gamma(n-1)} (n-1)z^{n-2}z^{-z} dz$$

$$\frac{1}{\Gamma(n)} \int\limits_{z = x}^{\infty} z^{n-1}z^{-z} dz = \frac{x^{n-1} e^{-x}}{(n-1)!} + \frac{1}{\Gamma(n-1)} \int\limits_{z = x}^{\infty} z^{n-2}z^{-z} dz$$

Notice the first term in this evaluation is $\frac{x^y e^{-x}}{y!}$ evaluated at $y = n-1$!

Notice also that we may continue the integration by parts for the term 

$$\frac{1}{\Gamma(n-1)} \int\limits_{z = x}^{\infty} z^{n-2}z^{-z} dz$$, 

which will yield two parts, the first being the $\frac{x^y e^{-x}}{y!}$ evaluated at $y = n-2$ and the second part being another integral we may break into parts. 

The above process may be repeated until we arrive at an integral in the $\int vdu = 0$

In summary, we can continue breaking the integral into parts which coincide with elements of the summation in reverse order ($\alpha - 1 \text{ or } n-1$), meaning these two values are equal. 

This is all to say that we have shown the relationship between the random variable $X \sim Poisson(x)$ and the random variable $Y \sim Gamma(\alpha, 1)$, as $P(X > x) = P(Y \leq \alpha - 1)$

\newpage 
## Q5
> Question: 3.24 (a), (c) Casella & Berger 
Note: You can skip the part about showing that the pdf is a pdf; also, in (c), the variance will not exist unless a>2.

Many "named" distributions are special cases of the more common distributions already discussed. For each of the following named distributions derive the form of the pdf, ..., and calculate the mean and variance. 

>> (a) 

If $X \sim Exponential(\beta)$, then $Y = X ^ {1/\gamma}$ has the Weibull$(\gamma, \beta)$ distribution, where $\gamma > 0$ is a constant.

>> (c) 

If $X \sim Gamma(a, b)$, then $Y = 1/X$ has the inverted Gamma IG(a,b) distribution. 

> Answer 
>> (a) 

$X \sim Exponential(\beta) \rightarrow f_X(x) = \frac{1}{\beta} e^{-x/\beta}$

Using the following general relation for two related random variables: 

Thm. 2.1.5: $$f_X(x) = f_z(g^-1(x)) | \frac{d}{dx} g^{-1}(x)|$$

We have: 

$$Y = X ^ {1/\gamma} \rightarrow f_Y(y) = f_x(y^\gamma)(d/dy) = \frac{1}{\beta} e^{-y^\gamma / \beta} \gamma y^{\gamma - 1}$$

For $x>0$, $y>0$

For simplicity, let us consider $EY^n = \int\limits_{y = 0}^{\infty} y^n f(y) = \int\limits_{y = 0}^{\infty} y^n \frac{1}{\beta} e^{-y^\gamma / \beta} \gamma y^{\gamma - 1}$

Simplifying terms somewhat, and removing terms that don't depend on y, we have: 

$$EY^n = \gamma / \beta \int\limits_{y = 0}^{\infty} y^{\gamma + n - 1} e^{-y^\gamma/ \beta} dy$$

Let $z = y^\gamma / \beta \rightarrow dz = \frac{\gamma}{\beta} y^{\gamma - 1}dy$
Such that $\frac{\beta}{\gamma y^{\gamma - 1}} dz = dy$

Change of variables allows us to rewrite the above as: 

$$EY^n = \frac{\gamma}{\beta} \frac{\beta}{\gamma} \beta^{\gamma + n - 1/ \gamma}  \int\limits_{z = 0}^{\infty} z^{\frac{\gamma + n - 1}{\gamma}} z^{\frac{1}{\gamma} - 1} e^{-z} dz$$ 
Simplifying terms somewhat gives us: 

$$EY^n = \beta^{\gamma + n - 1/ \gamma} \int\limits_{z = 0}^{\infty} z^{n/\gamma} e^{-z} dz$$ 

Recognizing this as the $\Gamma (n / \gamma + 1)$ function and simplifying constants, we thus have: 

$$EY^n = \beta ^{n / \gamma}\Gamma (n / \gamma + 1)$$

Using the above formula we have: 

$EY = \beta ^{1 / \gamma}\Gamma (1 / \gamma + 1)$

and

$EY^2 = \beta ^{2 / \gamma}\Gamma (2 / \gamma + 1)$

$$Var(Y) = EY^2 - (EY)^2 = \beta ^{2 / \gamma}\Gamma (2 / \gamma + 1) - (\beta ^{1 / \gamma}\Gamma (1 / \gamma + 1))^2 $$

$$Var(Y) = \beta^{2/ \gamma} [\Gamma (2 / \gamma + 1) - (\Gamma (1 / \gamma + 1))^2]$$

>> (c) 

$X \sim Gamma(a, b) \rightarrow f_X(x) = \frac{1}{\Gamma(a) b^a}x^{a - 1} e^{-x/b}$

With note of Thm. 2.1.5: $$f_X(x) = f_z(g^-1(x)) | \frac{d}{dx} g^{-1}(x)|$$

$Y = 1/X \rightarrow f_Y(y) = f_X(1/y)(d/dy) = \frac{1}{\Gamma(a) b^a}(1/y)^{a - 1} e^{-1/by}$

$EY = \int\limits_{y=0}^{\infty} y\frac{1}{\Gamma(a) b^a}(1/y)^{a - 1} e^{-1/by} dy$

$$EY = \frac{1}{\Gamma(a)b^a}\int\limits_{y=0}^{\infty} (\frac{1}{y})^a e^{-1/by} dy = \frac{\Gamma(a-1)b^{a-1}}{\Gamma(a)b^a} = \frac{1}{(a-1)b}$$

Note: $b \neq 0$ and $a \neq 1$ in order for the above to evaluate. 

$EY^2 = \int\limits_{y=0}^{\infty} y^2 \frac{1}{\Gamma(a) b^a}(1/y)^{a - 1} e^{-1/by} dy$

$$EY^2 = \frac{1}{\Gamma(a)b^a}\int\limits_{y=0}^{\infty} (\frac{1}{y})^{a-1} e^{-1/by} dy = \frac{\Gamma(a-2)b^{a-2}}{\Gamma(a)b^a} = \frac{1}{(a-1)(a-2)b^2}$$

$Var(Y) = EY^2 - (EY)^2 = \frac{1}{(a-1)(a-2)b^2} - (\frac{1}{(a-1)b})^2 = \frac{1}{(a-1)^2(a-2)b^2}$

Note: $Var(Y)$ does not exist unless $a > 2$ as variance must be positive. 

\newpage 
## Q6
> Question: 3.39 Casella & Berger 

Consider the Cauchy family defined in Section 3.3. This family can be extended to a location-scale family yielding pdfs of the form: 

$$f(x| \mu, \sigma) = \frac{1}{\sigma \pi ( 1 + (\frac{x - \mu}{\sigma})^2)}$$
For $-\infty < x < \infty$

The mean and variance do not exist for thje Cauchy distribution. So the parameters $\mu$, $\sigma^2$ are not the mean and variance. But they do have important meaning. Show that if X is a random variable with a Cauchy distribution with parameters $\mu$ and $\sigma$, then: 

>> (a) 

$\mu$ is the median of the distribution of X, that is, $P(X \geq \mu) = P(X \leq \mu) = \frac{1}{2}$

>> (b) 

$\mu + \sigma$ and $\mu - \sigma$ are the quartiles of the distribution of X, that is $P(X \geq \mu + \sigma) = P(X \leq \mu - \sigma) = \frac{1}{4}$

Hint: Prove this first for $\mu = 0$ and $sigma = 1$ and then use Exercise 3.38. 

Note: $\frac{d(arctanx)}{dx} = \frac{1}{1+x^2}$

> Answer 

>> (a) 

$\mu$ is the median if $$P(X \geq \mu) = P(X \leq \mu) = \frac{1}{2}$$

Let us consider the random Cauchy variable Z where $\mu=0$, $\sigma = 1$, 
$Z \sim Cauchy(\mu = 0, \sigma = 1)$

Let us consider $\mu = 0$, such that: $X = Z + \mu$

$P (Z \geq 0 ) = \int\limits_{z=0}^{\infty} \frac{1}{\pi} \frac{1}{1+z^2} dz = \frac{1}{\pi} arctan(z) \big|_{z=0}^{\infty} = \frac{1}{\pi}(\pi / 2 - 0) = \frac{1}{2}$

and 

$P (Z \leq 0 ) = \int\limits_{z=-\infty}^{0} \frac{1}{\pi} \frac{1}{1+z^2} dz = \frac{1}{\pi} arctan(z) \big|_{z=-\infty}^{0} = \frac{1}{\pi}(0 - (-\pi / 2)) = \frac{1}{2}$

We have shown then that for the random variable Z, $\mu$ is indeed the median. 

Taking advantage of the results of Exercise 3.38 we then have X belongs a location family of Z: 

So $P(Z \geq 0) = P( X \geq \mu) = \frac{1}{2}$

And 

So $P(Z \leq 0) = P( X \leq \mu) = \frac{1}{2}$

So the $mu$ is the median for the random variable X from the Cauchy family. 

>> (b) 

Let us consider the random Cauchy variable Z where $\mu=0$, $\sigma = 1$, such that: $X = \sigma Z + \mu$, $Z \sim Cauchy(\mu = 0, \sigma = 1)$

$$P(Z \geq \mu + \sigma) = P(Z \geq 1) = \int\limits_{z=1}^{\infty} \frac{1}{\pi} \frac{1}{1+z^2} dz  = \frac{1}{\pi}arctanz \big|_{z=1}^{\infty} = \frac{1}{\pi} (\pi /2 - \pi/4) = \frac{1}{4}$$

$$P(Z \leq \mu - \sigma) = P(Z \leq -1) = \int\limits_{z=-\infty}^{-1} \frac{1}{\pi} \frac{1}{1+z^2} dz  = \frac{1}{\pi}arctanz \big|_{z=-infty}^{-1} = \frac{1}{4}$$

Taking advantage of the results of Exercise 3.38 we then have X belongs to a scale-location family of Z and we may write: 

$P(Z \geq 1) = P(X \geq \mu + \sigma)$
$P(Z \leq - 1) = P(X \geq \mu - \sigma)$

So the following holds: 
$P(X \geq \mu + \sigma) = \frac{1}{4}$
$P(X \leq \mu - \sigma) = \frac{1}{4}$

Such that we have shown that $\mu + \sigma$ and $\mu - \sigma$ are the quartiles of the distribution of X. 

\newpage 
## Q7 
> Question: 

If $X \sim N(\mu, \sigma^2)$, find values of $\mu$ and $\sigma$ such that $P(|X|< 2) = \frac{1}{2}$. Prove or disprove that the values of $\mu$ and $\sigma$ are unique. 

> Answer 

Note: For any normally distributed random variable X, the standard normal Z may be derived by 

$$Z = \frac{X - \mu}{\sigma} \sim N(0, 1)$$

Furthermore, as  

$|X| < 2 \rightarrow -2 < X < 2$

Such that: 

$$P(|X| < 2) = P(-2 < X < 2) = P\left(\frac{-2 - \mu}{\sigma} < \frac{X - \mu}{\sigma} < \frac{2 - \mu}{\sigma}\right) = P\left(\frac{-2 - \mu}{\sigma} < Z < \frac{2 - \mu}{\sigma}\right)$$

$$P(|X| < 2) = P\left(Z < \frac{2 - \mu}{\sigma}\right) - P\left(Z < \frac{-2 - \mu}{\sigma} \right)$$

For the cdf of the standard normal $\Phi$, we have: 

$\Phi(\frac{2 - \mu}{\sigma}) - \Phi(\frac{-2 - \mu}{\sigma}) = \frac{1}{2}$

$\Phi(\frac{2 - \mu}{\sigma}) - (1 - \Phi(\frac{2 + \mu}{\sigma})) = \frac{3}{2}$

For simplicity, let $\mu = 0$ 

$\Phi(\frac{2}{\sigma}) + \Phi(\frac{2}{\sigma}) = \frac{3}{2} \rightarrow \Phi(\frac{2}{\sigma}) = \frac{3}{4}$

As $\Phi(\frac{2}{\sigma}) = \frac{3}{4}$, 
We may solve for $\sigma$ by using the inverse cdf, $\Phi^{-1}$, 

$\frac{2}{\sigma} = \Phi^{-1}(\frac{3}{4}) \rightarrow \frac{2}{\Phi^{-1}(\frac{3}{4})} = \sigma$

$\sigma \approx 2.97$

Notice the above held for $\mu = 0$. However, this choice was arbitrary. So while this probability is unique to $\sigma = 2.97$, it holds for any finite $\mu$. So $\sigma$ is unique, but $\mu$ is not. This is because the normal distribution is symmetrical about the mean, i.e. $\Phi(z) = 1 - \Phi(-z)$, $\forall z$. 