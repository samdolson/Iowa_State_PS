---
title: "HW4"
output:
  pdf_document: default
  html_document: default
date: "2024-09-29"
---

# Status 
  - Q1: DONE
  - Q2: DONE 
  - Q3:
  - Q4: 
  - Q5:
  - Q6:
  - Q7:
  
# Homework 4 
Due October 13 

## Q1 
> Question: 3.6 (a), (b) Casella & Berger

A large number of insects are expected to be attracted to a certain variety of rose plant. A commercial insecticide is advertised as being 99% effective. Suppose 2,000 insects infest a rose garden where the insecticide has been applied and let $X =$ number of surviving insects. 

>> (a)

What probability distribution might provide a reasonable model for this experiment?

>> (b) 

Write down, but do not evaluate, an expression for the probability that fewer than 100 insects survive, suing the model in part (a) 

> Answer 
>> (a)

We may interpret X as the number of "failures" given an effective rate of 99%, or $p=1 - 0.99 = 0.01$ (1% chance of failure). As we are counting the number of failures, we have a dicrete random variable. We know $n=2,000$, or our total number of "trials" for the insecticide. 

Taken together, we have X as a Binomial distributed random variable, or: 

$X \sim Binomial(n = 2,000, p = 0.01)$

It's worth noting that we can also represent this as a Poisson distributed random variable with parameter $\lambda = np = 2000 (0.01) = 20$

>> (b) 

$P(X < 100) = P(X \leq 99)$

$\sum\limits_{x=0}^{99} P(X=x) = \sum\limits_{x=0}^{99} f(x) = \sum\limits_{x=0}^{99} {n \choose x}(0.01)^x (0.99)^{2000-k}$

\newpage 
## Q2 
> Question: 3.13 (a) Casella & Berger

A truncated discrete distribution is one in which a particular class cannot be observed and is eliminated from the sample space. In particular, if X has range 0, 1, 2, ... and the 0 class cannot be observed (as is usually the case), the 0-truncated random variable $X_T$ has pmf: 

$$P(X_T = x) = \frac{P(X=x)}{P(X>0)}$$

for $x = 1, 2, ...$

Find the pmf, mean, and variance of the 0-truncated random variable starting from: 

(a)

$X \sim Poisson(\lambda)$

> Answer 
>> (a) 

The pmf of a Poisson distribution is: 

$P(X = x) = \frac{\lambda^x e^{-\lambda}}{x!}$

Also:

$P(X > 0) = 1 - P(X = 0) = 1 - \frac{\lambda^0 e^{-\lambda}}{0!} = 1 - e^{-\lambda}$

Combining these together gives us the truncated pmf: 

$$P(X_T = x) = \frac{P(X=x)}{P(X>0)} = \frac{\lambda^x e^{-\lambda}}{x!} / 1 - e^{-\lambda} = \frac{\lambda^x e^{-\lambda}}{x!( 1 - e^{-\lambda})}$$
For $x = 1, 2, ...$

Using the above pmf, we may find the mean as: 

$$E(X_T) = \sum\limits_{x=1}^{\infty} x P(X_T = x) = \sum\limits_{x=1}^{\infty} x \frac{\lambda^x e^{-\lambda}}{x!( 1 - e^{-\lambda})} = \frac{\lambda e^{-\lambda}}{(1 - e^{-\lambda})} \sum\limits_{x \geq 1} \frac{\lambda^{x-1}}{(x-1)!}$$

Let $y = x - 1$, such that we may rewrite the above as: 

$$E(X_T) = \frac{\lambda e^{-\lambda}}{(1 - e^{-\lambda})} \sum\limits_{y \geq 0} \frac{\lambda^{y}}{(y)!}$$

Using the infinite summation for Euler, namely: 

$$e^\lambda = \sum\limits_{y \geq 0} \lambda^y / y! $$

We may then evaluate this as: 

$$E(X_T) = \frac{\lambda e^{-\lambda}}{(1 - e^{-\lambda})} e^{\lambda} = \frac{\lambda}{(1 - e^{-\lambda})}$$

To then find the variance, let us consider $E(X_T ^2)$

$$E(X_T^2) = \sum\limits_{x \geq 1} x^2 P(X_T = x) = \sum\limits_{x\geq 1} x^2  \frac{\lambda^x e^{-\lambda}}{x!( 1 - e^{-\lambda})} = \frac{\lambda e^{-\lambda}}{(1-e^{-\lambda})} \sum\limits_{x\geq 1} x  \frac{\lambda^x}{(x-1)!}$$

$$E(X_T^2) = \frac{\lambda e^{-\lambda}}{(1-e^{-\lambda})} 
\big(\sum\limits_{x\geq 1} (x-1)  \frac{\lambda^{x-1}}{(x-1)!} + 
\sum\limits_{x\geq 1}   \frac{\lambda^{x-1}}{(x-1)!} \big) = 
\frac{\lambda e^{-\lambda}}{(1-e^{-\lambda})} 
\big(\lambda \sum\limits_{x\geq 2}  \frac{\lambda^{x-2}}{(x-2)!} + 
\sum\limits_{x\geq 1}   \frac{\lambda^{x-1}}{(x-1)!} \big)$$

Then, let $y = x-2$, $z= x-1$, we have: 

$$E(X_T^2) = \frac{\lambda e^{-\lambda}}{(1-e^{-\lambda})} 
\big(\lambda \sum\limits_{y\geq 0}  \frac{\lambda^{y}}{(y)!} + 
\sum\limits_{z \geq 0}   \frac{\lambda^{z}}{(z)!} \big) = 
\frac{\lambda e^{-\lambda}}{(1-e^{-\lambda})}
\big(\lambda e^{\lambda} + e^\lambda \big)$$

Thus: 

$$E(X_T^2) = \frac{\lambda^2 + \lambda}{(1-e^{-\lambda})}$$

$$Var(E_T) = E(X_T^2) - (E(X_T))^2 = \frac{\lambda^2 + \lambda}{(1-e^{-\lambda})} - (\frac{\lambda}{(1-e^{-\lambda})})^2$$

\newpage 
## Q3 
> Question: 3.17 Casella & Berger

Establish a formula simialr to (3.3.18) for the gamma distribution. If $X \sim Gamma (\alpha, \beta)$, then for any positive constant v, 

$EX^v = \frac{\beta^v \Gamma(v + \alpha)}{\Gamma(\alpha)}$

> Answer 

\newpage 
## Q4
> Question: 3.19 Casella & Berger 

Show that: 

$$\int\limits_{x}^{\infty} \frac{1}{\Gamma(\alpha)} z^{\alpha-1} e^{-z} dz = \sum\limits_{y=0}^{\alpha-1} \frac{x^y e^{-x}}{y!}$$

For $\alpha = 1, 2, 3, ...$

Hint: Use integration by parts. Express this formula as a probabilistic relationbship between Poisson and Gamma random variables. 

> Answer 

\newpage 
## Q5
> Question: 3.24 (a), (c) Casella & Berger 
Note: You can skip the part about showing that the pdf is a pdf; also, in (c), the variance will not exist unless a >2.

Many "named" distributions are special cases of the more common distributions already discussed. For each of the following named distributions derive the form of the pdf, ..., and calculate the mean and variance. 

>> (a) 

If $X \sim Exponential(\beta)$, then $Y = X ^ {1/\gamma}$ has the Weibull$(\gamma, \beta)$ distribution, where $\gamma > 0$ is a constant.

>> (c) 

If $X \sim Gamma(a, b)$, then $Y = 1/X$ has the inverted Gamma IG(a,b) distribution. 

> Answer 
>> (a) 

>> (c) 


\newpage 
## Q6
> Question: 3.39 Casella & Berger 

Consider the Cauchy family defined in Section 3.3. This family can be extended to a location-scale family yielding pdfs of the form: 

$$f(x| \mu, \sigma) = \frac{1}{\sigma \pi ( 1 + (\frac{x - \mu}{\sigma})^2)}$$
For $-\infty < x < \infty$

The mean and variance do not exist for thje Cauchy distribution. So the parameters $\mu$, $\sigma^2$ are not the mean and variance. But they do have important meaning. Show that if X is a random variable with a Cauchy distribution with parameters $\mu$ and $\sigma$, then: 

>> (a) 

$\mu$ is the median of the distribution of X, that is, $P(X \geq \mu) = P(X \leq \mu) = \frac{1}{2}$

>> (b) 

$\mu + \sigma$ and $\mu - \sigma$ are the quartiles of the distribution of X, that is $P(X \geq \mu + \sigma) = P(X \leq \mu - \sigma) = \frac{1}{4}$

Hint: Prove this first for $\mu = 0$ and $sigma = 1$ and then use Exercise 3.38. 

Note: $\frac{d(arctanx)}{dx} = \frac{1}{1+x^2}$

> Answer 
>> (a) 

>> (b) 

\newpage 
## Q7 
> Question: 

If $X \sim N(\mu, \sigma^2)$, find values of $\mu$ and $\sigma$ such that $P(|X|< 2) = \frac{1}{2}$. Prove or disprove that the values of $\mu$ and $\sigma$ are unique. 

> Answer 
