---
title: "HW4"
output:
  pdf_document: default
  html_document: default
date: "2024-09-29"
---

# Q's 
Q7
  
# Homework 4 
Due October 13 

## Q1 
> Question: 3.6 (a), (b) Casella & Berger

A large number of insects are expected to be attracted to a certain variety of rose plant. A commercial insecticide is advertised as being 99% effective. Suppose 2,000 insects infest a rose garden where the insecticide has been applied and let $X =$ number of surviving insects. 

>> (a)

What probability distribution might provide a reasonable model for this experiment?

>> (b) 

Write down, but do not evaluate, an expression for the probability that fewer than 100 insects survive, suing the model in part (a) 

> Answer 
>> (a)

We may interpret X as the number of "failures" given an effective rate of 99%, or $p=1 - 0.99 = 0.01$ (1% chance of failure). As we are counting the number of failures, we have a dicrete random variable. We know $n=2,000$, or our total number of "trials" for the insecticide. 

Taken together, we have X as a Binomial distributed random variable, or: 

$X \sim Binomial(n = 2,000, p = 0.01)$

It's worth noting that we can also represent this as a Poisson distributed random variable with parameter $\lambda = np = 2000 (0.01) = 20$

>> (b) 

$P(X < 100) = P(X \leq 99)$

$\sum\limits_{x=0}^{99} P(X=x) = \sum\limits_{x=0}^{99} f(x) = \sum\limits_{x=0}^{99} {2,000 \choose x}(0.01)^x (0.99)^{2000-x}$

\newpage 
## Q2 
> Question: 3.13 (a) Casella & Berger

A truncated discrete distribution is one in which a particular class cannot be observed and is eliminated from the sample space. In particular, if X has range 0, 1, 2, ... and the 0 class cannot be observed (as is usually the case), the 0-truncated random variable $X_T$ has pmf: 

$$P(X_T = x) = \frac{P(X=x)}{P(X>0)}$$

for $x = 1, 2, ...$

Find the pmf, mean, and variance of the 0-truncated random variable starting from: 

(a)

$X \sim Poisson(\lambda)$

> Answer 
>> (a) 

The pmf of a Poisson distribution is: 

$P(X = x) = \frac{\lambda^x e^{-\lambda}}{x!}$

Also:

$P(X > 0) = 1 - P(X = 0) = 1 - \frac{\lambda^0 e^{-\lambda}}{0!} = 1 - e^{-\lambda}$

Combining these together gives us the truncated pmf: 

$$P(X_T = x) = \frac{P(X=x)}{P(X>0)} = \frac{\lambda^x e^{-\lambda}}{x!} / 1 - e^{-\lambda} = \frac{\lambda^x e^{-\lambda}}{x!( 1 - e^{-\lambda})}$$
For $x = 1, 2, ...$

Using the above pmf, we may find the mean as: 

$$E(X_T) = \sum\limits_{x=1}^{\infty} x P(X_T = x) = \sum\limits_{x=1}^{\infty} x \frac{\lambda^x e^{-\lambda}}{x!( 1 - e^{-\lambda})} = \frac{\lambda e^{-\lambda}}{(1 - e^{-\lambda})} \sum\limits_{x \geq 1} \frac{\lambda^{x-1}}{(x-1)!}$$

Let $y = x - 1$, such that we may rewrite the above as: 

$$E(X_T) = \frac{\lambda e^{-\lambda}}{(1 - e^{-\lambda})} \sum\limits_{y \geq 0} \frac{\lambda^{y}}{(y)!}$$

Using the infinite summation for Euler, namely: 

$$e^\lambda = \sum\limits_{y \geq 0} \lambda^y / y! $$

We may then evaluate this as: 

$$E(X_T) = \frac{\lambda e^{-\lambda}}{(1 - e^{-\lambda})} e^{\lambda} = \frac{\lambda}{(1 - e^{-\lambda})}$$

To then find the variance, let us consider $E(X_T ^2)$

$$E(X_T^2) = \sum\limits_{x \geq 1} x^2 P(X_T = x) = \sum\limits_{x\geq 1} x^2  \frac{\lambda^x e^{-\lambda}}{x!( 1 - e^{-\lambda})} = \frac{\lambda e^{-\lambda}}{(1-e^{-\lambda})} \sum\limits_{x\geq 1} x  \frac{\lambda^x}{(x-1)!}$$

$$E(X_T^2) = \frac{\lambda e^{-\lambda}}{(1-e^{-\lambda})} 
\big(\sum\limits_{x\geq 1} (x-1)  \frac{\lambda^{x-1}}{(x-1)!} + 
\sum\limits_{x\geq 1}   \frac{\lambda^{x-1}}{(x-1)!} \big) = 
\frac{\lambda e^{-\lambda}}{(1-e^{-\lambda})} 
\big(\lambda \sum\limits_{x\geq 2}  \frac{\lambda^{x-2}}{(x-2)!} + 
\sum\limits_{x\geq 1}   \frac{\lambda^{x-1}}{(x-1)!} \big)$$

Then, let $y = x-2$, $z= x-1$, we have: 

$$E(X_T^2) = \frac{\lambda e^{-\lambda}}{(1-e^{-\lambda})} 
\big(\lambda \sum\limits_{y\geq 0}  \frac{\lambda^{y}}{(y)!} + 
\sum\limits_{z \geq 0}   \frac{\lambda^{z}}{(z)!} \big) = 
\frac{\lambda e^{-\lambda}}{(1-e^{-\lambda})}
\big(\lambda e^{\lambda} + e^\lambda \big)$$

Thus: 

$$E(X_T^2) = \frac{\lambda^2 + \lambda}{(1-e^{-\lambda})}$$

$$Var(E_T) = E(X_T^2) - (E(X_T))^2 = \frac{\lambda^2 + \lambda}{(1-e^{-\lambda})} - (\frac{\lambda}{(1-e^{-\lambda})})^2$$

\newpage 
## Q3 
> Question: 3.17 Casella & Berger

Establish a formula similar to (3.3.18) for the gamma distribution. If $X \sim Gamma (\alpha, \beta)$, then for any positive constant v, 

$EX^v = \frac{\beta^v \Gamma(v + \alpha)}{\Gamma(\alpha)}$

> Answer 

Formula 3.3.18: 

$$EX^n = \frac{B(\alpha + n, \beta)}{B(\alpha, \beta)} = \frac{\Gamma(\alpha + n)\Gamma(\alpha + \beta)}{\Gamma(\alpha + \beta + n)\Gamma(\alpha)}$$

$$EX^v = \int\limits_{x = 0}^{\infty} x^v \frac{1}{\Gamma(\alpha) \beta^\alpha}x^{\alpha - 1}e^{-x/\beta}dx = \frac{1}{\Gamma(\alpha) \beta^\alpha} \int\limits_{x = 0}^{\infty} x^{(v + \alpha - 1)} e^{-x/\beta}dx$$ 

Two items of note here: 

(1): 

$$\frac{\Gamma(v + \alpha) \beta ^ {v+\alpha}}{\Gamma(v + \alpha) \beta ^ {v+\alpha}} = 1$$, such that: 

$$\frac{1}{\Gamma(\alpha) \beta^\alpha} \int\limits_{x = 0}^{\infty} x^{(v + \alpha - 1)} e^{-x/\beta}dx = \frac{\Gamma(v + \alpha) \beta ^ {v+\alpha}}{\Gamma(v + \alpha) \beta ^ {v+\alpha}} \frac{1}{\Gamma(\alpha) \beta^\alpha} \int\limits_{x = 0}^{\infty} x^{(v + \alpha - 1)} e^{-x/\beta}dx  =  \frac{\Gamma(v + \alpha) \beta ^ {v+\alpha}}{\Gamma(\alpha) \beta^\alpha} \int\limits_{x = 0}^{\infty} \frac{1}{\Gamma(v + \alpha) \beta ^ {v+\alpha}} x^{(v + \alpha - 1)} e^{-x/\beta}dx$$

(2): 

$$\int\limits_{x = 0}^{\infty} \frac{1}{\Gamma(v + \alpha) \beta ^ {v+\alpha}} x^{(v + \alpha - 1)} e^{-x/\beta}dx$$ is the evaluation of the pdf of the $\Gamma(v + \alpha)$ distribution over its support (evaluates to 1!)

Taking (1) and (2) together, we may write: 

$$EX^v = \frac{\Gamma(v + \alpha) \beta ^ {v+\alpha}}{\Gamma(\alpha) \beta^\alpha} \int\limits_{x = 0}^{\infty} \frac{1}{\Gamma(v + \alpha) \beta ^ {v+\alpha}} x^{(v + \alpha - 1)} e^{-x/\beta}dx = \frac{\Gamma(v + \alpha) \beta ^ {v+\alpha}}{\Gamma(\alpha) \beta^\alpha} (1)$$
Special thanks to that cheeky: $\frac{\Gamma(v + \alpha) \beta ^ {v+\alpha}}{\Gamma(v + \alpha) \beta ^ {v+\alpha}} = 1$

Simplifying gives us: 

$$EX^v = \frac{\Gamma(v + \alpha) \beta ^ {v+\alpha}}{\Gamma(\alpha) \beta^\alpha} = \frac{\Gamma(v + \alpha) \beta ^ {v}}{\Gamma(\alpha)}$$
$\forall v > -\alpha$ as The Gamma function is only defined for positive values. 

\newpage 
## Q4
> Question: 3.19 Casella & Berger 

Show that: 

$$\int\limits_{x}^{\infty} \frac{1}{\Gamma(\alpha)} z^{\alpha-1} e^{-z} dz = \sum\limits_{y=0}^{\alpha-1} \frac{x^y e^{-x}}{y!}$$

For $\alpha = 1, 2, 3, ...$

Hint: Use integration by parts. Express this formula as a probabilistic relationship between Poisson and Gamma random variables. 

> Answer 

Via Integration by Parts, let us take out the constant $\frac{1}{\Gamma(\alpha)}$, and let $\alpha = n$, such that we have: 

$$u = z^{\alpha-1}$$

$$du = (\alpha - 1)z^{\alpha-2}$$

$$dv = e^{-z}dz$$

$$v = -e^{-z}$$

Thus, we have :

$$\int u dv = uv - \int vdu$$

Giving us: 


$$\int\limits_{z = x}^{\infty} z^{n-1}z^{-z} dz = x^{n-1} e^{-x} - \int\limits_{z = x}^{\infty} z^{n-2}(n-2)(-z^{-z}) dz$$

Simplifying and adding back in the $\frac{1}{\Gamma(n)}$ term gives us: 

$$\frac{1}{\Gamma(n)} \int\limits_{z = x}^{\infty} z^{n-1}z^{-z} dz = \frac{1}{\Gamma(n)} \big[ x^{n-1} e^{-x} + \int\limits_{z = x}^{\infty} (n-1)z^{n-2}z^{-z} dz\big]$$

We then note the expansion of $\Gamma(n) = (n-1) \Gamma (n-1) = (n-1)(n-2)\Gamma(n-2) = ... = (n-1)!\Gamma(1) = (n-1)! (1) = (n-1)!$. Using this relation above gives us: 

$$\frac{1}{\Gamma(n)} \int\limits_{z = x}^{\infty} z^{n-1}z^{-z} dz = \frac{x^{n-1} e^{-x}}{(n-1)!} + \int\limits_{z = x}^{\infty} \frac{1}{(n-1)\Gamma(n-1)} (n-1)z^{n-2}z^{-z} dz$$

$$\frac{1}{\Gamma(n)} \int\limits_{z = x}^{\infty} z^{n-1}z^{-z} dz = \frac{x^{n-1} e^{-x}}{(n-1)!} + \frac{1}{\Gamma(n-1)} \int\limits_{z = x}^{\infty} z^{n-2}z^{-z} dz$$

Notice the first term in this evaluation is $\frac{x^y e^{-x}}{y!}$ evaluated at $y = n-1$!

Notice also that we may continue the integration by parts for the term 

$$\frac{1}{\Gamma(n-1)} \int\limits_{z = x}^{\infty} z^{n-2}z^{-z} dz$$, 

which will yield two parts, the first being the $\frac{x^y e^{-x}}{y!}$ evaluated at $y = n-2$ and the second part being another integral we may break into parts. 

The above process may be repeated until we arrive at an integral in the $\int vdu = 0$

In summary, we can continue breaking the integral into parts which coincide with elements of the summation in reverse order ($\alpha - 1 \text{ or } n-1$), meaning these two values are equal. 

This is all to say that we have shown the relationship between the random variable $X \sim Poisson(x)$ and the random variable $Y \sim Gamma(\alpha, 1)$, as $P(X > x) = P(Y \leq \alpha - 1)$

\newpage 
## Q5
> Question: 3.24 (a), (c) Casella & Berger 
Note: You can skip the part about showing that the pdf is a pdf; also, in (c), the variance will not exist unless a>2.

Many "named" distributions are special cases of the more common distributions already discussed. For each of the following named distributions derive the form of the pdf, ..., and calculate the mean and variance. 

>> (a) 

If $X \sim Exponential(\beta)$, then $Y = X ^ {1/\gamma}$ has the Weibull$(\gamma, \beta)$ distribution, where $\gamma > 0$ is a constant.

>> (c) 

If $X \sim Gamma(a, b)$, then $Y = 1/X$ has the inverted Gamma IG(a,b) distribution. 

> Answer 
>> (a) 

$X \sim Exponential(\beta) \rightarrow f_X(x) = \frac{1}{\beta} e^{-x/\beta}$

Using the following general relation for two related random variables: 

Thm. 2.1.5: $$f_X(x) = f_z(g^-1(x)) | \frac{d}{dx} g^{-1}(x)|$$

We have: 

$$Y = X ^ {1/\gamma} \rightarrow f_Y(y) = f_x(y^\gamma)(d/dy) = \frac{1}{\beta} e^{-y^\gamma / \beta} \gamma y^{\gamma - 1}$$

For $x>0$, $y>0$

For simplicity, let us consider $EY^n = \int\limits_{y = 0}^{\infty} y^n f(y) = \int\limits_{y = 0}^{\infty} y^n \frac{1}{\beta} e^{-y^\gamma / \beta} \gamma y^{\gamma - 1}$

Simplifying terms somewhat, and removing terms that don't depend on y, we have: 

$$EY^n = \gamma / \beta \int\limits_{y = 0}^{\infty} y^{\gamma + n - 1} e^{-y^\gamma/ \beta} dy$$

Let $z = y^\gamma / \beta \rightarrow dz = \frac{\gamma}{\beta} y^{\gamma - 1}dy$
Such that $\frac{\beta}{\gamma y^{\gamma - 1}} dz = dy$

Change of variables allows us to rewrite the above as: 

$$EY^n = \frac{\gamma}{\beta} \frac{\beta}{\gamma} \beta^{\gamma + n - 1/ \gamma}  \int\limits_{z = 0}^{\infty} z^{\frac{\gamma + n - 1}{\gamma}} z^{\frac{1}{\gamma} - 1} e^{-z} dz$$ 
Simplifying terms somewhat gives us: 

$$EY^n = \beta^{\gamma + n - 1/ \gamma} \int\limits_{z = 0}^{\infty} z^{n/\gamma} e^{-z} dz$$ 

Recognizing this as the $\Gamma (n / \gamma + 1)$ function and simplifying constants, we thus have: 

$$EY^n = \beta ^{n / \gamma}\Gamma (n / \gamma + 1)$$

Using the above formula we have: 

$EY = \beta ^{1 / \gamma}\Gamma (1 / \gamma + 1)$

and

$EY^2 = \beta ^{2 / \gamma}\Gamma (2 / \gamma + 1)$

$$Var(Y) = EY^2 - (EY)^2 = \beta ^{2 / \gamma}\Gamma (2 / \gamma + 1) - (\beta ^{1 / \gamma}\Gamma (1 / \gamma + 1))^2 $$

$$Var(Y) = \beta^{2/ \gamma} [\Gamma (2 / \gamma + 1) - (\Gamma (1 / \gamma + 1))^2]$$

>> (c) 

$X \sim Gamma(a, b) \rightarrow f_X(x) = \frac{1}{\Gamma(a) b^a}x^{a - 1} e^{-x/b}$

With note of Thm. 2.1.5: $$f_X(x) = f_z(g^-1(x)) | \frac{d}{dx} g^{-1}(x)|$$

$Y = 1/X \rightarrow f_Y(y) = f_X(1/y)(d/dy) = \frac{1}{\Gamma(a) b^a}(1/y)^{a - 1} e^{-1/by}$

$EY = \int\limits_{y=0}^{\infty} y\frac{1}{\Gamma(a) b^a}(1/y)^{a - 1} e^{-1/by} dy$

$$EY = \frac{1}{\Gamma(a)b^a}\int\limits_{y=0}^{\infty} (\frac{1}{y})^a e^{-1/by} dy = \frac{\Gamma(a-1)b^{a-1}}{\Gamma(a)b^a} = \frac{1}{(a-1)b}$$

Note: $b \neq 0$ and $a \neq 1$ in order for the above to evaluate. 

$EY^2 = \int\limits_{y=0}^{\infty} y^2 \frac{1}{\Gamma(a) b^a}(1/y)^{a - 1} e^{-1/by} dy$

$$EY^2 = \frac{1}{\Gamma(a)b^a}\int\limits_{y=0}^{\infty} (\frac{1}{y})^{a-1} e^{-1/by} dy = \frac{\Gamma(a-2)b^{a-2}}{\Gamma(a)b^a} = \frac{1}{(a-1)(a-2)b^2}$$

$Var(Y) = EY^2 - (EY)^2 = \frac{1}{(a-1)(a-2)b^2} - (\frac{1}{(a-1)b})^2 = \frac{1}{(a-1)^2(a-2)b^2}$

Note: $Var(Y)$ does not exist unless $a > 2$ as variance must be positive. 

\newpage 
## Q6
> Question: 3.39 Casella & Berger 

Consider the Cauchy family defined in Section 3.3. This family can be extended to a location-scale family yielding pdfs of the form: 

$$f(x| \mu, \sigma) = \frac{1}{\sigma \pi ( 1 + (\frac{x - \mu}{\sigma})^2)}$$
For $-\infty < x < \infty$

The mean and variance do not exist for thje Cauchy distribution. So the parameters $\mu$, $\sigma^2$ are not the mean and variance. But they do have important meaning. Show that if X is a random variable with a Cauchy distribution with parameters $\mu$ and $\sigma$, then: 

>> (a) 

$\mu$ is the median of the distribution of X, that is, $P(X \geq \mu) = P(X \leq \mu) = \frac{1}{2}$

>> (b) 

$\mu + \sigma$ and $\mu - \sigma$ are the quartiles of the distribution of X, that is $P(X \geq \mu + \sigma) = P(X \leq \mu - \sigma) = \frac{1}{4}$

Hint: Prove this first for $\mu = 0$ and $sigma = 1$ and then use Exercise 3.38. 

Note: $\frac{d(arctanx)}{dx} = \frac{1}{1+x^2}$

> Answer 

>> (a) 

$\mu$ is the median if $$P(X \geq \mu) = P(X \leq \mu) = \frac{1}{2}$$

Let us consider the random Cauchy variable Z where $\mu=0$, $\sigma = 1$, 
$Z \sim Cauchy(\mu = 0, \sigma = 1)$

Let us consider $\mu = 0$, such that: $X = Z + \mu$

$P (Z \geq 0 ) = \int\limits_{z=0}^{\infty} \frac{1}{\pi} \frac{1}{1+z^2} dz = \frac{1}{\pi} arctan(z) \big|_{z=0}^{\infty} = \frac{1}{\pi}(\pi / 2 - 0) = \frac{1}{2}$

and 

$P (Z \leq 0 ) = \int\limits_{z=-\infty}^{0} \frac{1}{\pi} \frac{1}{1+z^2} dz = \frac{1}{\pi} arctan(z) \big|_{z=-\infty}^{0} = \frac{1}{\pi}(0 - (-\pi / 2)) = \frac{1}{2}$

We have shown then that for the random variable Z, $\mu$ is indeed the median. 

Taking advantage of the results of Exercise 3.38 we then have X belongs a location family of Z: 

So $P(Z \geq 0) = P( X \geq \mu) = \frac{1}{2}$

And 

So $P(Z \leq 0) = P( X \leq \mu) = \frac{1}{2}$

So the $mu$ is the median for the random variable X from the Cauchy family. 

>> (b) 

Let us consider the random Cauchy variable Z where $\mu=0$, $\sigma = 1$, such that: $X = \sigma Z + \mu$, $Z \sim Cauchy(\mu = 0, \sigma = 1)$

$$P(Z \geq \mu + \sigma) = P(Z \geq 1) = \int\limits_{z=1}^{\infty} \frac{1}{\pi} \frac{1}{1+z^2} dz  = \frac{1}{\pi}arctanz \big|_{z=1}^{\infty} = \frac{1}{\pi} (\pi /2 - \pi/4) = \frac{1}{4}$$

$$P(Z \leq \mu - \sigma) = P(Z \leq -1) = \int\limits_{z=-\infty}^{-1} \frac{1}{\pi} \frac{1}{1+z^2} dz  = \frac{1}{\pi}arctanz \big|_{z=-infty}^{-1} = \frac{1}{4}$$

Taking advantage of the results of Exercise 3.38 we then have X belongs to a scale-location family of Z and we may write: 

$P(Z \geq 1) = P(X \geq \mu + \sigma)$
$P(Z \leq - 1) = P(X \leq \mu - \sigma)$

So the following holds: 
$P(X \geq \mu + \sigma) = \frac{1}{4}$
$P(X \leq \mu - \sigma) = \frac{1}{4}$

Such that we have shown that $\mu + \sigma$ and $\mu - \sigma$ are the quartiles of the distribution of X. 

\newpage 
## Q7 
> Question: 

If $X \sim N(\mu, \sigma^2)$, find values of $\mu$ and $\sigma$ such that $P(|X|< 2) = \frac{1}{2}$. Prove or disprove that the values of $\mu$ and $\sigma$ are unique. 

> Answer 

Note: For any normally distributed random variable X, the standard normal Z may be derived by 

$$Z = \frac{X - \mu}{\sigma} \sim N(0, 1)$$

Furthermore, as  

$|X| < 2 \rightarrow -2 < X < 2$

Furthermore, note: 

$$X = \mu + \sigma Z \rightarrow Z = \frac{X - \mu}{\sigma}$$

Such that: 

$$P(|X| < 2) = P(-2 < X < 2) = P\left(\frac{-2 - \mu}{\sigma} < \frac{X - \mu}{\sigma} < \frac{2 - \mu}{\sigma}\right) = P\left(\frac{-2 - \mu}{\sigma} < Z < \frac{2 - \mu}{\sigma}\right)$$

$$P(|X| < 2) = P\left(Z < \frac{2 - \mu}{\sigma}\right) - P\left(Z < \frac{-2 - \mu}{\sigma} \right)$$

For the cdf of the standard normal $\Phi$, we have: 

$\Phi(\frac{2 - \mu}{\sigma}) - \Phi(\frac{-2 - \mu}{\sigma}) = \frac{1}{2}$

Note then that the above may be interpreted as a cdf of the normal distribution. Let us then denote the cdf of the standard normal distribution, $\Phi$, and proceed.

Taking advantage of the normal distribution being symmetrical, we have the following: 
$\Phi(z) = 1 - \Phi(-z)$

Such that: 

$$\Phi(\frac{-2 - \mu}{\sigma}) = 1 - \Phi(-(\frac{-2 - \mu}{\sigma})) = 1 - \Phi(\frac{2 + \mu}{\sigma})$$

Refering back to the above difference in cdfs, we then have: 

$\Phi(\frac{2 - \mu}{\sigma}) - (1 -  \Phi(\frac{2 + \mu}{\sigma})) = \frac{1}{2}$

$$\Phi(\frac{2 - \mu}{\sigma}) + \Phi(\frac{2 + \mu}{\sigma})) = \frac{3}{2}$$

# Alternativces 

By the definition of the cdf of the standard normal, 
For $$0 \leq \Phi(\frac{2 - \mu}{\sigma}) \leq 1$$, 
$$0 \leq \Phi(\frac{2 - \mu}{\sigma}) \leq 1$$, 

And from the above relation we have: 

$$\Phi(\frac{2 - \mu}{\sigma}) + \Phi(\frac{2 + \mu}{\sigma})) = \frac{3}{2}$$

# A Proof? 

Let us assume the above relations have no values of $\mu, \sigma$ that satisfy the relation. 

Then let $\mu = 0$, such that 

$$\Phi(\frac{2}{\sigma}) + \Phi(\frac{2}{\sigma})) = \frac{3}{2}$$

As the standard normal distribution is a monotonically increasing continuous function, then its inverse is defined, such that we have: 

$$2\Phi(\frac{2}{\sigma}) = \frac{3}{2} \rightarrow \Phi(\frac{2}{\sigma}) = \frac{3}{4}$$

And the following holds: 

$$\sigma = \frac{2}{\Phi^{-1}(\frac{3}{4})}  \approx 2.965$$

```{r}
qnorm(3/4)
```

As $\Phi^{-1}(\frac{3}{4}) \approx 0.6745$

So there does exist a solution for $\mu = 0$, $\approx 2.965$, so we have a contradiction. Thus we know that there exists at least one solution. 

Let us then assume there exists a unique soluition, $\mu = 0$, $\approx 2.965$, such that there are no other values which satisfy the given relation, $P(|X|< 2) = \frac{1}{2}$. 

Let us then assume that 

$$\Phi(\frac{2 - \mu}{\sigma}) = \Phi(\frac{2 + \mu}{\sigma})) + \frac{1}{8}$$
$$2\Phi(\frac{2 + \mu}{\sigma}))  + \frac{1}{8} = \frac{3}{2}$$

Giving us: 

$$\Phi(\frac{2 + \mu}{\sigma}))  + \frac{1}{8} = \frac{3}{4}$$

$$\Phi(\frac{2 + \mu}{\sigma}))  = \frac{5}{8}$$

By again taking advantage of the inverse cdf of the standard normal, we then have: 

$$\frac{2 + \mu}{\sigma}  = \Phi^{-1}(\frac{5}{8})$$
```{r}
qnorm(5/8)
```
Note: For this example we use 

$$\Phi(\frac{2 - \mu}{\sigma}) = \Phi(\frac{2 + \mu}{\sigma})) + \frac{1}{8}$$
and not 
$$\Phi(\frac{2 - \mu}{\sigma}) = \Phi(\frac{2 + \mu}{\sigma})) + \frac{1}{4}$$
To avoid potential divide by zero issues. Though this choice is dependent upon our constrains, the choice of $\frac{1}{8}$ is just to illustrate a second case. 

That being said: 

$$\mu = \sigma \Phi^{-1}(\frac{5}{8}) - 2 \approx \sigma (0.319) - 2$$

As $\sigma > 0$, let $\sigma = 1$, such that: 

$\mu = (0.319) - 2 = -1.681$, meaning we have found another pair of $(\mu, \sigma^2)$ to satisfy the relation. However, we assumed the values to satisfy $P(|X|< 2) = \frac{1}{2}$ and have arrived at a contradiction; we are left then to conclude that the values of $(\mu, \sigma^2)$ that satisfy $P(|X|< 2) = \frac{1}{2}$ are not unique. 

# A stroll and a frolick

Define $Z_\alpha$, such that $P (Z \geq Z_\alpha) = \alpha$, where $0 \leq \alpha \leq \frac{1}{2}$

Also define: 

$Z_{\alpha + \frac{1}{2}} \rightarrow P(Z \geq Z_{\alpha + \frac{1}{2}}) =  \alpha + \frac{1}{2}$

Note: The above are not cdfs. That being said, 

We have the following relations:

$$Z_{\alpha} = \frac{2- \mu}{\sigma}$$, 

$$Z_{\alpha + \frac{1}{2}} = \frac{- 2 - \mu}{\sigma}$$

The above gives us a system of equations that must hold for the desired property, $P(|X|< 2) = \frac{1}{2}$

Such that: 

$$Z_{\alpha} - Z_{\alpha + \frac{1}{2}} = \frac{2- \mu}{\sigma} - (\frac{- 2 - \mu}{\sigma}) = \frac{4}{\sigma}$$

Solving for $\sigma$ gives us: 

$\sigma = \frac{4}{Z_{\alpha} - Z_{\alpha + \frac{1}{2}}}$

The above gives us the form of $\sigma$, let us then define the form of $\mu$, 

$$Z_{\alpha} = \frac{2- \mu}{\sigma} \rightarrow z_{\alpha} \frac{4}{Z_{\alpha} - Z_{\alpha + \frac{1}{2}}} = 2 - \mu$$ 

$$\mu = 2 - \frac{4 z_{\alpha} }{Z_{\alpha} - Z_{\alpha + \frac{1}{2}}} = \frac{-2(z_{\alpha} + z_{\alpha + \frac{1}{2}})}{z_{\alpha} - z_{\alpha + \frac{1}{2}}}$$

Note: The above equations for $\sigma, \mu$ are valid as Z is continuous, such that $Z_\alpha, Z_{\alpha + \frac{1}{2}}$ exist and $Z_\alpha - Z_{\alpha + \frac{1}{2}} \neq 0$ by definition. 

On the subject of uniqueness: As we have a range of potential $\alpha$ values, as $0 \leq \alpha \leq \frac{1}{2}$, we have a number of possible solutions to the above system of equations (the values of $Z_\alpha - Z_{\alpha + \frac{1}{2}}$ that satisfy $P(|X|< 2) = \frac{1}{2}$). Thus the values of $(\mu, \sigma^2)$ are not unique.  