---
title: "HW8"
output: pdf_document
date: "2024-11-19"
---

# Outline 
  
  - Q1: DONE? Q on (c) 
  - Q2: DONE 
  - Q3: DONE 
  - Q4: DONE
  - Q5: WIP 
  - Q6: DONE 
  - Q7: DONE
  - Q8: DONE

# Q1 

Let $X_1$ and $X_2$ be independent exponential random variables with mean $\theta$.

## (a) 

Find the joint moment generating function of $X_1$ and $X_2$.

The single-variable MGF of:

$$
X \sim \text{Exponential}(\lambda)
$$ 

is given by: 

$$
M_X(t) = \frac{1}{1 - \lambda t}
$$

Where $t < \frac{1}{\lambda}$

For two independent (and identically distributed) exponential random variables $X_1$ and $X_2$, we may write their joint MGF as: 

$$
M_{X_1, X_2}(t_1, t_2) = M_{X_1}(t_1) M_{X_2}(t_2) = \frac{1}{(1 - \lambda t_1)(1 - \lambda t_2)}
$$

Where $t_1, t_2 < \frac{1}{\lambda}$

However, as $X_1 \text{ and } X_2$ are iid, then $t_1 = t_2 = t$ and we may simplify further, such that: 

$$
M_{X_1, X_2}(t_1, t_2) = \frac{1}{(1 - \lambda t)(1 - \lambda t)} = \frac{1}{(1-\lambda)}
$$

## (b) 

Give the definition of the moment generating function of $X_1 - X_2$ and show how this can be obtained from part (a).

Using the joint MGF, we have: 

$$
M_{X_1 - X_2}(t) = \frac{1}{(1 - \lambda t)(1 - \lambda(-t))} = \frac{1}{(1 - \lambda t)(1 + \lambda t)} = \frac{1}{1 - (\lambda t)^2}
$$

Where $|t| < \frac{1}{\lambda}$

## (c) 

Find the distribution of $Y = X_1 - X_2$. Using the mgf, one can find that this is a so-called Laplace or double-exponential distribution.

From the MGF found in part (b), we have:

$$
M_Y(t) = \frac{1}{1 - (\lambda t)^2}
$$

For $|t| < \frac{1}{\lambda}$. 

This MGF uniquely describes the distribution for the random variable Y defined by $Y = X_1 - X_2$ as the MGF is unique.

Using the MGF, we may then find the PDF of Y, 

As the MGF of Y follows a Laplace distribution with location parameter $\mu = 0$ and scale parameter $b = \lambda$, we know that the PDF of Y has the following form:

$$
f_Y(y) = \frac{1}{2\lambda} e^{\left(-\frac{|y|}{\lambda}\right)}
$$

Where $-\infty < y < \infty$. 

### Note 

To be frank, I was unsure what exactly was being asked for in part (c), as we found the MGF for Y in part (b) and know uniqueness of MGFs. So apologies in advance if any of the above is a bit of a hand wave. 

\newpage

# Q2: 4.30, Casella & Berger

Suppose the distribution of $Y$, conditional on $X = x$, is $N(x, x^2)$ and that the marginal distribution of $X$ is uniform $(0, 1)$.

## (a) 

Find $E[Y]$, $\text{Var}[Y]$, and $\text{Cov}(X, Y)$.

The law of total expectation states:

$$
E[Y] = E[E[Y | X]] = E[X] = \frac{1}{2}
$$
Consider then, $\text{Cov}(X, Y)$. We know: 

$$
\text{Var}(Y) = E[\text{Var}(Y | X)] + \text{Var}(E[Y | X]) = E[X^2] + \text{Var}(E[Y | X])
$$
And 

$$
E[\text{Var}(Y | X)] = E[X^2] = \int_0^1 x^2 \, dx = \frac{x^3}{3} \Big|_0^1 = \frac{1}{3}
$$

Also, we have: 

$$
\text{Var}(E[Y | X]) = \text{Var}(X) = \frac{1}{12}
$$

Again, with note that $X \sim \text{Uniform}(0, 1)$. 

Simplfying gives us: 

$$
\text{Var}(Y) = E[X^2] + \text{Var}(X) = \frac{1}{3} + \frac{1}{12} = \frac{5}{12}
$$

We then need to find $\text{Cov}(X, Y)$. To that end, we have: 

$$
E[XY] = E[X E[Y | X]] = E[X^2] = \frac{1}{3}
$$

We then have everything we need to calculate $\text{Cov}(X, Y)$, namely: 

$$
\text{Cov}(X, Y) = E[XY] - E[X]E[Y] = E[X^2] - E[X]E[Y] = \frac{1}{3} - \frac{1}{2}(\frac{1}{2}) = \frac{1}{12}
$$

## (b) 

Prove that $\frac{Y}{X}$ and $X$ are independent.

To prove that $\frac{Y}{X}$ and $X$ are independent, we need to show that the joint probability density function (PDF) of $(\frac{Y}{X}, X)$ can be written as the product of the marginal PDFs of $\frac{Y}{X}$ and $X$ (using a bivariate transformation). 

To that end let us start with the PDF of X, which we know given $X \sim \text{Uniform}(0, 1)$ is: 

$$
f_X(x) = 
\begin{cases} 
1 & \text{for } 0 < x < 1 \\ 
0 & \text{otherwise}
\end{cases}
$$

We then note the conditional distribution of $Y | X = x$ is $N(x, x^2)$, so the conditional PDF is:

$$
f_{Y | X}(y | x) = \frac{1}{\sqrt{2\pi x^2}} e^{\left(-\frac{(y - x)^2}{2x^2}\right)}
$$

for $-\infty < y < \infty$

Let us then consider the joint PDF of $X$ and $Y$:

$$
f_{X, Y}(x, y) = f_{Y | X}(y | x) f_X(x) = f_{Y | X}(y | x) (1)
$$

Because the pdf of X is 1, we may simplfy as: 

$$
f_{X, Y}(x, y) = 
\frac{1}{\sqrt{2\pi x^2}} e^{\left(-\frac{(y - x)^2}{2x^2}\right)}
$$

For $0 < x < 1 \text{ and } -\infty < y < \infty$

Let us then define the new random variable $Z = \frac{Y}{X}$. 

As, 

$$
Y = Z X
$$

The Jacobian of the transformation is:

$$
\begin{vmatrix}
\frac{\partial Y}{\partial Z} & \frac{\partial Y}{\partial X} \\
\frac{\partial X}{\partial Z} & \frac{\partial X}{\partial X}
\end{vmatrix}
=
\begin{vmatrix}
x & z \\
0 & 1
\end{vmatrix}
= x
$$

Using the above Jacobian, we then have the joint PDF of $(Z, X)$ may be written :

$$
f_{Z, X}(z, x) = f_{X, Y}(x, y) \cdot \left| J \right| = f_{X, Y}(x, z  x) | x | = f_{X, Y}(x, z  x) x 
$$

With note that X is always positive. 

We may further simplify the above joint pdf as: 

$$
f_{Z, X}(z, x) = \frac{1}{\sqrt{2\pi x^2}} e^{\left(-\frac{(z x - x)^2}{2x^2}\right)} x = \frac{x}{\sqrt{2\pi x^2}} e^{\left(-\frac{(x(z - 1))^2}{2x^2}\right)} = \frac{1}{\sqrt{2\pi}} e^{\left(-\frac{(z - 1)^2}{2}\right)} (1)
$$

Since $f_X(x) = 1$ for $0 < x < 1$, we have:

$$
f_{Z, X}(z, x) = \frac{1}{\sqrt{2\pi}} e^{\left(-\frac{(z - 1)^2}{2}\right)} f_X(x) = f_Z(z)f_X(x)
$$

And we conclude that $\frac{Y}{X}$ and $X$ are independent as we may write the joint PDF as the product of the marginal PDFs. 

\newpage

# Q3: 4.54, Casella & Berger

Find the pdf of $\prod_{i=1}^n X_i$, where the $X_i$'s are independent uniform $(0, 1)$ random variables.

*(Hint: Try to calculate the cdf, and remember the relationship between uniforms and exponentials.)*

Note: Each $X_i \sim \text{Uniform}(0, 1)$, and each has PDF:

$$
f_{X_i}(x) = 
\begin{cases} 
1 & \text{ for } 0 \leq x \leq 1 \\ 
0 & \text{otherwise}
\end{cases}
$$

Define a new random variable Y as:

$$
Y = \prod_{i=1}^n X_i
$$

As each $X_i$ has support [0,1], the support of Y is also [0, 1]. 

The PDF of $Y$ is then: 

$$
F_Y(y) = P(Y \leq y) = P\left(\prod_{i=1}^n X_i \leq y \right) = P\left( \sum_{i=1}^n - \ln(X_i) \geq - \ln(y )\right)
$$

Note that log transformation is a monotonic transformation. 

Let us then examine the distribution of $- ln(X_i)$:

As $X_i \sim \text{Uniform}(0, 1)$, $ln(X_i) \sim Exponential(1) \equiv Gamma (1, 1)$

Furthermore, we know that since $X_i \sim \text{Uniform}(0, 1) \forall i$, we know that $\sum_{i=1}^n - \ln(X_i) \sim \text{Gamma}(n, 1)$.

Turning then back to the CDF of Y, we may write: 

$$
F_Y(y) = \int\limits_{- ln(y)}^{\infty} \frac{1}{\Gamma(n)} z^{n-1}e^{-z}dz 
$$

To find the PDF of Y, we take the derivative of the CDF with respect to y, and write: 

$$
f_Y(y) = \frac{d}{dy} \int_{-ln(y)}^\infty \frac{1}{\Gamma(n)} z^{n-1}e^{-z}dz = -\frac{1}{\Gamma(n)}(-ln(y))^{n-1}e^{-\log y} \frac{d}{dy}(-ln(y))
$$

$$
F_Y(y) = \frac{1}{\Gamma(n)}(-ln(y))^{n-1} e^{-ln(y)} \left( \frac{1}{y} \right) = \frac{(-ln(y))^{n-1}}{\Gamma(n) y^2}
$$

For $0 < y < 1$ 

\newpage

# Q4: 4.47, Casella & Berger

*(Marginal normality does not imply bivariate normality.)* 

Let $X$ and $Y$ be independent $N(0,1)$ random variables, and define a new random variable $Z$ by

$$
Z = 
\begin{cases} 
X & \text{if } XY > 0, \\
-X & \text{if } XY < 0.
\end{cases}
$$

## (a) 

Show that $Z$ has a normal distribution, specifically $Z \sim N(0, 1)$.

As given from the definition of Z, Z takes either positive or negative values. However, there are two distinct cases for taking a positive or negative value: Z is positive if X and Y are both positive, or if X and Y are both negative. Similarly, Z is negative if X is positive and Y is negative, or if X  is negative and Y is positive. 

With this in mind we proceed as follows: 

### Positive Case

The probability that $XY > 0$ is the same as the probability that $X$ and $Y$ have the same sign. Due to independence and symmetry of the standard normal distribution:

$$
P(XY > 0) = P(X > 0, Y > 0) + P(X < 0, Y < 0) = P(X > 0)P(Y > 0) = \frac{1}{2}  (\frac{1}{2}) = \frac{1}{4}
$$

For the other positive case, X and Y both negative, we have

$$
P(X < 0, Y < 0) = \frac{1}{4}
$$

Taken together, we have: 

$$
P(XY > 0) = P(X < 0)P(Y < 0) + P(X < 0)P(Y < 0) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2}
$$

### The Negative Case 

The probability that $XY < 0$ is:

$$
P(XY < 0) = P(X > 0, Y < 0) + P(X < 0, Y > 0) = P(X > 0)P(Y < 0) = \frac{1}{4}
$$

For the other negative case, X negative and Y positive, we similarly have: 

$$
P(X < 0, Y > 0) = \frac{1}{4}
$$

Taken together then, 

$$
P(XY < 0) = P(X > 0)P(Y < 0) + P(X < 0, Y > 0) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2}
$$

### Conclusion 

From the above cases, we know there is symmetry to the distribution of Z, and that the "component" random variables of Z are standard normal distributions. 

Furthermore, we have $P(Z \leq z) = P(X \leq x)$, such that Z and X follow the same distribution. 

This is all to say that: 

$$
Z \sim N(0, 1)
$$

## (b) 

Show that the joint distribution of $Z$ and $Y$ is not bivariate normal. 

*(Hint: Show that $$Z$$ and $$Y$$ always have the same sign.)*

To show that the joint distribution of $Z$ and $Y$ is not bivariate normal, we must show that $Z$ and $Y$ always have the same sign, a case that does not occur for a bivariate normal distribution.

To that end, we recall the definition of the random variable Z: 

$$
Z =
\begin{cases}
X & \text{if } XY > 0, \\
-X & \text{if } XY < 0
\end{cases}
$$

Again, similar to part (a), there are two cases to consider: 

### Case 1 $XY > 0$:

If $XY > 0$, $X$ and $Y$ have the same sign. In this case, $Z = X$, and $Z$ retains the same sign as $X$, which matches the sign of $Y$.  

Therefore, $Z$ and $Y$ have the same sign.

### Case 2 $XY < 0$:

If $XY < 0$, $X$ and $Y$ have opposite signs. In this case, $Z = -X$, so $Z$ flips the sign of $X$. Since $X$ and $Y$ already have opposite signs, flipping the sign of $X$ ensures that $Z$ and $Y$ have the same sign. This is the same regardless of whether Y > 0 or Y < 0. 

### Conclusion 

In both cases above, we conclude that $Z$ and $Y$ always have the same sign, and the two cases exhaustively describe the possible values of Z and Y. This provides a constraint on the joint PDF of Z and Y (that it lies/has positive probability within the plane of Z and Y being the same sign and 0 otherwise). 

This is incompatible with a bivariate normal distribution, which allows for the two random variables composing it to carry opposite signs (a bivariate normal distribution has full support over $\mathbb{R}^2$. This is because a bivariate normal distribution has independence of signs (but not necessarily independence of the random variables), which the above provides evidence of being violated. 

\newpage

# Q5: 4.52, Casella & Berger 

Bullets are fired at the origin of an $(x, y)$ coordinate system, and the point hit, say $(X, Y)$, is a random variable. The variables $X$ and $Y$ are taken to be independent $N(0, 1)$ random variables. If two bullets are fired independently, what is the distribution of the distance between them?

The distance between the two points is given by the equation:

$$
R = \sqrt{(X_2 - X_1)^2 + (Y_2 - Y_1)^2}
$$

Our goal then is to find the distribution of $R$.

Since $X_1, X_2 \sim N(0, 1)$ are independent, the difference $X_2 - X_1$ is also normally distributed:

$$
X_2 - X_1 \sim N(0, 2)
$$

And similarly, $Y_2 - Y_1 \sim N(0, 2)$.

The squared distance is then the sum of two N(0, 2) random variables:

$$
R^2 = (X_2 - X_1)^2 + (Y_2 - Y_1)^2
$$

Let us define two new random variables as follows: 

Let $Z_1 = X_2 - X_1$, 
and 
Let $Z_2 = Y_2 - Y_1$. 

Then $Z_1, Z_2 \sim N(0, 2)$, and they are independent. The squared terms are:

$$
Z_1^2 \sim \text{Scaled-Chi-Square}(1, \sigma^2 = 2)
$$

and

$$
Z_2^2 \sim \text{Scaled-Chi-Square}(1, \sigma^2 = 2)
$$

For a standard normal variable $Z \sim N(0, 1)$, $Z^2 \sim \chi^2(1)$. 

Scaling by $\sigma^2 = 2$, $Z_1^2$ and $Z_2^2$ are scaled $\chi^2(1)$:

$$
Z_1^2 \sim 2 \chi^2(1)
$$

and

$$
Z_2^2 \sim 2 \chi^2(1)
$$

Since $Z_1^2 + Z_2^2$ is the sum of two independent, scaled $\chi^2(1)$ variables, it follows that:

$$
Z_1^2 + Z_2^2 \sim 2 \chi^2(2)
$$

We then note that a $\chi^2(2)$ distribution (Chi-squared distribution with 2 degrees of freedom) is equivalent to an $\text{Exponential}(1)$ distribution.

However, as we have a Scaled Chi-squared distribution, we similarly need to scale the Exponential distribution (by 2). When we do so, we have: 

$$
R^2 \sim \text{Exponential}\left(\frac{1}{2}\right)
$$

The random variable $R = \sqrt{R^2}$ is the square root of an $\text{Exponential}\left(\frac{1}{2}\right)$ random variable. 

The PDF of $R^2 \sim \text{Exponential}\left(\frac{1}{2}\right)$ is:

$$
f_{R^2}(r^2) = \frac{1}{2} e^{-r^2 / 2}, \quad r^2 \geq 0.
$$

To find the PDF of $R$, we apply the change of variables $R^2 = r^2$ with $R = \sqrt{r^2}$, giving us:

$$
f_R(r) = f_{R^2}(r^2) \left| \frac{d(r^2)}{dr} \right| = \frac{1}{2} e^{-r^2 / 2} 2r = r e^{-r^2 / 2}
$$

Where $r \geq 0$

Note: This is a specially named distribution! The distance $R$ between the two bullets follows a Rayleigh distribution with scale parameter $\sigma = \sqrt{2}$!

\newpage

# Q6: 4.55, Casella & Berger

A **parallel system** is one that functions as long as at least one component of it functions. 

A particular parallel system is composed of three independent components, each of which has a lifetime with an exponential $(\lambda)$ distribution. The lifetime of the system is the maximum of the individual lifetimes. 

What is the distribution of the lifetime of the system?

Let the lifetimes of the three components be represented by three random variables $X_1, X_2, X_3$, where each $X_i \sim \text{Exponential}(\lambda)$, and the lifetimes are independent. 

We may write the maximum of $X_1, X_2, X_3$ as a random variable. To that end we define the new random variable Y as follows: 

$$
Y = \max(X_1, X_2, X_3)
$$

The CDF of Y may be written as: 

$$
F_Y(y) = P(Y \leq y) = P(\max(X_1, X_2, X_3) \leq y) = P((X_1 \leq y) \cup X_2 \leq y \cup X_3 \leq y))
$$

As $X_1, X_2, X_3$ are iid, we have: 

$$
F_Y(y) = P(X_1 \leq y) P(X_2 \leq y) P(X_3 \leq y) = P(X \leq y) ^ 3
$$

For an exponential random variable $X \sim \text{Exponential}(\lambda)$, we know the CDF is given by:

$$
P(X \leq y) = 1 - e^{-y/\lambda}
$$

for $y > 0$

The CDF of Y may then be written: 

$$
F_Y(y) =  P(X \leq y)  ^ 3 = (1 - e^{-y/\lambda})^3
$$

To derive the PDF of Y, we differentiate the CDF with respect to y, giving us:

$$
f_Y(y)  = \frac{d}{dy} F_Y(y) = \frac{d}{dy} (1 - e^{-y/\lambda})^3 = 3 [1 - e^{-y\lambda}]^2 \frac{d}{dt} [1 - e^{- y/ \lambda}]
$$

Simplifying gives us: 

$$
f_Y(y)  = \frac{3}{\lambda} \left( 1 - e^{-y/\lambda} \right) ^2 e^{-y/ \lambda}
$$

for $y > 0$, and 0 otherwise. 

\newpage

# Q7: 4.28, Casella & Berger

Let $X$ and $Y$ be independent standard normal random variables.

## (a) 

Show that $\frac{X}{X + Y}$ has a Cauchy distribution.

Define a new random variable $Z = \frac{X}{X + Y}$.

This may also be written: 

$$
X = Z(X + Y)
$$

Furthermore

$$
X = ZX + ZY \Rightarrow \quad X(1 - Z) = ZY \Rightarrow \quad X = \frac{ZY}{1 - Z}
$$

Where $Z \neq 1$ (no dividing by zero). 

Given $X \sim N(0, 1)$ and $Y \sim N(0, 1)$ are independent, the joint PDF is equal to the product of the marginals:

$$
f_{X,Y}(x, y) = f_X(x)f_Y(y) = \frac{1}{2\pi} e^{-\frac{x^2 + y^2}{2}}
$$

Let us use a transformation, specifically:

$$
U = X + Y
$$

And: 

$$
V = Z = \frac{X}{X + Y}
$$

Note: This transformation is invertible, specifically:

$$
X = VU
$$

And

$$
Y = U - X = U - VU = U(1 - V)
$$

The Jacobian of this transformation is:

$$
\begin{vmatrix}
\frac{\partial X}{\partial U} & \frac{\partial X}{\partial V} \\[5pt]
\frac{\partial Y}{\partial U} & \frac{\partial Y}{\partial V}
\end{vmatrix}
=
\begin{vmatrix}
v & u \\[5pt]
1 - v & -u
\end{vmatrix}
= (-vu) - u(1 - v) = u
$$

We may then write the joint PDF of $U$ and $V$ is:

$$
f_{U,V}(u, v) = f_{X,Y}(vu, u(1-v)) |J| = f_{X,Y}(vu, u(1-v)) |u|
$$

Explicitly, this may be written: 

$$
f_{U,V}(u, v) = \frac{1}{2\pi} e^{\left(-\frac{(vu)^2 + (u(1 - v))^2}{2}\right)} |u| = \frac{1}{2\pi} e^{\left(-\frac{u^2(1 + v^2)}{2}\right)} |u|
$$

Now that we have the joint PDF of U and V, we may derive the marginal distribution of $V = Z$, by integration by u:

$$
f_V(v) = \int_{-\infty}^\infty f_{U,V}(u, v) du = \int_{-\infty}^\infty \frac{1}{2\pi} e^{\left(-\frac{u^2(1 + v^2)}{2}\right)} |u| du
$$

We simplify, with note of symmetry of the normal distribution: 

$$
f_V(v) = \int_{-\infty}^\infty \frac{1}{2\pi} e^{\left(-\frac{u^2(1 + v^2)}{2}\right)} |u| du = \int_{0}^\infty \frac{1}{\pi} e^{\left(-\frac{u^2(1 + v^2)}{2}\right)} u du = \frac{1}{\pi(1 + v^2)}
$$

The PDF $f_V(v) = \frac{1}{\pi(1 + v^2)}$ is the PDF of a standard Cauchy distribution. Therefore, $V \equiv Z \equiv \frac{X}{X + Y} \sim \text{Cauchy}(0,1)$, and $\frac{X}{X + Y}$ has a Cauchy distribution.

## (b) 

Find the distribution of $\frac{X}{|Y|}$.

Similar to part (a), let us define a new random variable Z, where $Z = \frac{X}{|Y|}$. 

Again, mirroring the procedure we did in part (a), we may derive the PDF of $Z$.

We know $X \sim N(0, 1)$ and $Y \sim N(0, 1)$. The joint PDF of $X$ and $Y$ is:

$$
f_{X,Y}(x, y) = \frac{1}{2\pi} e^{-\frac{x^2 + y^2}{2}}
$$

However, since $|Y| = |y|$, the PDF of $|Y|$ is given by:

$$
f_{|Y|}(y) = 
\begin{cases} 
\sqrt{\frac{2}{\pi}} e^{-\frac{y^2}{2}} & y \geq 0, \\
0 & y < 0
\end{cases}
$$

Thus, the joint PDF of $X$ and $|Y|$ is:

$$
f_{X,|Y|}(x, y) = 
\begin{cases} 
\frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}  \sqrt{\frac{2}{\pi}} e^{-\frac{y^2}{2}} & y \geq 0 \\
0 & y < 0
\end{cases}
$$

This simplifies to:

$$
f_{X,|Y|}(x, y) = 
\begin{cases} 
\frac{1}{\pi} e^{-\frac{x^2 + y^2}{2}} & y \geq 0 \\
0 & y < 0
\end{cases}
$$

Let us define another random variable W, where $W = |Y|$. We then have a transformation of X and Y using Z and W, specifically for: 

$$
X = ZW 
$$

$$
|Y| = W
$$

The Jacobian of this transformation is:

$$
\begin{vmatrix}
\frac{\partial X}{\partial Z} & \frac{\partial X}{\partial W} \\[5pt]
\frac{\partial |Y|}{\partial Z} & \frac{\partial |Y|}{\partial W}
\end{vmatrix}
=
\begin{vmatrix}
w & z \\[5pt]
0 & 1
\end{vmatrix}
= w
$$

Thus, the joint PDF of $Z$ and $W$ is:

$$
f_{Z,W}(z, w) = f_{X,|Y|}(zw, w) |w| = \frac{1}{\pi} e^{-\frac{(zw)^2 + w^2}{2}} w
$$

As we now have the joint distribution of Z and W, we may derive the marginal PDF of $Z$ by integrating with respect to $W$:

$$
f_Z(z) = \int_0^\infty f_{Z,W}(z, w) dw = \int_0^\infty \frac{1}{\pi} e^{-\frac{(zw)^2 + w^2}{2}} w dw 
$$

Evaluating the above requires (at least for me) a change variables. 

Let $u = \frac{w^2(z^2 + 1)}{2}$, such that $w^2 = \frac{2u}{z^2 + 1}$ and $dw = \frac{du}{w(z^2 + 1)}$. 

Then we may rewrite the above as: 

$$
f_Z(z) = \int_0^\infty \frac{1}{\pi} e^{-u} \frac{1}{z^2 + 1} du = \frac{1}{\pi(z^2 + 1)}
$$

The PDF of $Z = \frac{X}{|Y|}$ is the standard Cauchy distribution. Therefore:

$$
Z \equiv \frac{X}{|Y|} \sim \text{Cauchy}(0, 1)
$$


## (c) 

Is the answer to part (b) surprising? Can you formulate a general theorem?

I was not surprised that the answer to part (b) is a Cauchy distribution, but I was surprised to learn that both part (a) and part (b) are distributed the same and by the standard Cauchy distribution. 

We may be able to draw some insights and formulate a general theorem though. To that end:

The result in parts (a) and (b) can be unified under a broader theorem:

Start by noting the location-scale family of the Cauchy distribution. 

Furthermore, let X and Y be independent $N(0, \sigma^2)$ random variables. 

Define $Z = \frac{X}{aY + b}$, where $a, b \in \mathbb{R}$ and $a \neq 0$, to avoid dividing by zero. 

Then:

$$
Z \sim \text{Cauchy}\left(0, \frac{\sigma}{|a|}\right).
$$

### Some Cases to aid in the above generalization

#### Case 1: results from part (a) 

When a = 1 and b = 0, $Z = \frac{X}{Y}$, and the result is a standard Cauchy distribution:

$$
Z \sim \text{Cauchy}(0, \sigma)
$$

And for $\sigma^2 = 1$, 

$$
Z \sim \text{Cauchy}(0, 1)
$$

#### Case 2: 

When $Z = \frac{X}{|Y|}$, we use the symmetry of Y given its Normally distributed. The absolute value does not alter the result since the distribution depends on the magnitude of Y, not its sign. Thus:
   
$$
Z \sim \text{Cauchy}(0, \sigma)
$$

And for $\sigma^2 = 1$, 

$$
Z \sim \text{Cauchy}(0, 1)
$$

#### Case 3: Scaled and shifted

When $a \neq 1$ or $b \neq 0$, the location parameter is adjusted by the shift in the denominator, but the scale depends only on a.

#### Some properties of note: 

The theorem can be explained through the invariance of the ratio of independent Gaussian variables under scaling and translation (the rules/properties found individually for the location-scale family of Gaussian random variables):

The numerator, random variable X, introduces symmetry in the numerator.

The denominator, a linear combination of the random variable Y, aY + b, shifts and scales the variable but does not change the overall form of the distribution due to the independence of X and Y.

\newpage

# Q8: 4.50, Casella & Berger

If $(X, Y)$ has the bivariate normasl probability density function (pdf):

$$
f(x, y) = \frac{1}{2\pi(1-\rho^2)^{1/2}} \exp \left( -\frac{1}{2(1-\rho^2)} \left( x^2 - 2\rho xy + y^2 \right) \right),
$$

show that 

$$
\text{Corr}(X, Y) = \rho
$$ 

and 

$$
\text{Corr}(X^2, Y^2) = \rho^2.
$$ 

*Hint:* Conditional expectations will simplify calculations.

The correlation between $X$ and $Y$ is:

$$
\text{Corr}(X, Y) = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X) \text{Var}(Y)}}
$$

Given $(X,Y)$ follows a bivariate normal distribution, we know: 

(1): $X \sim N(0, 1)$, such that $E[X] = 0$ and $\text{Var}(X) = 1$, and 

(2): $Y \sim N(0, 1)$, such that $E[Y] = 0$ and $\text{Var}(Y) = 1$.

Thus:

$$
\text{Corr}(X, Y) = \frac{\text{Cov}(X, Y)}{\sqrt{1(1)}} = \text{Cov}(X, Y)
$$

The covariance for a bivariate normal random variable $(X, Y)$ is given by the parameter $\rho$:

$$
\text{Cov}(X, Y) = E[XY] - E[X]E[Y] = \rho
$$

Furthermore as $E[X]E[Y] = 0$, we also have: 
$$
\text{Corr}(X, Y) = \text{Cov}(X, Y) = E[XY] = \rho
$$

Then, for $X^2, X^2$, we have the correlation is given by:

$$
\text{Corr}(X^2, Y^2) = \frac{\text{Cov}(X^2, Y^2)}{\sqrt{\text{Var}(X^2) \text{Var}(Y^2)}}.
$$

Using conditional expectations, we consider $Y | X = x$, specifically:

$$
Y | X = x \sim N(\rho x, 1 - \rho^2)
$$

We then have: 

$$
E[X^2 Y^2] = E[X^2 \cdot E[Y^2 | X]]
$$

However, we know that $E[Y^2 | X]$ is given by: 

$$
E[Y^2 | X] = \text{Var}(Y | X) + (E[Y | X])^2
$$

Where:

(1): $\text{Var}(Y | X) = 1 - \rho^2$, and 

(2): $E[Y | X] = \rho X$

Thus:

$$
E[Y^2 | X] = (1 - \rho^2) + (\rho X)^2 = 1 - \rho^2 + \rho^2 X^2
$$

Such that we may simplify the initial formula for $E[X^2 Y^2]$:  

$$
E[X^2 Y^2] = E[X^2 (1 - \rho^2 + \rho^2 X^2)] = E[X^2] (1 - \rho^2) + \rho^2 E[X^4]
$$

Let us then recall that $X \sim N(0, 1)$, such that we know the following is true: 

(1): $E[X^2] = 1$, and 

(2): $E[X^4] = 3$ 

Thus we may simplify our above equation and write:

$$
E[X^2 Y^2] = (1)(1 - \rho^2) + \rho^2(3) = 1 - \rho^2 + 3\rho^2 = 1 + 2\rho^2
$$

Returning then to the covariance equation, we have:

$$
\text{Cov}(X^2, Y^2) = E[X^2 Y^2] - E[X^2]E[Y^2] = (1 + 2\rho^2) - (1)(1) = 2\rho^2
$$

We are then left with deriving $\text{Var}(X^2)$ and $\text{Var}(Y^2)$. To that end: 

The variance of $X^2$ is:

$$
\text{Var}(X^2) = E[X^4] - (E[X^2])^2 = 3 - (1)^2 = 2
$$

And as $X$ and $Y$ have the same distribution, we know then that: 

$$
\text{Var}(Y^2) = \text{Var}(X^2) = 2
$$

We may finally write our goal and evaluate $\text{Corr}(X^2, Y^2)$: 

$$
\text{Corr}(X^2, Y^2) = \frac{\text{Cov}(X^2, Y^2)}{\sqrt{\text{Var}(X^2) \text{Var}(Y^2)}} = \frac{2\rho^2}{\sqrt{2 (2)}} = \frac{2\rho^2}{2} = \rho^2
$$
