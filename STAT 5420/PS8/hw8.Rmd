---
title: "HW8"
output: pdf_document
date: "2024-11-19"
---

# Outline 
  
  - Q1: DONE? Q on (c) 
  - Q2: DONE 
  - Q3: WIP 
  - Q4: DRAFT
  - Q5: DRAFT 
  - Q6: DONE 
  - Q7: DRAFT
  - Q8: DONE

# Q1 

Let $X_1$ and $X_2$ be independent exponential random variables with mean $\theta$.

## (a) 

Find the joint moment generating function of $X_1$ and $X_2$.

The single-variable MGF of:

$$
X \sim \text{Exponential}(\lambda)
$$ 

is given by: 

$$
M_X(t) = \frac{1}{1 - \lambda t}
$$

Where $t < \frac{1}{\lambda}$

For two independent (and identically distributed) exponential random variables $X_1$ and $X_2$, we may write their joint MGF as: 

$$
M_{X_1, X_2}(t_1, t_2) = M_{X_1}(t_1) M_{X_2}(t_2) = \frac{1}{(1 - \lambda t_1)(1 - \lambda t_2)}
$$

Where $t_1, t_2 < \frac{1}{\lambda}$

However, as $X_1 \text{ and } X_2$ are iid, then $t_1 = t_2 = t$ and we may simplify further, such that: 

$$
M_{X_1, X_2}(t_1, t_2) = \frac{1}{(1 - \lambda t)(1 - \lambda t)} = \frac{1}{(1-\lambda)}
$$

## (b) 

Give the definition of the moment generating function of $X_1 - X_2$ and show how this can be obtained from part (a).

Using the joint MGF, we have: 

$$
M_{X_1 - X_2}(t) = \frac{1}{(1 - \lambda t)(1 - \lambda(-t))} = \frac{1}{(1 - \lambda t)(1 + \lambda t)} = \frac{1}{1 - (\lambda t)^2}
$$

Where $|t| < \frac{1}{\lambda}$

## (c) 

Find the distribution of $Y = X_1 - X_2$. Using the mgf, one can find that this is a so-called Laplace or double-exponential distribution.

From the MGF found in part (b), we have:

$$
M_Y(t) = \frac{1}{1 - (\lambda t)^2}
$$

For $|t| < \frac{1}{\lambda}$. 

This MGF uniquely describes the distribution for the random variable Y defined by $Y = X_1 - X_2$ as the MGF is unique.

Using the MGF, we may then find the PDF of Y, 

As the MGF of Y follows a Laplace distribution with location parameter $\mu = 0$ and scale parameter $b = \lambda$, we know that the PDF of Y has the following form:

$$
f_Y(y) = \frac{1}{2\lambda} e^{\left(-\frac{|y|}{\lambda}\right)}
$$

Where $-\infty < y < \infty$. 

### Note 

To be frank, I was unsure what exactly was being asked for in part (c), as we found the MGF for Y in part (b) and know uniqueness of MGFs. So apologies in advance if any of the above is a bit of a hand wave. 

\newpage

# Q2: 4.30, Casella & Berger

Suppose the distribution of $Y$, conditional on $X = x$, is $N(x, x^2)$ and that the marginal distribution of $X$ is uniform $(0, 1)$.

## (a) 

Find $E[Y]$, $\text{Var}[Y]$, and $\text{Cov}(X, Y)$.

The law of total expectation states:

$$
E[Y] = E[E[Y | X]] = E[X] = \frac{1}{2}
$$
Consider then, $\text{Cov}(X, Y)$. We know: 

$$
\text{Var}(Y) = E[\text{Var}(Y | X)] + \text{Var}(E[Y | X]) = E[X^2] + \text{Var}(E[Y | X])
$$
And 

$$
E[\text{Var}(Y | X)] = E[X^2] = \int_0^1 x^2 \, dx = \frac{x^3}{3} \Big|_0^1 = \frac{1}{3}
$$

Also, we have: 

$$
\text{Var}(E[Y | X]) = \text{Var}(X) = \frac{1}{12}
$$

Again, with note that $X \sim \text{Uniform}(0, 1)$. 

Simplfying gives us: 

$$
\text{Var}(Y) = E[X^2] + \text{Var}(X) = \frac{1}{3} + \frac{1}{12} = \frac{5}{12}
$$

We then need to find $\text{Cov}(X, Y)$. To that end, we have: 

$$
E[XY] = E[X E[Y | X]] = E[X^2] = \frac{1}{3}
$$

We then have everything we need to calculate $\text{Cov}(X, Y)$, namely: 

$$
\text{Cov}(X, Y) = E[XY] - E[X]E[Y] = E[X^2] - E[X]E[Y] = \frac{1}{3} - \frac{1}{2}(\frac{1}{2}) = \frac{1}{12}
$$

## (b) 

Prove that $\frac{Y}{X}$ and $X$ are independent.

To prove that $\frac{Y}{X}$ and $X$ are independent, we need to show that the joint probability density function (PDF) of $(\frac{Y}{X}, X)$ can be written as the product of the marginal PDFs of $\frac{Y}{X}$ and $X$ (using a bivariate transformation). 

To that end let us start with the PDF of X, which we know given $X \sim \text{Uniform}(0, 1)$ is: 

$$
f_X(x) = 
\begin{cases} 
1 & \text{for } 0 < x < 1 \\ 
0 & \text{otherwise}
\end{cases}
$$

We then note the conditional distribution of $Y | X = x$ is $N(x, x^2)$, so the conditional PDF is:

$$
f_{Y | X}(y | x) = \frac{1}{\sqrt{2\pi x^2}} e^{\left(-\frac{(y - x)^2}{2x^2}\right)}
$$

for $-\infty < y < \infty$

Let us then consider the joint PDF of $X$ and $Y$:

$$
f_{X, Y}(x, y) = f_{Y | X}(y | x) f_X(x) = f_{Y | X}(y | x) (1)
$$

Because the pdf of X is 1, we may simplfy as: 

$$
f_{X, Y}(x, y) = 
\frac{1}{\sqrt{2\pi x^2}} e^{\left(-\frac{(y - x)^2}{2x^2}\right)}
$$

For $0 < x < 1 \text{ and } -\infty < y < \infty$

Let us then define the new random variable $Z = \frac{Y}{X}$. 

As, 

$$
Y = Z X
$$

The Jacobian of the transformation is:

$$
\begin{vmatrix}
\frac{\partial Y}{\partial Z} & \frac{\partial Y}{\partial X} \\
\frac{\partial X}{\partial Z} & \frac{\partial X}{\partial X}
\end{vmatrix}
=
\begin{vmatrix}
x & z \\
0 & 1
\end{vmatrix}
= x
$$

Using the above Jacobian, we then have the joint PDF of $(Z, X)$ may be written :

$$
f_{Z, X}(z, x) = f_{X, Y}(x, y) \cdot \left| J \right| = f_{X, Y}(x, z  x) | x | = f_{X, Y}(x, z  x) x 
$$

With note that X is always positive. 

We may further simplify the above joint pdf as: 

$$
f_{Z, X}(z, x) = \frac{1}{\sqrt{2\pi x^2}} e^{\left(-\frac{(z x - x)^2}{2x^2}\right)} x = \frac{x}{\sqrt{2\pi x^2}} e^{\left(-\frac{(x(z - 1))^2}{2x^2}\right)} = \frac{1}{\sqrt{2\pi}} e^{\left(-\frac{(z - 1)^2}{2}\right)} (1)
$$

Since $f_X(x) = 1$ for $0 < x < 1$, we have:

$$
f_{Z, X}(z, x) = \frac{1}{\sqrt{2\pi}} e^{\left(-\frac{(z - 1)^2}{2}\right)} f_X(x) = f_Z(z)f_X(x)
$$

And we conclude that $\frac{Y}{X}$ and $X$ are independent as we may write the joint PDF as the product of the marginal PDFs. 

\newpage

# Q3: 4.54, Casella & Berger

Find the pdf of $\prod_{i=1}^n X_i$, where the $X_i$'s are independent uniform $(0, 1)$ random variables.

*(Hint: Try to calculate the cdf, and remember the relationship between uniforms and exponentials.)*

Note: Each $X_i \sim \text{Uniform}(0, 1)$, and each has PDF:

$$
f_{X_i}(x) = 
\begin{cases} 
1 & \text{ for } 0 \leq x \leq 1 \\ 
0 & \text{otherwise}
\end{cases}
$$

Define a new random variable W as:

$$
W = \prod_{i=1}^n X_i
$$

As each $X_i$ has support [0,1], the support of W is also [0, 1]. 

The PDF of $W$ is then: 

$$
F_W(w) = P(W \leq w) = P\left(\prod_{i=1}^n X_i \leq w\right) = P\left(\sum_{i=1}^n \ln(X_i) \leq \ln(w)\right)
$$

Note that log transformation is a monotonic transformation. 

Let us then define the random variables $Y_i$ as follows:

$$
Y_i = -\ln(X_i)
$$ 

As $X_i \sim \text{Uniform}(0, 1)$, $Y_i \sim Exponential(1)$

Let us then define the random variable S as the sum of $Y_i$. We similarly know the distribution of S as $S \sim \text{Gamma}(n, 1)$.

Specifically the PDF of $S$ is:

$$
f_S(s) = \frac{s^{n-1} e^{-s}}{\Gamma(n)}
$$

For $s > 0$

Thus:

$$
F_W(w) = P\left(\sum_{i=1}^n Y_i \leq -\ln(w)\right) = P(S \leq -\ln(w))
$$

The CDF of $W$ is:

$$
F_W(w) = \int_0^{-\ln(w)} \frac{s^{n-1} e^{-s}}{\Gamma(n)} \, ds
$$

For $0 < w \leq 1$

The PDF of $W$ is the derivative of the CDF:

$$
f_W(w) = \frac{d}{dw} \left[ \int_0^{-\ln(w)} \frac{s^{n-1} e^{-s}}{\Gamma(n)} \, ds \right].
$$

$$
f_W(w) = \frac{1}{w} \left( \frac{(-\ln(w))^{n-1} e^{-\ln(w)}}{\Gamma(n)} \right) = \frac{(-\ln(w))^{n-1}}{\Gamma(n) w^{n}}
$$

The PDF of $W = \prod_{i=1}^n X_i$ is:

$$
f_W(w) = \frac{(-\ln(w))^{n-1}}{\Gamma(n) w^n} \quad 0 < w \leq 1.
$$

\newpage

# Q4: 4.47, Casella & Berger

*(Marginal normality does not imply bivariate normality.)* 

Let $X$ and $Y$ be independent $N(0,1)$ random variables, and define a new random variable $Z$ by

$$
Z = 
\begin{cases} 
X & \text{if } XY > 0, \\
-X & \text{if } XY < 0.
\end{cases}
$$

## (a) 

Show that $Z$ has a normal distribution, specifically $Z \sim N(0, 1)$.

The condition $XY > 0$ corresponds to $X$ and $Y$ having the same sign:
- $X > 0, Y > 0$,
- $X < 0, Y < 0$.

The condition $XY < 0$ corresponds to $X$ and $Y$ having opposite signs:
- $X > 0, Y < 0$,
- $X < 0, Y > 0$.

Thus, $Z = X$ when $X$ and $Y$ have the same sign, and $Z = -X$ when $X$ and $Y$ have opposite signs.

This occurs when $XY > 0$, which happens in two subcases:
1. $X > 0, Y > 0$,
2. $X < 0, Y < 0$.

The probability that $XY > 0$ is the same as the probability that $X$ and $Y$ have the same sign. Due to independence and symmetry of the standard normal distribution:

$$
P(XY > 0) = P(X > 0, Y > 0) + P(X < 0, Y < 0).
$$

By independence:

$$
P(X > 0, Y > 0) = P(X > 0)P(Y > 0) = \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4},
$$

and similarly:

$$
P(X < 0, Y < 0) = \frac{1}{4}.
$$

Thus:

$$
P(XY > 0) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2}.
$$

This occurs when $XY < 0$, which happens in two subcases:
1. $X > 0, Y < 0$,
2. $X < 0, Y > 0$.

The probability that $XY < 0$ is:

$$
P(XY < 0) = P(X > 0, Y < 0) + P(X < 0, Y > 0).
$$

Similarly:

$$
P(X > 0, Y < 0) = P(X > 0)P(Y < 0) = \frac{1}{4},
$$

and:

$$
P(X < 0, Y > 0) = \frac{1}{4}.
$$

Thus:

$$
P(XY < 0) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2}.
$$

From the definition of $Z$, we see that:
- With probability $\frac{1}{2}$, $Z = X$,
- With probability $\frac{1}{2}$, $Z = -X$.

Thus, $Z$ is distributed as $X$ with equal probability of flipping the sign. The symmetry of the normal distribution ensures that flipping the sign does not alter the distribution. Therefore:

$$
Z \sim N(0, 1).
$$

We have shown that $Z$ has the same distribution as $X$, which is $N(0, 1)$. Thus:

$$
Z \sim N(0, 1).
$$

## (b) 

Show that the joint distribution of $Z$ and $Y$ is not bivariate normal. *(Hint: Show that $$Z$$ and $$Y$$ always have the same sign.)*

To show that the joint distribution of $Z$ and $Y$ is not bivariate normal, we analyze their dependence and demonstrate that $Z$ and $Y$ always have the same sign, violating a property of bivariate normal distributions.

From the definition of $Z$:
$$
Z =
\begin{cases}
X & \text{if } XY > 0, \\
-X & \text{if } XY < 0.
\end{cases}
$$

- **Case 1: When $XY > 0$:**  
  If $XY > 0$, $X$ and $Y$ have the same sign. In this case, $Z = X$, and $Z$ retains the same sign as $X$, which matches the sign of $Y$.  
  Therefore, $Z$ and $Y$ have the same sign.

- **Case 2: When $XY < 0$:**  
  If $XY < 0$, $X$ and $Y$ have opposite signs. In this case, $Z = -X$, so $Z$ flips the sign of $X$. Since $X$ and $Y$ already have opposite signs, flipping the sign of $X$ ensures that $Z$ and $Y$ have the same sign.

In both cases, we conclude:
$$
\text{$Z$ and $Y$ always have the same sign.}
$$

For two random variables $Z$ and $Y$ to follow a bivariate normal distribution:
- Any linear combination of $Z$ and $Y$ must also have a normal distribution.
- The joint PDF of $(Z, Y)$ would not impose deterministic constraints on their values.

However, the fact that $Z$ and $Y$ always share the same sign imposes a **deterministic relationship**:
- If $Z > 0$, then $Y > 0$.  
- If $Z < 0$, then $Y < 0$.  

This deterministic relationship violates the independence and symmetry properties expected of bivariate normal variables. Specifically:
- In a bivariate normal distribution, the joint distribution must allow the variables to take any values within their range, subject only to their covariance structure.
- The joint restriction that $Z$ and $Y$ must have the same sign introduces a nonlinear dependence, which is incompatible with bivariate normality.

The support of the joint distribution of $(Z, Y)$ is constrained to:

$$
\{(z, y): z > 0 \text{ and } y > 0\} \cup \{(z, y): z < 0 \text{ and } y < 0\}.
$$

This means that the joint density of $(Z, Y)$ has **zero probability** in the regions where $Z$ and $Y$ have opposite signs, such as:
- $Z > 0$ and $Y < 0$,
- $Z < 0$ and $Y > 0$.

This behavior is incompatible with a bivariate normal distribution, whose joint density would assign nonzero probability to all regions in \(\mathbb{R}^2\).

The joint distribution of $Z$ and $Y$ is not bivariate normal because:
- $Z$ and $Y$ always have the same sign, which imposes a deterministic nonlinear constraint on their values.  
- This behavior violates the symmetry and independence properties required for a bivariate normal distribution.

\newpage

# Q5: 4.52, Casella & Berger 

Bullets are fired at the origin of an $(x, y)$ coordinate system, and the point hit, say $(X, Y)$, is a random variable. The variables $X$ and $Y$ are taken to be independent $N(0, 1)$ random variables. If two bullets are fired independently, what is the distribution of the distance between them?

Two bullets are fired independently at points $(X_1, Y_1)$ and $(X_2, Y_2)$, where $X_1, Y_1, X_2, Y_2 \sim N(0, 1)$ are independent standard normal random variables. The distance between the two points is given by:

$$
R = \sqrt{(X_2 - X_1)^2 + (Y_2 - Y_1)^2}.
$$

We aim to find the distribution of $R$.

Since $X_1, X_2 \sim N(0, 1)$ are independent, the difference $X_2 - X_1$ is also normally distributed:

$$
X_2 - X_1 \sim N(0, 2).
$$

Similarly, $Y_2 - Y_1 \sim N(0, 2)$.

The squared distance is:

$$
R^2 = (X_2 - X_1)^2 + (Y_2 - Y_1)^2.
$$

Let $Z_1 = X_2 - X_1$ and $Z_2 = Y_2 - Y_1$. Then $Z_1, Z_2 \sim N(0, 2)$, and they are independent. The squared terms are:

$$
Z_1^2 \sim \text{Scaled-Chi-Square}(1, \sigma^2 = 2), \quad Z_2^2 \sim \text{Scaled-Chi-Square}(1, \sigma^2 = 2).
$$

For a standard normal variable $Z \sim N(0, 1)$, $Z^2 \sim \chi^2(1)$. Scaling by $\sigma^2 = 2$, $Z_1^2$ and $Z_2^2$ are scaled $\chi^2(1)$:

$$
Z_1^2 \sim 2 \cdot \chi^2(1), \quad Z_2^2 \sim 2 \cdot \chi^2(1).
$$

Since $Z_1^2 + Z_2^2$ is the sum of two independent scaled $\chi^2(1)$ variables, it follows that:

$$
Z_1^2 + Z_2^2 \sim 2 \cdot \chi^2(2).
$$

A $\chi^2(2)$ distribution is equivalent to an $\text{Exponential}(1)$ distribution. Scaling by 2, we have:

$$
R^2 \sim \text{Exponential}\left(\frac{1}{2}\right).
$$

The random variable $R = \sqrt{R^2}$ is the square root of an $\text{Exponential}\left(\frac{1}{2}\right)$ random variable. The PDF of $R^2 \sim \text{Exponential}\left(\frac{1}{2}\right)$ is:

$$
f_{R^2}(r^2) = \frac{1}{2} e^{-r^2 / 2}, \quad r^2 \geq 0.
$$

To find the PDF of $R$, we apply the change of variables $R^2 = r^2$ with $R = \sqrt{r^2}$, giving:

$$
f_R(r) = f_{R^2}(r^2) \cdot \left| \frac{d(r^2)}{dr} \right| = \frac{1}{2} e^{-r^2 / 2} \cdot 2r = r e^{-r^2 / 2}, \quad r \geq 0.
$$

The distance $R$ between the two bullets follows a **Rayleigh distribution** with scale parameter $\sigma = \sqrt{2}$:

$$
f_R(r) = r e^{-r^2 / 2}, \quad r \geq 0.
$$

\newpage

# Q6: 4.55, Casella & Berger

A **parallel system** is one that functions as long as at least one component of it functions. 

A particular parallel system is composed of three independent components, each of which has a lifetime with an exponential $(\lambda)$ distribution. The lifetime of the system is the maximum of the individual lifetimes. 

What is the distribution of the lifetime of the system?

Let the lifetimes of the three components be represented by three random variables $X_1, X_2, X_3$, where each $X_i \sim \text{Exponential}(\lambda)$, and the lifetimes are independent. 

We may write the maximum of $X_1, X_2, X_3$ as a random variable. To that end we define the new random variable Y as follows: 

$$
Y = \max(X_1, X_2, X_3)
$$

The CDF of Y may be written as: 

$$
F_Y(y) = P(Y \leq y) = P(\max(X_1, X_2, X_3) \leq y) = P((X_1 \leq y) \cup X_2 \leq y \cup X_3 \leq y))
$$

As $X_1, X_2, X_3$ are iid, we have: 

$$
F_Y(y) = P(X_1 \leq y) P(X_2 \leq y) P(X_3 \leq y) = P(X \leq y) ^ 3
$$

For an exponential random variable $X \sim \text{Exponential}(\lambda)$, we know the CDF is given by:

$$
P(X \leq y) = 1 - e^{-y/\lambda}
$$

for $y > 0$

The CDF of Y may then be written: 

$$
F_Y(y) =  P(X \leq y)  ^ 3 = (1 - e^{-y/\lambda})^3
$$

To derive the PDF of Y, we differentiate the CDF with respect to y, giving us:

$$
f_Y(y)  = \frac{d}{dy} F_Y(y) = \frac{d}{dy} (1 - e^{-y/\lambda})^3 = 3 [1 - e^{-y\lambda}]^2 \frac{d}{dt} [1 - e^{- y/ \lambda}]
$$

Simplifying gives us: 

$$
f_Y(y)  = \frac{3}{\lambda} \left( 1 - e^{-y/\lambda} \right) ^2 e^{-y/ \lambda}
$$

for $y > 0$, and 0 otherwise. 

\newpage

# Q7: 4.28, Casella & Berger

Let $X$ and $Y$ be independent standard normal random variables.

## (a) 

Show that $\frac{X}{X + Y}$ has a Cauchy distribution.

To show that $\frac{X}{X + Y}$ has a Cauchy distribution, let's proceed as follows:

Let $Z = \frac{X}{X + Y}$. To study the distribution of $Z$, rewrite it as:

$$
Z = \frac{X}{X + Y}.
$$

This can be rearranged to express $X$ in terms of $Z$ and $Y$:

$$
X = Z(X + Y).
$$

Expanding this gives:

$$
X = ZX + ZY \quad \Rightarrow \quad X(1 - Z) = ZY \quad \Rightarrow \quad X = \frac{ZY}{1 - Z},
$$

where $Z \neq 1$ to avoid division by zero.

Since $X \sim N(0, 1)$ and $Y \sim N(0, 1)$ are independent, the joint probability density function (PDF) is:

$$
f_{X,Y}(x, y) = f_X(x)f_Y(y) = \frac{1}{2\pi} e^{-\frac{x^2 + y^2}{2}}.
$$

Let:

$$
U = X + Y, \quad V = Z = \frac{X}{X + Y}.
$$

This transformation is invertible. The inverse is:

$$
X = VU, \quad Y = U - X = U - VU = U(1 - V).
$$

The Jacobian of the transformation is:

$$
\begin{vmatrix}
\frac{\partial X}{\partial U} & \frac{\partial X}{\partial V} \\[5pt]
\frac{\partial Y}{\partial U} & \frac{\partial Y}{\partial V}
\end{vmatrix}
=
\begin{vmatrix}
V & U \\[5pt]
1 - V & -U
\end{vmatrix}
= (-VU) - U(1 - V) = U.
$$

The absolute value of the Jacobian determinant is $|U|$.

The joint PDF of $U$ and $V$ is:

$$
f_{U,V}(u, v) = f_{X,Y}(x, y) \cdot |J|,
$$

where $(X, Y)$ are expressed in terms of $(U, V)$:

$$
f_{U,V}(u, v) = \frac{1}{2\pi} \exp\left(-\frac{(vu)^2 + (u(1 - v))^2}{2}\right) |u|.
$$

Simplifying the exponent:

$$
(vu)^2 + (u(1 - v))^2 = u^2(v^2 + (1 - v)^2) = u^2(v^2 + 1 - 2v + v^2) = u^2(1 + v^2).
$$

Thus:

$$
f_{U,V}(u, v) = \frac{1}{2\pi} \exp\left(-\frac{u^2(1 + v^2)}{2}\right) |u|.
$$

To find the marginal distribution of $V = Z$, integrate out $u$:

$$
f_V(v) = \int_{-\infty}^\infty f_{U,V}(u, v) \, du.
$$

Substitute $f_{U,V}(u, v)$:

$$
f_V(v) = \int_{-\infty}^\infty \frac{1}{2\pi} \exp\left(-\frac{u^2(1 + v^2)}{2}\right) |u| \, du.
$$

Factorize the integral using the symmetry of the normal distribution and recognize a Gaussian integral:

$$
\int_{-\infty}^\infty \exp\left(-\frac{u^2(1 + v^2)}{2}\right) |u| \, du = \frac{1}{1 + v^2}.
$$

Thus:

$$
f_V(v) = \frac{1}{\pi(1 + v^2)}.
$$

The PDF $f_V(v) = \frac{1}{\pi(1 + v^2)}$ is the PDF of a standard Cauchy distribution. Therefore, $\frac{X}{X + Y}$ has a Cauchy distribution.

## (b) 

Find the distribution of $\frac{X}{|Y|}$.

To find the distribution of $\frac{X}{|Y|}$, where $X$ and $Y$ are independent standard normal random variables, let's proceed step by step.

Let $Z = \frac{X}{|Y|}$. To determine the distribution of $Z$, we calculate its PDF.

We know $X \sim N(0, 1)$ and $Y \sim N(0, 1)$. The joint PDF of $X$ and $Y$ is:

$$
f_{X,Y}(x, y) = \frac{1}{2\pi} e^{-\frac{x^2 + y^2}{2}}.
$$

Since $|Y| = |y|$, the PDF of $|Y|$ is given by:

$$
f_{|Y|}(y) = 
\begin{cases} 
\sqrt{\frac{2}{\pi}} e^{-\frac{y^2}{2}}, & y \geq 0, \\
0, & y < 0.
\end{cases}
$$

Thus, the joint PDF of $X$ and $|Y|$ is:

$$
f_{X,|Y|}(x, y) = 
\begin{cases} 
\frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \cdot \sqrt{\frac{2}{\pi}} e^{-\frac{y^2}{2}}, & y \geq 0, \\
0, & y < 0.
\end{cases}
$$

For $y \geq 0$, this simplifies to:

$$
f_{X,|Y|}(x, y) = \frac{1}{\pi} e^{-\frac{x^2 + y^2}{2}}.
$$

Let $Z = \frac{X}{|Y|}$ and $W = |Y|$. Then:
$$
X = ZW \quad \text{and} \quad |Y| = W.
$$

The Jacobian of this transformation is:

$$
\begin{vmatrix}
\frac{\partial X}{\partial Z} & \frac{\partial X}{\partial W} \\[5pt]
\frac{\partial |Y|}{\partial Z} & \frac{\partial |Y|}{\partial W}
\end{vmatrix}
=
\begin{vmatrix}
W & Z \\[5pt]
0 & 1
\end{vmatrix}
= W.
$$

Thus, the joint PDF of $Z$ and $W$ is:

$$
f_{Z,W}(z, w) = f_{X,|Y|}(zw, w) \cdot |W| = \frac{1}{\pi} e^{-\frac{(zw)^2 + w^2}{2}} \cdot w.
$$

The marginal PDF of $Z$ is obtained by integrating out $W$:

$$
f_Z(z) = \int_0^\infty f_{Z,W}(z, w) \, dw.
$$

Substitute $f_{Z,W}(z, w)$:
$$
f_Z(z) = \int_0^\infty \frac{1}{\pi} e^{-\frac{(zw)^2 + w^2}{2}} w \, dw.
$$

Factorize the exponent:

$$
(zw)^2 + w^2 = w^2(z^2 + 1),
$$
so:
$$
f_Z(z) = \int_0^\infty \frac{1}{\pi} e^{-\frac{w^2(z^2 + 1)}{2}} w \, dw.
$$

Change variables to simplify the integral. Let $u = \frac{w^2(z^2 + 1)}{2}$, so $w^2 = \frac{2u}{z^2 + 1}$ and $dw = \frac{du}{w(z^2 + 1)}$. When $w = 0$, $u = 0$; as $w \to \infty$, $u \to \infty$. Substituting, we get:
$$
f_Z(z) = \int_0^\infty \frac{1}{\pi} e^{-u} \cdot \frac{1}{z^2 + 1} \, du.
$$

The integral of $e^{-u}$ from $0$ to $\infty$ is $1$, so:
$$
f_Z(z) = \frac{1}{\pi(z^2 + 1)}.
$$

The PDF of $Z = \frac{X}{|Y|}$ is:
$$
f_Z(z) = \frac{1}{\pi(1 + z^2)}.
$$

This is the standard Cauchy distribution. Therefore:

$$
\frac{X}{|Y|} \sim \text{Cauchy}(0, 1).
$$


## (c) 

Is the answer to part (b) surprising? Can you formulate a general theorem?

At first glance, the result that \(\frac{X}{|Y|}\) has a standard Cauchy distribution might seem surprising because \(|Y|\) (the absolute value of a standard normal random variable) is strictly positive and not symmetrically distributed like \(X\). Despite this, the independence of \(X\) and \(|Y|\) and the particular relationship between their distributions result in a cancellation of dependencies, yielding the familiar Cauchy distribution. This is a remarkable property of the ratio of independent normal and scaled distributions, arising due to specific symmetry properties.

The result in parts (a) and (b) can be unified under a broader theorem:

Let \(X\) and \(Y\) be independent \(N(0, \sigma^2)\) random variables. Define \(R = \frac{X}{aY + b}\), where \(a, b \in \mathbb{R}\) and \(a \neq 0\). Then:

$$
R \sim \text{Cauchy}\left(0, \frac{\sigma}{|a|}\right).
$$

1. **Case 1 (Part (b)):**
   When \(a = 1\) and \(b = 0\), \(R = \frac{X}{Y}\), and the result is a standard Cauchy distribution:
   $$
   R \sim \text{Cauchy}(0, \sigma).
   $$

2. **Case 2 (Part (b), absolute value modification):**
   When \(R = \frac{X}{|Y|}\), we use the symmetry of \(Y\). The absolute value does not alter the result since the distribution depends on the magnitude of \(Y\), not its sign. Thus:
   $$
   R \sim \text{Cauchy}(0, \sigma).
   $$

3. **Case 3 (Scaled and shifted denominator):**
   When \(a \neq 1\) or \(b \neq 0\), the location parameter is adjusted by the shift in the denominator, but the scale depends only on \(a\).

The theorem can be explained through the invariance of the ratio of independent Gaussian variables under scaling and translation:

- The numerator \(X\) introduces symmetry in the numerator.
- The denominator \(aY + b\) shifts and scales the variable but does not change the overall form of the distribution due to the independence of \(X\) and \(Y\).

This property of the ratio of independent Gaussians is tied to the definition of the Cauchy distribution, which arises naturally in this context.

\newpage

# Q8: 4.50, Casella & Berger

If $(X, Y)$ has the bivariate normal probability density function (pdf):

$$
f(x, y) = \frac{1}{2\pi(1-\rho^2)^{1/2}} \exp \left( -\frac{1}{2(1-\rho^2)} \left( x^2 - 2\rho xy + y^2 \right) \right),
$$

show that 

$$
\text{Corr}(X, Y) = \rho
$$ 

and 

$$
\text{Corr}(X^2, Y^2) = \rho^2.
$$ 

*Hint:* Conditional expectations will simplify calculations.

The correlation between $X$ and $Y$ is:

$$
\text{Corr}(X, Y) = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X) \text{Var}(Y)}}
$$

Given $(X,Y)$ follows a bivariate normal distribution, we know: 

(1): $X \sim N(0, 1)$, such that $E[X] = 0$ and $\text{Var}(X) = 1$, and 

(2): $Y \sim N(0, 1)$, such that $E[Y] = 0$ and $\text{Var}(Y) = 1$.

Thus:

$$
\text{Corr}(X, Y) = \frac{\text{Cov}(X, Y)}{\sqrt{1(1)}} = \text{Cov}(X, Y)
$$

The covariance for a bivariate normal random variable $(X, Y)$ is given by the parameter $\rho$:

$$
\text{Cov}(X, Y) = E[XY] - E[X]E[Y] = \rho
$$

Furthermore as $E[X]E[Y] = 0$, we also have: 
$$
\text{Corr}(X, Y) = \text{Cov}(X, Y) = E[XY] = \rho
$$

Then, for $X^2, X^2$, we have the correlation is given by:

$$
\text{Corr}(X^2, Y^2) = \frac{\text{Cov}(X^2, Y^2)}{\sqrt{\text{Var}(X^2) \text{Var}(Y^2)}}.
$$

Using conditional expectations, we consider $Y | X = x$, specifically:

$$
Y | X = x \sim N(\rho x, 1 - \rho^2)
$$

We then have: 

$$
E[X^2 Y^2] = E[X^2 \cdot E[Y^2 | X]]
$$

However, we know that $E[Y^2 | X]$ is given by: 

$$
E[Y^2 | X] = \text{Var}(Y | X) + (E[Y | X])^2
$$

Where:

(1): $\text{Var}(Y | X) = 1 - \rho^2$, and 

(2): $E[Y | X] = \rho X$

Thus:

$$
E[Y^2 | X] = (1 - \rho^2) + (\rho X)^2 = 1 - \rho^2 + \rho^2 X^2
$$

Such that we may simplify the initial formula for $E[X^2 Y^2]$:  

$$
E[X^2 Y^2] = E[X^2 (1 - \rho^2 + \rho^2 X^2)] = E[X^2] (1 - \rho^2) + \rho^2 E[X^4]
$$

Let us then recall that $X \sim N(0, 1)$, such that we know the following is true: 

(1): $E[X^2] = 1$, and 

(2): $E[X^4] = 3$ 

Thus we may simplify our above equation and write:

$$
E[X^2 Y^2] = (1)(1 - \rho^2) + \rho^2(3) = 1 - \rho^2 + 3\rho^2 = 1 + 2\rho^2
$$

Returning then to the covariance equation, we have:

$$
\text{Cov}(X^2, Y^2) = E[X^2 Y^2] - E[X^2]E[Y^2] = (1 + 2\rho^2) - (1)(1) = 2\rho^2
$$

We are then left with deriving $\text{Var}(X^2)$ and $\text{Var}(Y^2)$. To that end: 

The variance of $X^2$ is:

$$
\text{Var}(X^2) = E[X^4] - (E[X^2])^2 = 3 - (1)^2 = 2
$$

And as $X$ and $Y$ have the same distribution, we know then that: 

$$
\text{Var}(Y^2) = \text{Var}(X^2) = 2
$$

We may finally write our goal and evaluate $\text{Corr}(X^2, Y^2)$: 

$$
\text{Corr}(X^2, Y^2) = \frac{\text{Cov}(X^2, Y^2)}{\sqrt{\text{Var}(X^2) \text{Var}(Y^2)}} = \frac{2\rho^2}{\sqrt{2 (2)}} = \frac{2\rho^2}{2} = \rho^2
$$
