---
title: "HW8"
output: pdf_document
date: "2024-11-19"
---

# Outline 
  
  - Q1: DRAFT
  - Q2: DRAFT 
  - Q3: DRAFT 
  - Q4: DRAFT
  - Q5: DRAFT 
  - Q6: DRAFT 
  - Q7: DRAFT
  - Q8: DRAFT

# Q1 

Let $X_1$ and $X_2$ be independent exponential random variables with mean $\theta$.

## (a) 

Find the joint moment generating function of $X_1$ and $X_2$.

We start with the single variable MGF: 

$$
M_X(t) = \mathbb{E}[e^{tX}]
$$

For an $\text{Exponential}(\theta)$ random variable, the mean is $\theta$, and the rate parameter $\lambda = 1/\theta$. The MGF of $X \sim \text{Exponential}(\lambda)$ is:

$$
M_X(t) = \frac{\lambda}{\lambda - t}, \quad t < \lambda.
$$

Since $X_1$ and $X_2$ are independent exponential random variables, the joint MGF is the product of the individual MGFs:

$$
M_{X_1, X_2}(t_1, t_2) = \mathbb{E}[e^{t_1 X_1 + t_2 X_2}]
$$

Using independence:

$$
M_{X_1, X_2}(t_1, t_2) = \mathbb{E}[e^{t_1 X_1}] \cdot \mathbb{E}[e^{t_2 X_2}] = M_{X_1}(t_1) \cdot M_{X_2}(t_2).
$$

Each $M_{X_i}(t)$ has the same form as the MGF of an exponential random variable. Substituting $\lambda = 1/\theta$, we get:

$$
M_{X_1}(t_1) = \frac{\frac{1}{\theta}}{\frac{1}{\theta} - t_1} = \frac{1}{1 - \theta t_1}, \quad t_1 < \frac{1}{\theta}.
$$

$$
M_{X_2}(t_2) = \frac{\frac{1}{\theta}}{\frac{1}{\theta} - t_2} = \frac{1}{1 - \theta t_2}, \quad t_2 < \frac{1}{\theta}.
$$

Thus, the joint MGF is:

$$
M_{X_1, X_2}(t_1, t_2) = \frac{1}{(1 - \theta t_1)} \cdot \frac{1}{(1 - \theta t_2)} = \frac{1}{(1 - \theta t_1)(1 - \theta t_2)}, \quad t_1, t_2 < \frac{1}{\theta}.
$$

## (b) 

Give the definition of the moment generating function of $X_1 - X_2$ and show how this can be obtained from part (a).

We start with the MGF of a single variable: 

$$
M_X(t) = \mathbb{E}[e^{tX}].
$$

For the random variable $X_1 - X_2$, the MGF is given by:

$$
M_{X_1 - X_2}(t) = \mathbb{E}[e^{t(X_1 - X_2)}].
$$

From part (a), the joint MGF of $X_1$ and $X_2$ is:

$$
M_{X_1, X_2}(t_1, t_2) = \frac{1}{(1 - \theta t_1)(1 - \theta t_2)}, \quad t_1, t_2 < \frac{1}{\theta}.
$$

To find the MGF of $X_1 - X_2$, substitute $t_1 = t$ and $t_2 = -t$ into the joint MGF, because $t(X_1 - X_2) = tX_1 - tX_2$:

$$
M_{X_1 - X_2}(t) = M_{X_1, X_2}(t, -t).
$$

Substituting into the expression for the joint MGF:

$$
M_{X_1 - X_2}(t) = \frac{1}{(1 - \theta t)(1 - \theta(-t))}.
$$

Simplify the denominator:

$$
M_{X_1 - X_2}(t) = \frac{1}{(1 - \theta t)(1 + \theta t)}.
$$

Expand the product in the denominator:

$$
M_{X_1 - X_2}(t) = \frac{1}{1 - (\theta t)^2}, \quad |t| < \frac{1}{\theta}.
$$

The moment generating function of $X_1 - X_2$ is:

$$
M_{X_1 - X_2}(t) = \frac{1}{1 - (\theta t)^2}, \quad |t| < \frac{1}{\theta}.
$$

## (c) 

Find the distribution of $Y = X_1 - X_2$. Using the mgf, one can find that this is a so-called Laplace or double-exponential distribution.

To find the distribution of $ Y = X_1 - X_2 $, we use the moment generating function (MGF) obtained in part (b):

$$
M_Y(t) = \frac{1}{1 - (\theta t)^2}, \quad |t| < \frac{1}{\theta}.
$$

The MGF of a Laplace (double-exponential) random variable $Y$ with location parameter $\mu$ and scale parameter $b$ is:

$$
M_Y(t) = \frac{1}{1 - b^2 t^2}, \quad |t| < \frac{1}{b}.
$$

By comparing this with the MGF derived above, we identify that $b = \theta$ and $\mu = 0$. Therefore, $Y $follows a **Laplace distribution** with location parameter $\mu = 0$ and scale parameter $b = \theta$.

The probability density function of a Laplace random variable $Y$ with parameters $\mu = 0$ and $b = \theta$ is:

$$
f_Y(y) = \frac{1}{2\theta} \exp\left(-\frac{|y|}{\theta}\right), \quad y \in \mathbb{R}.
$$

Thus, the distribution of $Y = X_1 - X_2$ is:

$$
f_Y(y) = \frac{1}{2\theta} \exp\left(-\frac{|y|}{\theta}\right), \quad y \in \mathbb{R}.
$$

This result confirms that the difference of two independent exponential random variables (with the same mean) follows a Laplace distribution centered at 0, with scale parameter equal to the mean of the exponential distribution. This distribution is often called a **double-exponential distribution** because it has exponential decay in both positive and negative directions.

\newpage

# Q2: 4.30, Casella & Berger

Suppose the distribution of $Y$, conditional on $X = x$, is $N(x, x^2)$ and that the marginal distribution of $X$ is uniform $(0, 1)$.

## (a) 

Find $E[Y]$, $\text{Var}[Y]$, and $\text{Cov}(X, Y)$.

To find $E[Y]$, $\text{Var}[Y]$, and $\text{Cov}(X, Y)$, we use the law of total expectation and total variance.

We are given that $Y | X = x \sim N(x, x^2)$, so:
- The conditional mean is $\mathbb{E}[Y | X = x] = x$.
- The conditional variance is $\text{Var}(Y | X = x) = x^2$.

The law of total expectation states:

$$
\mathbb{E}[Y] = \mathbb{E}[\mathbb{E}[Y | X]].
$$

From the conditional mean, $\mathbb{E}[Y | X = x] = x$. Thus:

$$
\mathbb{E}[Y] = \mathbb{E}[X].
$$

Since $X \sim \text{Uniform}(0, 1)$, we know:

$$
\mathbb{E}[X] = \frac{1}{2}.
$$

Therefore:

$$
\mathbb{E}[Y] = \frac{1}{2}.
$$

The law of total variance states:

$$
\text{Var}(Y) = \mathbb{E}[\text{Var}(Y | X)] + \text{Var}(\mathbb{E}[Y | X]).
$$

- From the conditional variance, $\text{Var}(Y | X = x) = x^2$. Thus:

$$
\mathbb{E}[\text{Var}(Y | X)] = \mathbb{E}[x^2].
$$

Since $X \sim \text{Uniform}(0, 1)$, $\mathbb{E}[x^2]$ is the second moment of a uniform distribution:

$$
\mathbb{E}[x^2] = \int_0^1 x^2 \, dx = \frac{x^3}{3} \Big|_0^1 = \frac{1}{3}.
$$

- From the conditional mean, $\mathbb{E}[Y | X = x] = x$, so:

$$
\text{Var}(\mathbb{E}[Y | X]) = \text{Var}(X).
$$

For $X \sim \text{Uniform}(0, 1)$, $\text{Var}(X)$ is:

$$
\text{Var}(X) = \frac{1}{12}.
$$

Substitute these results into the law of total variance:

$$
\text{Var}(Y) = \mathbb{E}[x^2] + \text{Var}(X) = \frac{1}{3} + \frac{1}{12}.
$$

Simplify:

$$
\text{Var}(Y) = \frac{4}{12} + \frac{1}{12} = \frac{5}{12}.
$$

The covariance is given by:

$$
\text{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y].
$$

From $\mathbb{E}[Y | X = x] = x$, we have:

$$
\mathbb{E}[XY] = \mathbb{E}[X \cdot \mathbb{E}[Y | X]] = \mathbb{E}[X^2].
$$

For $X \sim \text{Uniform}(0, 1)$, $\mathbb{E}[X^2] = \frac{1}{3}$. Also, we know $\mathbb{E}[X] = \frac{1}{2}$ and $\mathbb{E}[Y] = \frac{1}{2}$. Thus:

$$
\text{Cov}(X, Y) = \mathbb{E}[X^2] - \mathbb{E}[X]\mathbb{E}[Y] = \frac{1}{3} - \frac{1}{2} \cdot \frac{1}{2}.
$$

Simplify:

$$
\text{Cov}(X, Y) = \frac{1}{3} - \frac{1}{4} = \frac{4}{12} - \frac{3}{12} = \frac{1}{12}.
$$
In Summary: 

1. $\mathbb{E}[Y] = \frac{1}{2}$,
2. $\text{Var}(Y) = \frac{5}{12}$,
3. $\text{Cov}(X, Y) = \frac{1}{12}$.

## (b) 

Prove that $\frac{Y}{X}$ and $X$ are independent.

To prove that $\frac{Y}{X}$ and $X$ are independent, we need to show that the joint probability density function (PDF) of $(\frac{Y}{X}, X)$ can be written as the product of the marginal PDFs of $\frac{Y}{X}$ and $X$.

The marginal distribution of $X$ is uniform $(0, 1)$, so its PDF is:

$$
f_X(x) = 
\begin{cases} 
1, & 0 < x < 1, \\ 
0, & \text{otherwise}.
\end{cases}
$$

The conditional distribution of $Y | X = x$ is $N(x, x^2)$, so the conditional PDF is:

$$
f_{Y | X}(y | x) = \frac{1}{\sqrt{2\pi x^2}} \exp\left(-\frac{(y - x)^2}{2x^2}\right), \quad -\infty < y < \infty.
$$

The joint PDF of $X$ and $Y$ is:

$$
f_{X, Y}(x, y) = f_{Y | X}(y | x) f_X(x).
$$

Substituting $f_X(x) = 1$ for $0 < x < 1$:

$$
f_{X, Y}(x, y) = 
\begin{cases} 
\frac{1}{\sqrt{2\pi x^2}} \exp\left(-\frac{(y - x)^2}{2x^2}\right), & 0 < x < 1, \, -\infty < y < \infty, \\ 
0, & \text{otherwise}.
\end{cases}
$$

Define $Z = \frac{Y}{X}$ and $X = X$. Then:

$$
Y = Z \cdot X.
$$

The Jacobian of the transformation is:

$$
\begin{vmatrix}
\frac{\partial Y}{\partial Z} & \frac{\partial Y}{\partial X} \\
\frac{\partial X}{\partial Z} & \frac{\partial X}{\partial X}
\end{vmatrix}
=
\begin{vmatrix}
X & Z \\
0 & 1
\end{vmatrix}
= X.
$$

Thus, the joint PDF of $(Z, X)$ is:

$$
f_{Z, X}(z, x) = f_{X, Y}(x, y) \cdot \left| \text{Jacobian} \right| = f_{X, Y}(x, z \cdot x) \cdot x.
$$

Substitute $Y = z \cdot x$ into $f_{X, Y}(x, y)$:

$$
f_{X, Y}(x, y) = \frac{1}{\sqrt{2\pi x^2}} \exp\left(-\frac{(y - x)^2}{2x^2}\right),
$$

so:

$$
f_{Z, X}(z, x) = \frac{1}{\sqrt{2\pi x^2}} \exp\left(-\frac{(z \cdot x - x)^2}{2x^2}\right) \cdot x.
$$

Simplify $z \cdot x - x = x(z - 1)$:

$$
f_{Z, X}(z, x) = \frac{x}{\sqrt{2\pi x^2}} \exp\left(-\frac{(x(z - 1))^2}{2x^2}\right).
$$

Cancel $x$ terms in the denominator:

$$
f_{Z, X}(z, x) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(z - 1)^2}{2}\right) \cdot f_X(x).
$$

Since $f_X(x) = 1$ for $0 < x < 1$, we have:

$$
f_{Z, X}(z, x) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(z - 1)^2}{2}\right) \cdot 1.
$$

- The marginal PDF of $Z = \frac{Y}{X}$ is:

$$
f_Z(z) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(z - 1)^2}{2}\right), \quad -\infty < z < \infty.
$$

- The marginal PDF of $X$ is:

$$
f_X(x) = 1, \quad 0 < x < 1.
$$

The joint PDF $f_{Z, X}(z, x)$ factors as:

$$
f_{Z, X}(z, x) = f_Z(z) \cdot f_X(x).
$$

Since the joint PDF is the product of the marginal PDFs, $Z = \frac{Y}{X}$ and $X$ are independent.

$\frac{Y}{X}$ and $X$ are independent.

\newpage

# Q3: 4.54, Casella & Berger

Find the pdf of $\prod_{i=1}^n X_i$, where the $X_i$'s are independent uniform $(0, 1)$ random variables.

*(Hint: Try to calculate the cdf, and remember the relationship between uniforms and exponentials.)*

To find the PDF of $W = \prod_{i=1}^n X_i$, where the $X_i$'s are independent $\text{Uniform}(0, 1)$ random variables, we proceed as follows:

Each $X_i \sim \text{Uniform}(0, 1)$, meaning:

$$
f_{X_i}(x) = 
\begin{cases} 
1, & 0 \leq x \leq 1, \\ 
0, & \text{otherwise}.
\end{cases}
$$

We define $W = \prod_{i=1}^n X_i$, which maps the product of the $X_i$'s into a single random variable $W$ on $(0, 1)$. 

To find the PDF of $W$, we first compute its CDF, $F_W(w) = P(W \leq w)$, and then differentiate.

$$
F_W(w) = P(W \leq w) = P\left(\prod_{i=1}^n X_i \leq w\right).
$$

Take the logarithm on both sides (recall that $\ln$ is monotonic and $\ln(w) < 0$ for $w \in (0, 1)$):

$$
P\left(\prod_{i=1}^n X_i \leq w\right) = P\left(\sum_{i=1}^n \ln(X_i) \leq \ln(w)\right).
$$

Let $Y_i = -\ln(X_i)$. Each $Y_i$ is an **Exponential(1)** random variable because if $X_i \sim \text{Uniform}(0, 1)$, then $-\ln(X_i) \sim \text{Exponential}(1)$. Thus, the sum $S = \sum_{i=1}^n Y_i$ follows a **Gamma distribution** with shape parameter $n$ and rate parameter $1$, denoted $S \sim \text{Gamma}(n, 1)$. 

The PDF of $S$ is:

$$
f_S(s) = \frac{s^{n-1} e^{-s}}{\Gamma(n)}, \quad s > 0.
$$

Thus:

$$
F_W(w) = P\left(\sum_{i=1}^n Y_i \leq -\ln(w)\right) = P(S \leq -\ln(w)).
$$

The CDF of $W$ is:

$$
F_W(w) = \int_0^{-\ln(w)} \frac{s^{n-1} e^{-s}}{\Gamma(n)} \, ds, \quad 0 < w \leq 1.
$$

The PDF of $W$ is the derivative of the CDF:

$$
f_W(w) = \frac{d}{dw} \left[ \int_0^{-\ln(w)} \frac{s^{n-1} e^{-s}}{\Gamma(n)} \, ds \right].
$$

Using the fundamental theorem of calculus and the chain rule, we get:

$$
f_W(w) = \frac{1}{w} \cdot \frac{(-\ln(w))^{n-1} e^{-\ln(w)}}{\Gamma(n)}, \quad 0 < w \leq 1.
$$

Simplify $e^{-\ln(w)} = \frac{1}{w}$:

$$
f_W(w) = \frac{(-\ln(w))^{n-1}}{\Gamma(n) w^{n}}, \quad 0 < w \leq 1.
$$

The PDF of $W = \prod_{i=1}^n X_i$ is:

$$
f_W(w) = \frac{(-\ln(w))^{n-1}}{\Gamma(n) w^n}, \quad 0 < w \leq 1.
$$

\newpage

# Q4: 4.47, Casella & Berger

*(Marginal normality does not imply bivariate normality.)* 

Let $X$ and $Y$ be independent $N(0,1)$ random variables, and define a new random variable $Z$ by

$$
Z = 
\begin{cases} 
X & \text{if } XY > 0, \\
-X & \text{if } XY < 0.
\end{cases}
$$

## (a) 

Show that $Z$ has a normal distribution.

Our goal is to show that $Z \sim N(0, 1)$.

The condition $XY > 0$ corresponds to $X$ and $Y$ having the same sign:
- $X > 0, Y > 0$,
- $X < 0, Y < 0$.

The condition $XY < 0$ corresponds to $X$ and $Y$ having opposite signs:
- $X > 0, Y < 0$,
- $X < 0, Y > 0$.

Thus, $Z = X$ when $X$ and $Y$ have the same sign, and $Z = -X$ when $X$ and $Y$ have opposite signs.

This occurs when $XY > 0$, which happens in two subcases:
1. $X > 0, Y > 0$,
2. $X < 0, Y < 0$.

The probability that $XY > 0$ is the same as the probability that $X$ and $Y$ have the same sign. Due to independence and symmetry of the standard normal distribution:

$$
P(XY > 0) = P(X > 0, Y > 0) + P(X < 0, Y < 0).
$$

By independence:

$$
P(X > 0, Y > 0) = P(X > 0)P(Y > 0) = \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4},
$$

and similarly:

$$
P(X < 0, Y < 0) = \frac{1}{4}.
$$

Thus:

$$
P(XY > 0) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2}.
$$

This occurs when $XY < 0$, which happens in two subcases:
1. $X > 0, Y < 0$,
2. $X < 0, Y > 0$.

The probability that $XY < 0$ is:

$$
P(XY < 0) = P(X > 0, Y < 0) + P(X < 0, Y > 0).
$$

Similarly:

$$
P(X > 0, Y < 0) = P(X > 0)P(Y < 0) = \frac{1}{4},
$$

and:

$$
P(X < 0, Y > 0) = \frac{1}{4}.
$$

Thus:

$$
P(XY < 0) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2}.
$$

From the definition of $Z$, we see that:
- With probability $\frac{1}{2}$, $Z = X$,
- With probability $\frac{1}{2}$, $Z = -X$.

Thus, $Z$ is distributed as $X$ with equal probability of flipping the sign. The symmetry of the normal distribution ensures that flipping the sign does not alter the distribution. Therefore:

$$
Z \sim N(0, 1).
$$

We have shown that $Z$ has the same distribution as $X$, which is $N(0, 1)$. Thus:

$$
Z \sim N(0, 1).
$$

## (b) 

Show that the joint distribution of $Z$ and $Y$ is not bivariate normal. *(Hint: Show that $$Z$$ and $$Y$$ always have the same sign.)*

To show that the joint distribution of $Z$ and $Y$ is not bivariate normal, we analyze their dependence and demonstrate that $Z$ and $Y$ always have the same sign, violating a property of bivariate normal distributions.

From the definition of $Z$:
$$
Z =
\begin{cases}
X & \text{if } XY > 0, \\
-X & \text{if } XY < 0.
\end{cases}
$$

- **Case 1: When $XY > 0$:**  
  If $XY > 0$, $X$ and $Y$ have the same sign. In this case, $Z = X$, and $Z$ retains the same sign as $X$, which matches the sign of $Y$.  
  Therefore, $Z$ and $Y$ have the same sign.

- **Case 2: When $XY < 0$:**  
  If $XY < 0$, $X$ and $Y$ have opposite signs. In this case, $Z = -X$, so $Z$ flips the sign of $X$. Since $X$ and $Y$ already have opposite signs, flipping the sign of $X$ ensures that $Z$ and $Y$ have the same sign.

In both cases, we conclude:
$$
\text{$Z$ and $Y$ always have the same sign.}
$$

For two random variables $Z$ and $Y$ to follow a bivariate normal distribution:
- Any linear combination of $Z$ and $Y$ must also have a normal distribution.
- The joint PDF of $(Z, Y)$ would not impose deterministic constraints on their values.

However, the fact that $Z$ and $Y$ always share the same sign imposes a **deterministic relationship**:
- If $Z > 0$, then $Y > 0$.  
- If $Z < 0$, then $Y < 0$.  

This deterministic relationship violates the independence and symmetry properties expected of bivariate normal variables. Specifically:
- In a bivariate normal distribution, the joint distribution must allow the variables to take any values within their range, subject only to their covariance structure.
- The joint restriction that $Z$ and $Y$ must have the same sign introduces a nonlinear dependence, which is incompatible with bivariate normality.

The support of the joint distribution of $(Z, Y)$ is constrained to:

$$
\{(z, y): z > 0 \text{ and } y > 0\} \cup \{(z, y): z < 0 \text{ and } y < 0\}.
$$

This means that the joint density of $(Z, Y)$ has **zero probability** in the regions where $Z$ and $Y$ have opposite signs, such as:
- $Z > 0$ and $Y < 0$,
- $Z < 0$ and $Y > 0$.

This behavior is incompatible with a bivariate normal distribution, whose joint density would assign nonzero probability to all regions in \(\mathbb{R}^2\).

The joint distribution of $Z$ and $Y$ is not bivariate normal because:
- $Z$ and $Y$ always have the same sign, which imposes a deterministic nonlinear constraint on their values.  
- This behavior violates the symmetry and independence properties required for a bivariate normal distribution.

\newpage

# Q5: 4.52, Casella & Berger 

Bullets are fired at the origin of an $(x, y)$ coordinate system, and the point hit, say $(X, Y)$, is a random variable. The variables $X$ and $Y$ are taken to be independent $N(0, 1)$ random variables. If two bullets are fired independently, what is the distribution of the distance between them?

Two bullets are fired independently at points $(X_1, Y_1)$ and $(X_2, Y_2)$, where $X_1, Y_1, X_2, Y_2 \sim N(0, 1)$ are independent standard normal random variables. The distance between the two points is given by:

$$
R = \sqrt{(X_2 - X_1)^2 + (Y_2 - Y_1)^2}.
$$

We aim to find the distribution of $R$.

Since $X_1, X_2 \sim N(0, 1)$ are independent, the difference $X_2 - X_1$ is also normally distributed:

$$
X_2 - X_1 \sim N(0, 2).
$$

Similarly, $Y_2 - Y_1 \sim N(0, 2)$.

The squared distance is:

$$
R^2 = (X_2 - X_1)^2 + (Y_2 - Y_1)^2.
$$

Let $Z_1 = X_2 - X_1$ and $Z_2 = Y_2 - Y_1$. Then $Z_1, Z_2 \sim N(0, 2)$, and they are independent. The squared terms are:

$$
Z_1^2 \sim \text{Scaled-Chi-Square}(1, \sigma^2 = 2), \quad Z_2^2 \sim \text{Scaled-Chi-Square}(1, \sigma^2 = 2).
$$

For a standard normal variable $Z \sim N(0, 1)$, $Z^2 \sim \chi^2(1)$. Scaling by $\sigma^2 = 2$, $Z_1^2$ and $Z_2^2$ are scaled $\chi^2(1)$:

$$
Z_1^2 \sim 2 \cdot \chi^2(1), \quad Z_2^2 \sim 2 \cdot \chi^2(1).
$$

Since $Z_1^2 + Z_2^2$ is the sum of two independent scaled $\chi^2(1)$ variables, it follows that:

$$
Z_1^2 + Z_2^2 \sim 2 \cdot \chi^2(2).
$$

A $\chi^2(2)$ distribution is equivalent to an $\text{Exponential}(1)$ distribution. Scaling by 2, we have:

$$
R^2 \sim \text{Exponential}\left(\frac{1}{2}\right).
$$

The random variable $R = \sqrt{R^2}$ is the square root of an $\text{Exponential}\left(\frac{1}{2}\right)$ random variable. The PDF of $R^2 \sim \text{Exponential}\left(\frac{1}{2}\right)$ is:

$$
f_{R^2}(r^2) = \frac{1}{2} e^{-r^2 / 2}, \quad r^2 \geq 0.
$$

To find the PDF of $R$, we apply the change of variables $R^2 = r^2$ with $R = \sqrt{r^2}$, giving:

$$
f_R(r) = f_{R^2}(r^2) \cdot \left| \frac{d(r^2)}{dr} \right| = \frac{1}{2} e^{-r^2 / 2} \cdot 2r = r e^{-r^2 / 2}, \quad r \geq 0.
$$

The distance $R$ between the two bullets follows a **Rayleigh distribution** with scale parameter $\sigma = \sqrt{2}$:

$$
f_R(r) = r e^{-r^2 / 2}, \quad r \geq 0.
$$

\newpage

# Q6: 4.55, Casella & Berger

A **parallel system** is one that functions as long as at least one component of it functions. 

A particular parallel system is composed of three independent components, each of which has a lifetime with an exponential $(\lambda)$ distribution. The lifetime of the system is the maximum of the individual lifetimes. 

What is the distribution of the lifetime of the system?

Let the lifetimes of the three components be $X_1, X_2, X_3$, where each $X_i \sim \text{Exponential}(\lambda)$, and the lifetimes are independent. The lifetime of the parallel system is the maximum of the individual lifetimes:

$$
T = \max(X_1, X_2, X_3).
$$

We aim to find the distribution of $T$.

To determine the distribution of $T$, we first compute its **CDF**, $F_T(t)$, defined as:

$$
F_T(t) = P(T \leq t).
$$

The maximum $T \leq t$ if and only if all the individual lifetimes satisfy $X_1 \leq t$, $X_2 \leq t$, and $X_3 \leq t$. Because the $X_i$ are independent, the joint probability is the product of the individual probabilities:

$$
P(T \leq t) = P(X_1 \leq t) \cdot P(X_2 \leq t) \cdot P(X_3 \leq t).
$$

For an exponential random variable $X \sim \text{Exponential}(\lambda)$, the CDF is:

$$
P(X \leq t) = 1 - e^{-\lambda t}, \quad t \geq 0.
$$

Thus, the CDF of $T$ becomes:

$$
F_T(t) = [P(X_1 \leq t)] \cdot [P(X_2 \leq t)] \cdot [P(X_3 \leq t)] = [1 - e^{-\lambda t}]^3, \quad t \geq 0.
$$

To find the PDF of $T$, we differentiate the CDF:

$$
f_T(t) = \frac{d}{dt} F_T(t) = \frac{d}{dt} [1 - e^{-\lambda t}]^3.
$$

Using the chain rule:

$$
f_T(t) = 3 [1 - e^{-\lambda t}]^2 \cdot \frac{d}{dt} [1 - e^{-\lambda t}].
$$

The derivative of $1 - e^{-\lambda t}$ is:

$$
\frac{d}{dt} [1 - e^{-\lambda t}] = \lambda e^{-\lambda t}.
$$

Substitute this into the expression for $f_T(t)$:

$$
f_T(t) = 3 [1 - e^{-\lambda t}]^2 \cdot \lambda e^{-\lambda t}, \quad t \geq 0.
$$

The lifetime of the parallel system $T = \max(X_1, X_2, X_3)$ has the PDF:

$$
f_T(t) = 3 \lambda [1 - e^{-\lambda t}]^2 e^{-\lambda t}, \quad t \geq 0.
$$

\newpage

# Q7: 4.28, Casella & Berger

Let $X$ and $Y$ be independent standard normal random variables.

## (a) 

Show that $\frac{X}{X + Y}$ has a Cauchy distribution.

To show that $\frac{X}{X + Y}$ has a Cauchy distribution, let's proceed as follows:

Let $Z = \frac{X}{X + Y}$. To study the distribution of $Z$, rewrite it as:

$$
Z = \frac{X}{X + Y}.
$$

This can be rearranged to express $X$ in terms of $Z$ and $Y$:

$$
X = Z(X + Y).
$$

Expanding this gives:

$$
X = ZX + ZY \quad \Rightarrow \quad X(1 - Z) = ZY \quad \Rightarrow \quad X = \frac{ZY}{1 - Z},
$$

where $Z \neq 1$ to avoid division by zero.

Since $X \sim N(0, 1)$ and $Y \sim N(0, 1)$ are independent, the joint probability density function (PDF) is:

$$
f_{X,Y}(x, y) = f_X(x)f_Y(y) = \frac{1}{2\pi} e^{-\frac{x^2 + y^2}{2}}.
$$

Let:

$$
U = X + Y, \quad V = Z = \frac{X}{X + Y}.
$$

This transformation is invertible. The inverse is:

$$
X = VU, \quad Y = U - X = U - VU = U(1 - V).
$$

The Jacobian of the transformation is:

$$
\begin{vmatrix}
\frac{\partial X}{\partial U} & \frac{\partial X}{\partial V} \\[5pt]
\frac{\partial Y}{\partial U} & \frac{\partial Y}{\partial V}
\end{vmatrix}
=
\begin{vmatrix}
V & U \\[5pt]
1 - V & -U
\end{vmatrix}
= (-VU) - U(1 - V) = U.
$$

The absolute value of the Jacobian determinant is $|U|$.

The joint PDF of $U$ and $V$ is:

$$
f_{U,V}(u, v) = f_{X,Y}(x, y) \cdot |J|,
$$

where $(X, Y)$ are expressed in terms of $(U, V)$:

$$
f_{U,V}(u, v) = \frac{1}{2\pi} \exp\left(-\frac{(vu)^2 + (u(1 - v))^2}{2}\right) |u|.
$$

Simplifying the exponent:

$$
(vu)^2 + (u(1 - v))^2 = u^2(v^2 + (1 - v)^2) = u^2(v^2 + 1 - 2v + v^2) = u^2(1 + v^2).
$$

Thus:

$$
f_{U,V}(u, v) = \frac{1}{2\pi} \exp\left(-\frac{u^2(1 + v^2)}{2}\right) |u|.
$$

To find the marginal distribution of $V = Z$, integrate out $u$:

$$
f_V(v) = \int_{-\infty}^\infty f_{U,V}(u, v) \, du.
$$

Substitute $f_{U,V}(u, v)$:

$$
f_V(v) = \int_{-\infty}^\infty \frac{1}{2\pi} \exp\left(-\frac{u^2(1 + v^2)}{2}\right) |u| \, du.
$$

Factorize the integral using the symmetry of the normal distribution and recognize a Gaussian integral:

$$
\int_{-\infty}^\infty \exp\left(-\frac{u^2(1 + v^2)}{2}\right) |u| \, du = \frac{1}{1 + v^2}.
$$

Thus:

$$
f_V(v) = \frac{1}{\pi(1 + v^2)}.
$$

The PDF $f_V(v) = \frac{1}{\pi(1 + v^2)}$ is the PDF of a standard Cauchy distribution. Therefore, $\frac{X}{X + Y}$ has a Cauchy distribution.

## (b) 

Find the distribution of $\frac{X}{|Y|}$.

To find the distribution of $\frac{X}{|Y|}$, where $X$ and $Y$ are independent standard normal random variables, let's proceed step by step.

Let $Z = \frac{X}{|Y|}$. To determine the distribution of $Z$, we calculate its PDF.

We know $X \sim N(0, 1)$ and $Y \sim N(0, 1)$. The joint PDF of $X$ and $Y$ is:

$$
f_{X,Y}(x, y) = \frac{1}{2\pi} e^{-\frac{x^2 + y^2}{2}}.
$$

Since $|Y| = |y|$, the PDF of $|Y|$ is given by:

$$
f_{|Y|}(y) = 
\begin{cases} 
\sqrt{\frac{2}{\pi}} e^{-\frac{y^2}{2}}, & y \geq 0, \\
0, & y < 0.
\end{cases}
$$

Thus, the joint PDF of $X$ and $|Y|$ is:

$$
f_{X,|Y|}(x, y) = 
\begin{cases} 
\frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \cdot \sqrt{\frac{2}{\pi}} e^{-\frac{y^2}{2}}, & y \geq 0, \\
0, & y < 0.
\end{cases}
$$

For $y \geq 0$, this simplifies to:

$$
f_{X,|Y|}(x, y) = \frac{1}{\pi} e^{-\frac{x^2 + y^2}{2}}.
$$

Let $Z = \frac{X}{|Y|}$ and $W = |Y|$. Then:
$$
X = ZW \quad \text{and} \quad |Y| = W.
$$

The Jacobian of this transformation is:

$$
\begin{vmatrix}
\frac{\partial X}{\partial Z} & \frac{\partial X}{\partial W} \\[5pt]
\frac{\partial |Y|}{\partial Z} & \frac{\partial |Y|}{\partial W}
\end{vmatrix}
=
\begin{vmatrix}
W & Z \\[5pt]
0 & 1
\end{vmatrix}
= W.
$$

Thus, the joint PDF of $Z$ and $W$ is:

$$
f_{Z,W}(z, w) = f_{X,|Y|}(zw, w) \cdot |W| = \frac{1}{\pi} e^{-\frac{(zw)^2 + w^2}{2}} \cdot w.
$$

The marginal PDF of $Z$ is obtained by integrating out $W$:

$$
f_Z(z) = \int_0^\infty f_{Z,W}(z, w) \, dw.
$$

Substitute $f_{Z,W}(z, w)$:
$$
f_Z(z) = \int_0^\infty \frac{1}{\pi} e^{-\frac{(zw)^2 + w^2}{2}} w \, dw.
$$

Factorize the exponent:

$$
(zw)^2 + w^2 = w^2(z^2 + 1),
$$
so:
$$
f_Z(z) = \int_0^\infty \frac{1}{\pi} e^{-\frac{w^2(z^2 + 1)}{2}} w \, dw.
$$

Change variables to simplify the integral. Let $u = \frac{w^2(z^2 + 1)}{2}$, so $w^2 = \frac{2u}{z^2 + 1}$ and $dw = \frac{du}{w(z^2 + 1)}$. When $w = 0$, $u = 0$; as $w \to \infty$, $u \to \infty$. Substituting, we get:
$$
f_Z(z) = \int_0^\infty \frac{1}{\pi} e^{-u} \cdot \frac{1}{z^2 + 1} \, du.
$$

The integral of $e^{-u}$ from $0$ to $\infty$ is $1$, so:
$$
f_Z(z) = \frac{1}{\pi(z^2 + 1)}.
$$

The PDF of $Z = \frac{X}{|Y|}$ is:
$$
f_Z(z) = \frac{1}{\pi(1 + z^2)}.
$$

This is the standard Cauchy distribution. Therefore:

$$
\frac{X}{|Y|} \sim \text{Cauchy}(0, 1).
$$


## (c) 

Is the answer to part (b) surprising? Can you formulate a general theorem?

At first glance, the result that \(\frac{X}{|Y|}\) has a standard Cauchy distribution might seem surprising because \(|Y|\) (the absolute value of a standard normal random variable) is strictly positive and not symmetrically distributed like \(X\). Despite this, the independence of \(X\) and \(|Y|\) and the particular relationship between their distributions result in a cancellation of dependencies, yielding the familiar Cauchy distribution. This is a remarkable property of the ratio of independent normal and scaled distributions, arising due to specific symmetry properties.

The result in parts (a) and (b) can be unified under a broader theorem:

Let \(X\) and \(Y\) be independent \(N(0, \sigma^2)\) random variables. Define \(R = \frac{X}{aY + b}\), where \(a, b \in \mathbb{R}\) and \(a \neq 0\). Then:
$$
R \sim \text{Cauchy}\left(0, \frac{\sigma}{|a|}\right).
$$

1. **Case 1 (Part (b)):**
   When \(a = 1\) and \(b = 0\), \(R = \frac{X}{Y}\), and the result is a standard Cauchy distribution:
   $$
   R \sim \text{Cauchy}(0, \sigma).
   $$

2. **Case 2 (Part (b), absolute value modification):**
   When \(R = \frac{X}{|Y|}\), we use the symmetry of \(Y\). The absolute value does not alter the result since the distribution depends on the magnitude of \(Y\), not its sign. Thus:
   $$
   R \sim \text{Cauchy}(0, \sigma).
   $$

3. **Case 3 (Scaled and shifted denominator):**
   When \(a \neq 1\) or \(b \neq 0\), the location parameter is adjusted by the shift in the denominator, but the scale depends only on \(a\).

The theorem can be explained through the invariance of the ratio of independent Gaussian variables under scaling and translation:

- The numerator \(X\) introduces symmetry in the numerator.
- The denominator \(aY + b\) shifts and scales the variable but does not change the overall form of the distribution due to the independence of \(X\) and \(Y\).

This property of the ratio of independent Gaussians is tied to the definition of the Cauchy distribution, which arises naturally in this context.

\newpage

# Q8: 4.50, Casella & Berger

If $(X, Y)$ has the bivariate normal probability density function (pdf):

$$
f(x, y) = \frac{1}{2\pi(1-\rho^2)^{1/2}} \exp \left( -\frac{1}{2(1-\rho^2)} \left( x^2 - 2\rho xy + y^2 \right) \right),
$$

show that 

$$
\text{Corr}(X, Y) = \rho
$$ 

and 

$$
\text{Corr}(X^2, Y^2) = \rho^2.
$$ 

*Hint:* Conditional expectations will simplify calculations.

We are given that $(X, Y)$ has a bivariate normal PDF:

$$
f(x, y) = \frac{1}{2\pi(1-\rho^2)^{1/2}} \exp \left( -\frac{1}{2(1-\rho^2)} \left( x^2 - 2\rho xy + y^2 \right) \right),
$$

and we are tasked to show:

1. $\text{Corr}(X, Y) = \rho$,  
2. $\text{Corr}(X^2, Y^2) = \rho^2$.

We will proceed step-by-step.

The correlation between $X$ and $Y$ is:

$$
\text{Corr}(X, Y) = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X) \cdot \text{Var}(Y)}}.
$$

From the properties of the bivariate normal distribution:
- $X \sim N(0, 1)$, so $\text{Var}(X) = 1$,
- $Y \sim N(0, 1)$, so $\text{Var}(Y) = 1$.

Thus:

$$
\text{Corr}(X, Y) = \text{Cov}(X, Y).
$$

The covariance for a bivariate normal random variable $(X, Y)$ is given by the parameter $\rho$:

$$
\text{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y].
$$

Since both $X$ and $Y$ are standard normal random variables, $\mathbb{E}[X] = \mathbb{E}[Y] = 0$. Therefore:

$$
\text{Cov}(X, Y) = \mathbb{E}[XY].
$$

For a bivariate normal random variable with correlation $\rho$, it is a known result that:

$$
\mathbb{E}[XY] = \rho.
$$

Thus:

$$
\text{Cov}(X, Y) = \rho,
$$
and therefore:

$$
\text{Corr}(X, Y) = \rho.
$$

The correlation between $X^2$ and $Y^2$ is:

$$
\text{Corr}(X^2, Y^2) = \frac{\text{Cov}(X^2, Y^2)}{\sqrt{\text{Var}(X^2) \cdot \text{Var}(Y^2)}}.
$$

For a bivariate normal random variable, the conditional expectation simplifies calculations. Specifically, for $Y | X = x$, we have:

$$
Y | X = x \sim N(\rho x, 1 - \rho^2).
$$

Using this, we calculate the necessary terms.

We use the law of total expectation:

$$
\mathbb{E}[X^2 Y^2] = \mathbb{E}[X^2 \cdot \mathbb{E}[Y^2 | X]].
$$

The conditional variance and mean of $Y$ given $X$ allow us to compute $\mathbb{E}[Y^2 | X]$:

$$
\mathbb{E}[Y^2 | X] = \text{Var}(Y | X) + (\mathbb{E}[Y | X])^2.
$$

Here:
- $\text{Var}(Y | X) = 1 - \rho^2$,
- $\mathbb{E}[Y | X] = \rho X$.

Thus:

$$
\mathbb{E}[Y^2 | X] = (1 - \rho^2) + (\rho X)^2 = 1 - \rho^2 + \rho^2 X^2.
$$

Substitute this into $\mathbb{E}[X^2 Y^2]$:

$$
\mathbb{E}[X^2 Y^2] = \mathbb{E}[X^2 \cdot (1 - \rho^2 + \rho^2 X^2)].
$$

Expand:

$$
\mathbb{E}[X^2 Y^2] = \mathbb{E}[X^2] \cdot (1 - \rho^2) + \rho^2 \mathbb{E}[X^4].
$$

Since $X \sim N(0, 1)$, the moments are:
- $\mathbb{E}[X^2] = 1$,
- $\mathbb{E}[X^4] = 3$ (from properties of normal distribution).

Thus:

$$
\mathbb{E}[X^2 Y^2] = (1)(1 - \rho^2) + \rho^2(3) = 1 - \rho^2 + 3\rho^2 = 1 + 2\rho^2.
$$

The covariance is:

$$
\text{Cov}(X^2, Y^2) = \mathbb{E}[X^2 Y^2] - \mathbb{E}[X^2]\mathbb{E}[Y^2].
$$

From above, $\mathbb{E}[X^2 Y^2] = 1 + 2\rho^2$. Also:
- $\mathbb{E}[X^2] = 1$,
- $\mathbb{E}[Y^2] = 1$.

Thus:

$$
\text{Cov}(X^2, Y^2) = (1 + 2\rho^2) - (1)(1) = 2\rho^2.
$$

The variance of $X^2$ is:

$$
\text{Var}(X^2) = \mathbb{E}[X^4] - (\mathbb{E}[X^2])^2.
$$

From above:

$$
\text{Var}(X^2) = 3 - (1)^2 = 2.
$$

By symmetry:

$$
\text{Var}(Y^2) = 2.
$$

Finally:

$$
\text{Corr}(X^2, Y^2) = \frac{\text{Cov}(X^2, Y^2)}{\sqrt{\text{Var}(X^2) \cdot \text{Var}(Y^2)}}.
$$

Substitute the values:

$$
\text{Corr}(X^2, Y^2) = \frac{2\rho^2}{\sqrt{2 \cdot 2}} = \frac{2\rho^2}{2} = \rho^2.
$$

Sumamry: 

1. $\text{Corr}(X, Y) = \rho$,
2. $\text{Corr}(X^2, Y^2) = \rho^2$. 
