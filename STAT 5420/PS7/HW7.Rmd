---
title: "HW7"
output: pdf_document
date: "2024-11-03"
---

# Q1 

Consider continuous random vector (X, Y) where $X \sim N(0, 1)$ (standard normal) and $Y | X = x \sim N(x, 1)$ (the conditional distribution of Y given X = x is normal with mean x and variance 1). 

## (a) 

Find the joint pdf of (X, Y).

$$
f_X(x) = \frac{1}{\sqrt{2\pi}} e^{\left(-\frac{x^2}{2}\right)}
$$

$$
f_{Y|X}(y \mid x) = \frac{1}{\sqrt{2\pi}} e^{\left(-\frac{(y - x)^2}{2}\right)}
$$

$$
f_{X,Y}(x, y) = f_X(x) f_{Y|X}(y | x) = \frac{1}{\sqrt{2\pi}} e^{\left(-\frac{x^2}{2}\right)} \frac{1}{\sqrt{2\pi}} e^{\left(-\frac{(y - x)^2}{2}\right)} = \frac{1}{2\pi} e^{\left(-\frac{x^2}{2} - \frac{(y - x)^2}{2}\right)}
$$

## (b) 

Find the marginal distribution of Y. 

$$
f_Y(y) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) \, dx = \int_{-\infty}^{\infty} \frac{1}{2\pi} e^{\left(-\frac{x^2}{2} - \frac{(y - x)^2}{2}\right)} \, dx
$$

$$
f_Y(y) = \int_{-\infty}^{\infty} \frac{1}{2\pi} e^{-\frac{x^2}{2} - \frac{y^2 - 2yx + x^2}{2}} \, dx = \int_{-\infty}^{\infty} \frac{1}{2\pi} e^{-\frac{y^2}{2} + yx - x^2} \, dx
$$

$$
f_Y(y) = \int_{-\infty}^{\infty} \frac{1}{2\pi} e^{-\frac{y^2}{2} + \frac{y^2}{4}} e^{-\left(x - \frac{y}{2}\right)^2} \, dx
$$

$$
f_Y(y) = \int_{-\infty}^{\infty} \frac{1}{2\pi} e^{-\frac{y^2}{4}} e^{-\left(x - \frac{y}{2}\right)^2} \, dx
$$

$$
f_Y(y) = \frac{1}{2\pi} e^{-\frac{y^2}{4}} \int_{-\infty}^{\infty} e^{-\left(x - \frac{y}{2}\right)^2} \, dx
$$

Take advantage of "knowing" $\int_{-\infty}^{\infty} e^{-\left(x - \frac{y}{2}\right)^2} \, dx = \sqrt{\pi}$

$$
f_Y(y) = \frac{1}{2\pi} e^{-\frac{y^2}{4}} \sqrt{\pi} = \frac{1}{2\sqrt{\pi}} e^{-\frac{y^2}{4}}
$$

$$
Y \sim N(0, 2)
$$


## (c) 

Consider random vector (V, W) where $W \sim N(0, 2)$ (normal with mean 0 and variance 2). How should the conditional distribution V | W = w be chosen so that (V, W) has the same distribution as (X, Y)? 

### First Method 

We may find the distribution of $V \mid W = w$ directly by calculating the closed form of its pdf. The following does so for $X \mid Y = y$, which we know to be $V \mid W = w$ given equivalence of the random variable X $\equiv$ V, Y $\equiv$ W. To that end: 

$$
f_{X | Y = y}(x) = \frac{f_{X,Y}(x, y)}{f_Y(y)} = \frac{\frac{1}{2\pi} e^{-\frac{2x^2 - 2xy + y^2}{2}}}{\frac{1}{2\sqrt{\pi}} e^{-\frac{y^2}{4}}}
$$

$$
f_{X | Y = y}(x) = \frac{1}{\sqrt{\pi}} e^{-\frac{2x^2 - 2xy + y^2}{2} + \frac{y^2}{4}}
$$

Simplifying (or at least attempting to simplify) the exponent: 

$$
-\frac{2x^2 - 2xy + y^2}{2} + \frac{y^2}{4} = -\frac{2x^2}{2} + \frac{2xy}{2} - \frac{y^2}{2} + \frac{y^2}{4} = -x^2 + xy - \frac{y^2}{4}
$$

$$
-\frac{2x^2 - 2xy + y^2}{2} + \frac{y^2}{4} = \left(x - \frac{y}{2}\right)^2 - \frac{y^2}{4} = -\left(x - \frac{y}{2}\right)^2
$$

$$
f_{X | Y = y}(x) = \frac{1}{\sqrt{\pi}} e^{-\frac{(x - \frac{y}{2})^2}{2(\frac{1}{2})}}
$$

$$
X | Y = y \sim N\left(\frac{y}{2}, \frac{1}{2}\right)
$$

So, similar to the evaluation given in part (c) above, we do indeed have: 

$$
V | W = w \sim N\left(\frac{w}{2}, \frac{1}{2}\right)
$$

### A Brief Alternative Formulation

First, consider the joint distribution of (X, Y); we know this is a multivariate normal distribution. As such, given $W \sim N(0, 2)$, $V \sim N(0,1)$, we know that the random variable V | W = w must also be normally distributed. Our goal then is to obtain the parameters of this normal distribution to uniquely characterize its distribution. To that end: 

Consider the random vector (X, Y), made up of the Mean vector and Covariance Matrix: 

Recall also that we know the Covariance Matrix as $Y | X = x \sim N(x, 1)$, meaning Cov(X, Y) = 1 = Cov(Y, X). 

More details on why Cov(X, Y) = 1: 

$$
Cov(X, Y) = E[X Y] - E[X] E[Y]
$$

And: 

$$
E[X Y] = E[E[X Y | X]] = E[X E[Y | X]] = E[X(X)] = E[X^2] 
$$

$$
E[X Y] = E[X^2] = \text{Var}(X) - E[X]^2 = 1 - 0^2 = 1
$$

Then, we have: 

$$
Cov(X, Y) = 1 - E[X] E[Y] = 1 - 0(0) = 1
$$

$$
Cov(X, Y) = 1
$$

Taking this into account then: 

$$
\begin{pmatrix}
X \\
Y
\end{pmatrix}
\sim N\left(
\begin{pmatrix}
0 \\
0
\end{pmatrix},
\begin{pmatrix}
1 & 1 \\
1 & 2
\end{pmatrix}
\right)
$$

As $\mu_V = 0$: 

$$
E(V \mid W = w) = \mu_V + \frac{Cov(V, W)}{Var(W)} (w-\mu_W) = \mu_V + \frac{Cov(V, W)}{Var(W)} (w-0) = \frac{1}{2} w
$$

$$
E(V \mid W = w) = \mu_V + \frac{Cov(V, W)}{Var(W)} w = 0 + \frac{Cov(V, W)}{Var(W)} w = \frac{1}{2} w
$$

As Var(V) = 1
     
$$
Var(V | W = w) = Var(V) - \frac{Cov(V, W)^2}{Var(W)} = 1 - \frac{1^2}{2} = \frac{1}{2}
$$

Furthermore, as we have found the mean and variance of V given W = w, we may describe its distribution as: 

$$
V \mid W = w \sim N\left(\frac{1}{2} w, \frac{1}{2}\right)
$$

\newpage

# Q2: 4.7 Casella & Berger

A woman leaves for work between 8AM and 8:30AM and takes between 40 and 50 minutes to get there. Let the random variable X denote her time of departure, and the random variable Y the travel time. Assuming that these variables are independent and uniformly distributed, find the probability the the woman arrives at work before 9AM. 

$$
X \sim U(0, 30), Y \sim U(40, 50)
$$

Given X and Y are both Uniformly distributed and independent, then:

$$
f_{X, Y}(x,y) = f_X(x)f_Y(y) = \frac{1}{30}(\frac{1}{50-40}) = \frac{1}{300}
$$

Define the random variable Z = X + Y, then we want to know the probability Z < 60 (1 hour, 60 minutes starting at the earliest time 8:00AM): 

$$
P(Z < 60) = P(X + Y < 60) = \int\limits_{40}^{50} \int\limits_{0}^{60-y} f_{X, Y}(x,y) dx dy = \int\limits_{40}^{50} \int\limits_{0}^{60-y} \frac{1}{300} dx dy = \int\limits_{40}^{50} \frac{60-y}{300}dy = -\frac{(y-120(y)}{600} \big|_{y=40}^{50}= \frac{1}{2}
$$

\newpage

# Q3: 4.10 Casella & Berger

The random pair (X, Y) has the distribution: 

```{r, echo=FALSE, fig.cap="Img", out.width = '100%'}
knitr::include_graphics("dist.png")
```

## (a) 

Show that X and Y are dependent. 

To show that two variables are not independent (are dependent), we need to show that the product of the marginal probabilities is not equal to joint probability. To that end: 

$$
P(X = 1) = \frac{1}{12} + \frac{1}{6} = \frac{3}{12} = \frac{1}{4}
$$

$$
P(Y = 4) = \frac{1}{3}
$$ 

$$
P(X = 1, Y = 4) = 0
$$

$$
P(X = 1, Y = 4) = 0 \neq \frac{1}{12} = \frac{1}{4}(\frac{1}{3}) = P(X = 1)P(Y = 4)
$$

## (b) 

Give a probability table for random variables U and V that have the same marginals as X and Y but are independent. 

We need the sum of the probabilities of the rows to all be equal to one another, as well as the sum of the probabilities of column U = 1 and U = 3 to be equal to one another (with the sum of probabilities of column U = 2 equal to twice that). We can effectively treat this as a system of linear equations satisfying: 
  1. P(U = 1) = P(U = 3) = 3/12
  2. P(V = 2) = P(V = 3) = P(V = 4) = 1/3
  3. P(U = 2) = P(U = 1) + P(U - 3) 
  
To satisfy these conditions we have: 

|       |     U       |       |       |
|-------|------------------------|-------|-------|
|   V   | 1| 2 | 3 |
|-------|------------|------------|------------|
| 2     | 1/12       | 1/6        | 1/12       |
| 3     | 1/12       | 1/6        | 1/12       |
| 4     | 1/12       | 1/6        | 1/12       |

And to verify independence, take one example: 

$$
P(U = 1, V = 2) = \frac{1}{12} = \frac{1}{4} (\frac{1}{3}) = P(U=1)P(V=2)
$$

We would need to run through all possible combinations to validate, but given the form of the above table, it suffices to say that U and V are independent. 

\newpage

# Q4 

Suppose X and Y are independent random variables, where both have the same (marginal) geometric(p) distribution for 0 < p < 1. Find the conditional distribution of X given X + Y = k (integer k $\geq 2$).

Define two new random variables, U and V, such that: 

$$
U = X
$$

$$
V = X + Y
$$

Note then that we may write X and Y as: 

$$
X = U
$$

$$
Y = V - U
$$

Furthermore, we have: 

$$
|J| = \left| 
\begin{pmatrix}
\frac{\partial X}{\partial U} & \frac{\partial X}{\partial V} \\
\frac{\partial Y}{\partial U} & \frac{\partial Y}{\partial V}
\end{pmatrix}
\right| 
= \left| \begin{array}{cc}
1 & 0 \\
-1 & 1
\end{array} \right| = |1 - 0| = 1
$$

Then, we have: 

$$
f(U|V) f(V) = f(U,V) \rightarrow f(U|V) = \frac{f(U,V)}{f(V)}
$$

First, note: 

Let us take each part of this fraction to evaluate the point in question: 

$$
f(U,V) = f_{X,Y}(u, v-u)|J| = f_{X,Y}(u, v-u) = f_X(u)f_Y(v-u)
$$

With note of independence between X and Y. This simplifies and can be evaluated to: 

$$
f(U,V) = (1 - p)^{x - 1} p \left((1 - p)^{k - x - 1} p\right) = (1-p)^{k-2}p^2
$$

Then we evaluate the bottom of the initial formula, noting that $f(V)$ is the marginal distribution of V: 

$$
f(V) = \sum_{u \in U} f(U,V) = \sum_{u=1}^{k-1} f(U,V) = \sum_{u=1}^{k-1} (1 - p)^{k - 2} p^2 = (k - 1) (1 - p)^{k - 2} p^2
$$

Returning the overall formula, we evaluate as: 

$$
P(U | V) = \frac{P(U,V)}{P(V)} = \frac{f(U,V)}{f(V)} = \frac{(1 - p)^{k - 2} p^2}{(k - 1) (1 - p)^{k - 2} p^2} = \frac{1}{k - 1}
$$

For u = 1, 2, ..., k - 1, v = 1, 2, ..., k - 1. 

So we have: 

$$
f(U|V) = \frac{f(U,V)}{f(V)} = \frac{1}{k-1}
$$

We then conclude:

$$
U | V \sim Uniform(1, k) \rightarrow X | (X + Y = k) \sim Uniform(1, k)
$$

\newpage

# Q5: 4.27 Casella & Berger 

Let $X \sim n(\mu, \sigma^2)$, and let $Y \sim n(\gamma, \sigma^2)$. Suppose X and Y are independent. Define: U = X + Y and V = X - Y. Show that U and V are independent normal random variables. Find the distribution of each of them. 

Hey, so I am not entirely sure why this problem uses n to represent the normal instead of N as we usually do, but for this problem I just stayed (or attempted to stay) consistent with their format. 

For X and Y independent, we may write the joint pdf as the product of the marginals: 

$$
f_{X, Y}(x, y) = f_X(x) f_Y(f) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-(x-\mu)^2}{2\sigma^2}} \frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-(y-\gamma)^2}{2\sigma^2}} = \frac{1}{2\pi\sigma^2}e^{\frac{-\left((x-\mu)^2 + (y - \gamma)^2\right)}{2\sigma^2}}
$$

$$
U = X + Y \text{ and } V = X - Y \rightarrow X = \frac{1}{2}(U + V), Y - \frac{1}{2}(U - V)
$$

We then have the Jacobian: 

$$
|J| = \left| 
\begin{pmatrix}
\frac{\partial X}{\partial U} & \frac{\partial X}{\partial V} \\
\frac{\partial Y}{\partial U} & \frac{\partial Y}{\partial V}
\end{pmatrix}
\right| 
= \left| \begin{array}{cc}
\frac{1}{2} & \frac{1}{2} \\
\frac{1}{2} & -\frac{1}{2}
\end{array} \right| = |- \frac{1}{4} - \frac{1}{4} | = |-\frac{1}{2}| = \frac{1}{2}
$$

And we have the new support, that being the support of U, V as: 

$$
(u, v) \in (-\infty, \infty) \text{x} (-\infty, \infty)
$$

Using the above relations, we may calculate the joint pdf of (U, V): 

$$
f_{UV}(u, v) = \frac{1}{2 \pi \sigma^2} e^{-\frac{1}{2 \sigma^2} \left[ \left( \frac{u+v}{2} - \mu \right)^2 + \left( \frac{u-v}{2} - \gamma \right)^2 \right]} \frac{1}{2}
$$

$$
f_{UV}(u, v) = \frac{1}{4 \pi \sigma^2} e^{-\frac{1}{2 \sigma^2} \left[ 2 \left( \frac{u}{2} \right)^2 - u (\mu + \gamma) + \left( \frac{\mu + \gamma}{2} \right)^2 + 2 \left( \frac{v}{2} \right)^2 - v (\mu - \gamma) + \left( \frac{\mu - \gamma}{2} \right)^2 \right]}
$$

$$
f_{UV}(u, v) = g(u) \frac{1}{4 \pi \sigma^2} e^{-\frac{1}{2(2 \sigma^2)} \left( u - (\mu + \gamma) \right)^2} h(v) e^{-\frac{1}{2(2 \sigma^2)} \left( v - (\mu - \gamma) \right)^2}
$$

However, we already know the distributions of U and V, namely: 

The marginal pdf of U is: 

$$
f_{U}(u) = g(u) \frac{1}{4 \pi \sigma^2} e^{-\frac{1}{2(2 \sigma^2)} \left( u - (\mu + \gamma) \right)^2}
$$

With note of Corollary 4.6.10:

$$
U \sim N(\mu + \gamma, 2\sigma^2)
$$

And the marginal pdf of V is: 

$$
f_{V}(v) = h(v) e^{-\frac{1}{2(2 \sigma^2)} \left( v - (\mu - \gamma) \right)^2}
$$ 

Again with note of Corollary 4.6.10:

$$
V \sim N(\mu - \gamma, 2\sigma^2)
$$

Taken together, we have: 

$$
f_{UV}(u, v) = f_{U}(u) f_{V}(v)
$$

Such that we have shown that the joint pdf of (U, V) is equal to the product of the marginal pdfs of U and V and conclude that U and V are independent. 

\newpage

# Q6: 4.42 Casella & Berger  

Let X and Y be independent random variables with means $\mu_X, \mu_Y$ and variances $\sigma_X^2, \sigma_Y^2$. Find an expression for the correlation of XY and Y in terms of these means and variances. 

Said differently, we want to find: $\rho_{XY, Y}$

$$
\rho_{XY, Y} = \frac{Cov(XY, Y)}{\sigma_{XY} \sigma_Y} = \frac{E(XY^2) - \mu_X \mu_Y}{\sigma_{XY} \sigma_Y} = \frac{EX EY^2 - \mu_X \mu_Y \mu_Y}{\sigma_{XY} \sigma_Y}
$$

$$
\rho_{XY, Y} = \frac{\mu_{X} (\sigma_{Y}^2 + \mu_{Y}^2) - \mu_X \mu_Y \mu_Y}{\sigma_{XY} \sigma_Y} = \frac{\mu_{X} (\sigma_{Y}^2)}{\sigma_{XY} \sigma_Y}
$$

With note of X and Y being independent, we then have the denominator: 

$$
\sigma_{XY}^2 = E(XY)^2 - [E(XY)]^2 = EX^2 EY^2 - (EX)^2 (EY)^2 = (Var(X) + E[X]^2)(Var(Y) + E[Y]^2) - (EX)^2 (EY)^2
$$

Evaluating with known quantities, we have: 

$$
\sigma_{XY}^2 = (\sigma_X^2 + \mu_X^2)(\sigma_Y^2 + \mu_Y^2) - \mu_X^2 \mu_Y^2 = \sigma_X^2 \sigma_Y^2 + \sigma_X^2 \mu_Y^2 + \sigma_Y^2 \mu_X^2 + \mu_X^2 \mu_Y^2 - \mu_X^2 \mu_Y^2 = \sigma_X^2 \sigma_Y^2 + \sigma_X^2 \mu_Y^2 + \sigma_Y^2 \mu_X^2
$$

Substituting into the original equation gives us: 

$$
\rho_{XY, Y} = \frac{\mu_{X} (\sigma_{Y}^2)}{\sqrt{(\sigma_X^2 \sigma_Y^2 + \sigma_X^2 \mu_Y^2 + \sigma_Y^2 \mu_X^2)} \sigma_Y} = \frac{\mu_X \sigma_Y}{\sqrt{(\mu_X^2 \sigma_Y^2 + \mu_Y^2 \sigma_X^2 + \sigma_X^2 \sigma_Y^2)}}
$$

\newpage

# Q7 

Suppose that random variables $X_1, X_2, X_3$ have joint pdf $f(x_1, x_2, x_3) = 6$ for $0 < x_1 < x_2 < x_3 < 1$. 

## (a) 

Are $X_1, X_2, X_3$ independent? Give an intuitive explanation. 

No they are not independent, as knowing the value of one of the random variables limits the range of possible values that the other random variables can take. 

## (b) 

Find the marginal pdf of $X_2$ and identify the distribution of $X_2$ as a member of an important family of distributions. 

Let us first calculate the marginal of $X_2$: 

$$
f_{X_2}(x_2) = \int_0^{x_2} \int_{x_2}^1 6 \, dx_3 \, dx_1 = \int_{x_2}^1 6 \, dx_3 = 6(1 - x_2)
$$

$$
f_{X_2}(x_2) = \int_0^{x_2} 6(1 - x_2) \, dx_1 = 6(1 - x_2) \int_0^{x_2} 1 \, dx_1 = 6(1 - x_2)x_2
$$

As $0 < x_2 < 1$, the range of $x_2$ gives us a clue as to the distribution of $X_2$. Specifically: 

$$
X_2 \sim Beta(2, 2)
$$

## (c) 

Find the conditional pdf $f(x_1, x_3 | x_2)$ of $X_1, X_3 \text{ given } X_2 = x_2 \in (0,1)$.

$$
f(x_1, x_3 \mid x_2) = \frac{f(x_1, x_2, x_3)}{f_{X_2}(x_2)} = \frac{6}{6x_2(1 - x_2)} = \frac{1}{x_2(1 - x_2)}
$$

## (d) 

Show that $X_1, X_3$ are independent given $X_2 = x_2 \in (0, 1)$ (i.e., think of what must be true of the conditional pdf of $f(x_1, x_3 | x_2)$ in this case). 

Let us start by considering the joint pdf: 

$$
f(x_1, x_3 \mid x_2) = \frac{1}{x_2 (1 - x_2)}
$$

Let us turn then to the marginals, with the goal of showing the product of the marginals is equal to the joint as specified. To that end:

### i. 

$$
f_{X_1 \mid X_2}(x_1 \mid x_2) = \int_{x_2}^1 f(x_1, x_3 \mid x_2) \, dx_3 = \int_{x_2}^1 \frac{1}{x_2 (1 - x_2)} \, dx_3
$$

$$
f_{X_1 \mid X_2}(x_1 \mid x_2) = \frac{1}{x_2 (1 - x_2)} \int_{x_2}^1 1 \, dx_3 = \frac{1}{x_2 (1 - x_2)} (1 - x_2) = \frac{1}{x_2}
$$

### ii. 

$$
f_{X_3 \mid X_2}(x_3 \mid x_2) = \int_0^{x_2} f(x_1, x_3 \mid x_2) \, dx_1 = \int_0^{x_2} \frac{1}{x_2 (1 - x_2)} \, dx_1
$$

$$
f_{X_3 \mid X_2}(x_3 \mid x_2) = \frac{1}{x_2 (1 - x_2)} \int_0^{x_2} 1 \, dx_1 = \frac{1}{x_2 (1 - x_2)} x_2 = \frac{1}{1 - x_2}
$$

Taking the results of i. and ii., we then have: 

$$
f_{X_1 \mid X_2}(x_1 \mid x_2) = \frac{1}{x_2}
$$

$$
f_{X_3 \mid X_2}(x_3 \mid x_2) = \frac{1}{1 - x_2}
$$

$$
f(x_1, x_3 \mid x_2) = \frac{1}{x_2 (1 - x_2)} = f_{X_1 \mid X_2}(x_1 \mid x_2) f_{X_3 \mid X_2}(x_3 \mid x_2)
$$

Since the conditional joint pdf can be written as a product of the marginal conditional pdfs, $X_1$ and $X_3$ are independent given $X_2 = x_2$.

## (e) 

Find the covariance of $X_1 \text{ and } X_3$ given $X_2 = x_2$.

From part (d), we know that $X_1$ and $X_3$ are independent given $X_2 = x_2$, so we know that conditional on/given $X_2 = x_2$ that the covariance of $X_1$ and $X_3$ would be zero, or: 

$$
Cov(X_1, X_3 | X_2 = x_2) = 0
$$