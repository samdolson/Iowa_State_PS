---
title: "HW7"
output: pdf_document
date: "2024-11-03"
---

Outline
Q1: PARTIAL check (b), (c)
Q2: DONE 
Q3: DONE
Q4: DONE
Q5: DONE(ish)
Q6: DONE
Q7: DONE(ish)

# Q1 

Consider continuous random vector (X, Y) where $X \sim N(0, 1)$ (standard normal) and $Y | X = x \sim N(x, 1)$ (the conditional distribution of Y given X= x is normal with mean x and variance 1). 

## (a) 

Find the joint pdf of (X, Y).

$$
f_X(x) = \frac{1}{\sqrt{2\pi}} e^{\left(-\frac{x^2}{2}\right)}
$$

$$
f_{Y|X}(y \mid x) = \frac{1}{\sqrt{2\pi}} e^{\left(-\frac{(y - x)^2}{2}\right)}
$$

$$
f_{X,Y}(x, y) = f_X(x) f_{Y|X}(y | x) = \frac{1}{\sqrt{2\pi}} e^{\left(-\frac{x^2}{2}\right)} \frac{1}{\sqrt{2\pi}} e^{\left(-\frac{(y - x)^2}{2}\right)} = \frac{1}{2\pi} e^{\left(-\frac{x^2}{2} - \frac{(y - x)^2}{2}\right)}
$$

## (b) 

Find the marginal distribution of Y. 

$$
f_Y(y) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) dx = \int_{-\infty}^{\infty} \frac{1}{2\pi} e^{\left(-\frac{x^2}{2} - \frac{(y - x)^2}{2}\right)} dx
$$
Simplifying the expression in the exponent, we have: 

$$
-\frac{x^2}{2} - \frac{(y - x)^2}{2} = -\frac{x^2}{2} - \frac{y^2 - 2xy + x^2}{2} = -\frac{y^2}{2} - \frac{1}{2} \left[ x^2 - 2xy \right] = -\frac{y^2}{2} - \frac{1}{2} (x - y)^2
$$

$$
f_Y(y) = \frac{1}{2\pi} e^{\left(-\frac{y^2}{2}\right)} \int_{-\infty}^{\infty} e^{\left(-\frac{(x - y)^2}{2}\right)} dx
$$
Looking at just the term(s) in the inside of the integral, we have: 

$$
\int_{-\infty}^{\infty} e^{\left(-\frac{(x - y)^2}{2}\right)} dx = \sqrt{2\pi}
$$

Such that we can evaluate the full expression now as: 

$$
f_Y(y) = \frac{1}{2\pi} e^{\left(-\frac{y^2}{2}\right)} \sqrt{2\pi} = \frac{1}{\sqrt{2\pi}} e^{\left(-\frac{y^2}{2}\right)}
$$
This pdf is the standard normal! Such that we say the marginal of Y given above describes the random variable Y as: 

$$
Y \sim N(0, 1)
$$


## (c) 

Consider random vector (V, W) where $W \sim N(0, 2)$ (normal with mean 0 and variance 2). How should the conditional distribution V | W = w be chosen so that (V, W) has the same distribution as (X, Y)? 

$$
\begin{pmatrix}
X \\
Y
\end{pmatrix}
\sim N\left(
\begin{pmatrix}
0 \\
0
\end{pmatrix},
\begin{pmatrix}
1 & 1 \\
1 & 2
\end{pmatrix}
\right)
$$

$$
\text{E}(V \mid W = w) = \frac{Cov(V, W)}{Var(W)} \times w = \frac{1}{2} w
$$
     
$$
Var(V | W = w) = Var(V) - \frac{Cov(V, W)^2}{Var(W)} = 1 - \frac{1^2}{2} = \frac{1}{2}
$$

$$
V \mid W = w \sim N\left(\frac{1}{2} w, \frac{1}{2}\right)
$$

$$
V \mid W = w \sim N\left(\frac{1}{2} w, \frac{1}{2}\right)
$$

\newpage

# Q2: 4.7 Casella & Berger

A woman leaves for work between 8AM and 8:30AM and takes between 40 and 50 minutes to get there. Let the random variable X denote her time of departure, and the random variable Y the travel time. Assuming that these variables are independent and uniformly distributed, find the probability the the woman arrives at work before 9AM. 

$$
X \sim U(0, 30), Y \sim U(40, 50)
$$

Given X and Y are both Uniformly distributed and independent, then:

$$
f_{X, Y}(x,y) = f_X(x)f_Y(y) = \frac{1}{30}(\frac{1}{50-40}) = \frac{1}{300}
$$

Define the random variable Z = X + Y, then we want to know the probability Z < 60 (1 hour, 60 minutes starting at the earliest time 8:00AM): 

$$
P(Z < 60) = P(X + Y < 60) = \int\limits_{40}^{50} \int\limits_{0}^{60-y} f_{X, Y}(x,y) dx dy = \int\limits_{40}^{50} \int\limits_{0}^{60-y} \frac{1}{300} dx dy = \int\limits_{40}^{50} \frac{60-y}{300}dy = -\frac{(y-120(y)}{600} \big|_{y=40}^{50}= \frac{1}{2}
$$

\newpage

# Q3: 4.10 Casella & Berger

The random pair (X, Y) has the distribution: 

```{r, echo=FALSE, fig.cap="Img", out.width = '100%'}
knitr::include_graphics("dist.png")
```

## (a) 

Show that X and Y are dependent. 

To show that two variables are not independent (are dependent), we need to show that the product of the marginal probabilities is not equal to joint probability. To that end: 

$$
P(X = 1) = \frac{1}{12} + \frac{1}{6} = \frac{3}{12} = \frac{1}{4}
$$

$$
P(Y = 4) = \frac{1}{3}
$$ 

$$
P(X = 1, Y = 4) = 0
$$

$$
P(X = 1, Y = 4) = 0 \neq \frac{1}{12} = \frac{1}{4}(\frac{1}{3}) = P(X = 1)P(Y = 4)
$$

## (b) 

Give a probability table for random variables U and V that have the same marginals as X and Y but are independent. 

We need probabilities of the rows to all be equal to one another. We need P(U = 1) = P(U = 3), and P(V = 2) = P(V = 3) = P(V = 4). To satisfy these conditions we have: 

|       |     U       |       |       |
|-------|------------------------|-------|-------|
|   V   | 1| 2 | 3 |
|-------|------------|------------|------------|
| 2     | 1/12       | 1/6        | 1/12       |
| 3     | 1/12       | 1/6        | 1/12       |
| 4     | 1/12       | 1/6        | 1/12       |

\newpage

# Q4 

Suppose X and Y are independent random variables, where both have the same (marginal) geometric(p) distribution for 0 < p < 1. Find the conditional distribution of X given X + Y = k (integer k $\geq 2$).

$$
P(X = x | S = k) = \frac{P(X = x,  S = k)}{P(S = k)}
$$

For the top part of the equation: 

$$
P(X = x, Y  = k - x) = P(X = x) P(Y = k - x)
$$
And:

$$
P(X = x) = (1 - p)^{x - 1} p 
$$

And: 

$$
P(Y = k - x) = (1 - p)^{k - x - 1} p
$$

Given X and Y are independent:

$$
P(X = x, S = k) = (1 - p)^{x - 1} p (1 - p)^{k - x - 1} p = P(X = x, S = k) = (1 - p)^{k - 2} p^2
$$

Consider then the bottom of the initial formula: 

$$
P(S = k) = \sum_{x=1}^{k-1} P(X =x, Y  = k - x) = (k - 1) (1 - p)^{k - 2} p^2
$$

Returning the overall, conditional formula, we have: 

$$
P(X = x | S = k) = \frac{P(X = x,  S = k)}{P(S = k)} = \frac{(1 - p)^{k - 2} p^2}{(k - 1) (1 - p)^{k - 2} p^2} = \frac{1}{k - 1}
$$

For x = 1, 2, ..., k - 1

We then conclude

$$
X | (X + Y = k) \sim U(1, k - 1)
$$

\newpage

# Q5: 4.27 Casella & Berger 

Let $X \sim n(\mu, \sigma^2)$, and let $Y \sim n(\gamma, \sigma^2)$. Suppose X and Y are independent. Define: U = X + Y and V = X - Y. Show that U and V are independent normal random variables. Find the distribution of each of them. 

$$
U \sim n(\mu + \gamma , 2\sigma^2)
$$

$$
V \sim n(\mu - \gamma, 2\sigma^2)
$$

For X and Y independent, we may write the joint pdf as the product of the marginals: 

$$
f_{X, Y}(x, y) = f_X(x) f_Y(f) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-(x-\mu)^2}{2\sigma^2}} \frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-(y-\gamma)^2}{2\sigma^2}} = \frac{1}{2\pi\sigma^2}e^{\frac{-\left((x-\mu)^2 + (y - \gamma)^2\right)}{2\sigma^2}}
$$
$$
U = X + Y and V = X - Y \rightarrow X = \frac{1}{2}(U + V), Y - \frac{1}{2}(U - V)
$$
We then have the Jacobian: 

$$
|J| = \left| \begin{array}{cc}
\frac{1}{2} & \frac{1}{2} \\
\frac{1}{2} & -\frac{1}{2}
\end{array} \right| = |- \frac{1}{4} - \frac{1}{4} | = |-\frac{1}{2}| = \frac{1}{2}
$$

And we have the new support of U, V as: 

$$
(u, v) \in (-\infty, \infty) \text{x} (-\infty, \infty)
$$

Using the above relations, we have: 

$$
f_{UV}(u, v) = \frac{1}{2 \pi \sigma^2} e^{-\frac{1}{2 \sigma^2} \left[ \left( \frac{u+v}{2} - \mu \right)^2 + \left( \frac{u-v}{2} - \gamma \right)^2 \right]} \frac{1}{2}
$$

$$
f_{UV}(u, v) = \frac{1}{4 \pi \sigma^2} e^{-\frac{1}{2 \sigma^2} \left[ 2 \left( \frac{u}{2} \right)^2 - u (\mu + \gamma) + \left( \frac{\mu + \gamma}{2} \right)^2 + 2 \left( \frac{v}{2} \right)^2 - v (\mu - \gamma) + \left( \frac{\mu - \gamma}{2} \right)^2 \right]}
$$

$$
f_{UV}(u, v) = g(u) \frac{1}{4 \pi \sigma^2} e^{-\frac{1}{2(2 \sigma^2)} \left( u - (\mu + \gamma) \right)^2} h(v) e^{-\frac{1}{2(2 \sigma^2)} \left( v - (\mu - \gamma) \right)^2}
$$
Such that we have shown: 

For: 

$$
f_{U}(u) = g(u) \frac{1}{4 \pi \sigma^2} e^{-\frac{1}{2(2 \sigma^2)} \left( u - (\mu + \gamma) \right)^2}
$$

And: 

$$
f_{V}(v) = h(v) e^{-\frac{1}{2(2 \sigma^2)} \left( v - (\mu - \gamma) \right)^2}
$$ 

$$
f_{UV}(u, v) = f_{U}(u) f_{V}(v)
$$
and we conclude that U and V are independent. 

\newpage

# Q6: 4.42 Casella & Berger  

Let X and Y be independent random variables with means $\mu_X, \mu_Y$ and variances $\sigma_X^2, \sigma_Y^2$. Find an expression for the correlation of XY and Y in terms of these means and variances. 

Said differently, we want to find: $\rho_{XY, Y}$

$$
\rho_{XY, Y} = \frac{Cov(XY, Y)}{\sigma_{XY} \sigma_Y} = \frac{E(XY^2) - \mu_X \mu_Y}{\sigma_{XY} \sigma_Y} = \frac{EX EY^2 - \mu_X \mu_Y \mu_Y}{\sigma_{XY} \sigma_Y}
$$

$$
\rho_{XY, Y} = \frac{\mu_{X} (\sigma_{Y}^2 + \mu_{Y}^2) - \mu_X \mu_Y \mu_Y}{\sigma_{XY} \sigma_Y} = \frac{\mu_{X} (\sigma_{Y}^2)}{\sigma_{XY} \sigma_Y}
$$

With note of X and Y being independent, we then have the denominator: 

$$
\sigma_{XY}^2 = E(XY)^2 - [E(XY)]^2 = EX^2 EY^2 - (EX)^2 (EY)^2 = (Var(X) + E[X]^2)(Var(Y) + E[Y]^2) - (EX)^2 (EY)^2
$$

Evaluating with known quantities, we have: 

$$
\sigma_{XY}^2 = (\sigma_X^2 + \mu_X^2)(\sigma_Y^2 + \mu_Y^2) - \mu_X^2 \mu_Y^2 = \sigma_X^2 \sigma_Y^2 + \sigma_X^2 \mu_Y^2 + \sigma_Y^2 \mu_X^2 + \mu_X^2 \mu_Y^2 - \mu_X^2 \mu_Y^2 = \sigma_X^2 \sigma_Y^2 + \sigma_X^2 \mu_Y^2 + \sigma_Y^2 \mu_X^2
$$

Substituting into the original equation gives us: 

$$
\rho_{XY, Y} = \frac{\mu_{X} (\sigma_{Y}^2)}{\sqrt{(\sigma_X^2 \sigma_Y^2 + \sigma_X^2 \mu_Y^2 + \sigma_Y^2 \mu_X^2)} \sigma_Y} = \frac{\mu_X \sigma_Y}{\sqrt{(\mu_X^2 \sigma_Y^2 + \mu_Y^2 \sigma_X^2 + \sigma_X^2 \sigma_Y^2)}}
$$

\newpage

# Q7 

Suppose that random variables $X_1, X_2, X_3$ have joint pdf $f(x_1, x_2, x_3) = 6$ for $0 < x_1 < x_2 < x_3 < 1$. 

## (a) 

Are $X_1, X_2, X_3$ independent? Give an intuitive explanation. 

No they are not independent, as knowing the value of one of the random variables limits the range of possible values that the other random variables can take. 

## (b) 

Find the marginal pdf of $X_2$ and identify the distribution of $X_2$ as a member of an important family of distributions. 

To find the marginal pdf of \( X_2 \), we need to integrate out \( X_1 \) and \( X_3 \) from the joint pdf \( f(x_1, x_2, x_3) = 6 \) for \( 0 < x_1 < x_2 < x_3 < 1 \).

The marginal pdf of \( X_2 \) is given by
\[
f_{X_2}(x_2) = \int_0^{x_2} \int_{x_2}^1 6 \, dx_3 \, dx_1.
\]

**Step 1: Integrate with respect to \( x_3 \)**
\[
\int_{x_2}^1 6 \, dx_3 = 6(1 - x_2).
\]

**Step 2: Integrate with respect to \( x_1 \)**
\[
f_{X_2}(x_2) = \int_0^{x_2} 6(1 - x_2) \, dx_1 = 6(1 - x_2) \int_0^{x_2} 1 \, dx_1 = 6(1 - x_2)x_2.
\]

**Marginal pdf of \( X_2 \):**
\[
f_{X_2}(x_2) = 6x_2(1 - x_2) \quad \text{for } 0 < x_2 < 1.
\]

### Identifying the Distribution
The form \( f_{X_2}(x_2) = 6x_2(1 - x_2) \) resembles the probability density function of a **Beta distribution** with parameters \( \alpha = 2 \) and \( \beta = 2 \).

Hence, \( X_2 \) follows a Beta distribution:
\[
X_2 \sim \text{Beta}(2, 2).
\]

## (c) 

Find the conditional pdf $f(x_1, x_3 | x_2)$ of $X_1, X_3 \text{ given } X_2 = x_2 \in (0,1)$.

To find the conditional pdf \( f(x_1, x_3 \mid x_2) \) of \( X_1 \) and \( X_3 \) given \( X_2 = x_2 \), we use the relation

\[
f(x_1, x_3 \mid x_2) = \frac{f(x_1, x_2, x_3)}{f_{X_2}(x_2)},
\]

where \( f(x_1, x_2, x_3) \) is the joint pdf and \( f_{X_2}(x_2) \) is the marginal pdf of \( X_2 \).

### Given Information
- The joint pdf: \( f(x_1, x_2, x_3) = 6 \) for \( 0 < x_1 < x_2 < x_3 < 1 \).
- The marginal pdf: \( f_{X_2}(x_2) = 6x_2(1 - x_2) \) for \( 0 < x_2 < 1 \).

### Conditional pdf Calculation
We focus on the range of \( x_1 \) and \( x_3 \) given \( x_2 \):
\[
0 < x_1 < x_2 \quad \text{and} \quad x_2 < x_3 < 1.
\]

The conditional pdf is given by
\[
f(x_1, x_3 \mid x_2) = \frac{f(x_1, x_2, x_3)}{f_{X_2}(x_2)} = \frac{6}{6x_2(1 - x_2)} = \frac{1}{x_2(1 - x_2)}.
\]

### Range of \( x_1 \) and \( x_3 \)
- \( 0 < x_1 < x_2 \)
- \( x_2 < x_3 < 1 \)

### Final Form
\[
f(x_1, x_3 \mid x_2) = \frac{1}{x_2 (1 - x_2)} \quad \text{for } 0 < x_1 < x_2 \text{ and } x_2 < x_3 < 1.
\]

## (d) 

Show that $X_1, X_3$ are independent given $X_2 = x_2 \in (0, 1)$ (i.e., think of what must be true of the conditional pdf of $f(x_1, x_3 | x_2)$ in this case). 

To show that \( X_1 \) and \( X_3 \) are independent given \( X_2 = x_2 \), we need to establish that the conditional joint pdf \( f(x_1, x_3 \mid x_2) \) can be written as a product of the marginal conditional pdfs of \( X_1 \) and \( X_3 \) given \( X_2 \).

### Step 1: Write the Conditional pdf
We have already found that the conditional pdf is given by
\[
f(x_1, x_3 \mid x_2) = \frac{1}{x_2 (1 - x_2)} \quad \text{for } 0 < x_1 < x_2 \text{ and } x_2 < x_3 < 1.
\]

### Step 2: Determine the Marginal Conditional pdfs
To determine the marginal conditional pdfs, we need to consider the distributions of \( X_1 \) and \( X_3 \) separately given \( X_2 = x_2 \):

1. **Marginal pdf of \( X_1 \mid X_2 = x_2 \)**
   \[
   f_{X_1 \mid X_2}(x_1 \mid x_2) = \frac{1}{x_2} \quad \text{for } 0 < x_1 < x_2.
   \]
   This comes from the fact that \( X_1 \) is uniformly distributed over \( (0, x_2) \).

2. **Marginal pdf of \( X_3 \mid X_2 = x_2 \)**
   \[
   f_{X_3 \mid X_2}(x_3 \mid x_2) = \frac{1}{1 - x_2} \quad \text{for } x_2 < x_3 < 1.
   \]
   This comes from the fact that \( X_3 \) is uniformly distributed over \( (x_2, 1) \).

### Step 3: Verify Independence
To prove that \( X_1 \) and \( X_3 \) are independent given \( X_2 = x_2 \), we check if
\[
f(x_1, x_3 \mid x_2) = f_{X_1 \mid X_2}(x_1 \mid x_2) \times f_{X_3 \mid X_2}(x_3 \mid x_2).
\]

Substituting the expressions:
\[
f_{X_1 \mid X_2}(x_1 \mid x_2) \times f_{X_3 \mid X_2}(x_3 \mid x_2) = \frac{1}{x_2} \times \frac{1}{1 - x_2} = \frac{1}{x_2 (1 - x_2)}.
\]

This matches \( f(x_1, x_3 \mid x_2) \), which implies that
\[
f(x_1, x_3 \mid x_2) = f_{X_1 \mid X_2}(x_1 \mid x_2) \times f_{X_3 \mid X_2}(x_3 \mid x_2).
\]

### Conclusion
Since the conditional joint pdf can be written as a product of the marginal conditional pdfs, \( X_1 \) and \( X_3 \) are independent given \( X_2 = x_2 \).

## (e) 

Find the covariance of $X_1 \text{ and } X_2$ given $X_2 = x_2$.

To find the covariance of \( X_1 \) and \( X_3 \) given \( X_2 = x_2 \), we use the definition of covariance:

\[
\text{Cov}(X_1, X_3 \mid X_2 = x_2) = \mathbb{E}[X_1 X_3 \mid X_2 = x_2] - \mathbb{E}[X_1 \mid X_2 = x_2] \mathbb{E}[X_3 \mid X_2 = x_2].
\]

### Step 1: Compute \( \mathbb{E}[X_1 \mid X_2 = x_2] \)
Given that \( X_1 \) is uniformly distributed over \( (0, x_2) \):
\[
\mathbb{E}[X_1 \mid X_2 = x_2] = \frac{0 + x_2}{2} = \frac{x_2}{2}.
\]

### Step 2: Compute \( \mathbb{E}[X_3 \mid X_2 = x_2] \)
Given that \( X_3 \) is uniformly distributed over \( (x_2, 1) \):
\[
\mathbb{E}[X_3 \mid X_2 = x_2] = \frac{x_2 + 1}{2}.
\]

### Step 3: Compute \( \mathbb{E}[X_1 X_3 \mid X_2 = x_2] \)
We use the conditional joint pdf \( f(x_1, x_3 \mid x_2) = \frac{1}{x_2(1 - x_2)} \) for \( 0 < x_1 < x_2 \) and \( x_2 < x_3 < 1 \):

\[
\mathbb{E}[X_1 X_3 \mid X_2 = x_2] = \iint_{0 < x_1 < x_2, x_2 < x_3 < 1} x_1 x_3 \cdot \frac{1}{x_2 (1 - x_2)} \, dx_3 \, dx_1.
\]

**Computing the integral:**

1. Integrate with respect to \( x_3 \):
   \[
   \int_{x_2}^1 x_3 \, dx_3 = \left[ \frac{x_3^2}{2} \right]_{x_2}^1 = \frac{1}{2} - \frac{x_2^2}{2}.
   \]

2. Integrate with respect to \( x_1 \):
   \[
   \int_0^{x_2} x_1 \, dx_1 = \left[ \frac{x_1^2}{2} \right]_0^{x_2} = \frac{x_2^2}{2}.
   \]

Combining these, we get
\[
\mathbb{E}[X_1 X_3 \mid X_2 = x_2] = \frac{1}{x_2 (1 - x_2)} \cdot \frac{x_2^2}{2} \left( \frac{1}{2} - \frac{x_2^2}{2} \right).
\]

Simplifying,
\[
\mathbb{E}[X_1 X_3 \mid X_2 = x_2] = \frac{x_2}{4} - \frac{x_2^3}{4}.
\]

### Step 4: Compute the Covariance
\[
\text{Cov}(X_1, X_3 \mid X_2 = x_2) = \left( \frac{x_2}{4} - \frac{x_2^3}{4} \right) - \left( \frac{x_2}{2} \right) \left( \frac{x_2 + 1}{2} \right).
\]

Simplifying:
\[
\text{Cov}(X_1, X_3 \mid X_2 = x_2) = \frac{x_2}{4} - \frac{x_2^3}{4} - \frac{x_2 (x_2 + 1)}{4}.
\]

Further simplification yields:
\[
\text{Cov}(X_1, X_3 \mid X_2 = x_2) = -\frac{x_2 (1 - x_2)}{12}.
\]

### Final Answer
\[
\text{Cov}(X_1, X_3 \mid X_2 = x_2) = -\frac{x_2 (1 - x_2)}{12}.
\]