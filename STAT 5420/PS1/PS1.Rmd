---
title: "HW1"
output:
  pdf_document: default
  html_document: default
date: "2024-09-07"
---

# Note 

Exercises start ~ pg 67 

Progress

  1. DONE 
    - CHECK STATUS: Does this proof work or can it be infinite?
  2. DONE \ 
  3. DONE \ 
  4. DONE \ 
  5. DONE \ 
  6. DONE \ 
  7. DONE 
  8. DONE \ 
  9. IN PROGRESS \ 

\newpage 

> 1. 1.12 It was noted in Section 1.2.1 that statisticans who follow the deFinetti school do not accept the Axiom of Countable Additivity, instead adhering to the Axiom of Finite Additivity. 

(a) 

Show that the Axiom of Countable Additivity implies Finite Additivity. 

(1) Definition of Axiom of Countable Additivity: 

If $A_1, A_2, ... \in \mathbb{B}$ are pairwise disjoint, then 

$P(\bigcup\limits_{i=1}^{\infty}{A_i}) = \sum\limits_{i=1}^{\infty}{P(A_i)}$

(2) Goal: 

We wish to prove the Axiom of Finity Additivity given Coutable Additivity, which is defined as: 

$A \in \mathbb{B}$, $B \in \mathbb{B}$ disjoint, then: 

$P(A \cup B) = P(A) + P(B)$

(3) Method: 

Define $A_1, A_2, ... \in \mathbb{B}$ as pairwise disjoint sets.  

Let $A, B \in \mathbb{B}$ and assume Countable Additivity holds (1). 

Define $A \equiv A_1$ and $B \equiv A_2 \cup A_3 \cup ...$

As $A_1, A_2, A_3, ...$ are pairwise disjoint, $A, B$ are disjoint sets.  

Thus, using Theorem 1.2.9, we have: 

1.2.9:  $P(A_1 \cup A_2 \cup ...) = P(A \cup B) = P(A) + P(B) - P(A \cap B)$

$P(A \cup B) = P(A) + P(B) - 0 = P(A) + P(B)$ .// 

\newpage 

> 2. 1.13: If $P(A) = \frac{1}{3}$ and $P(B^{c}) = \frac{1}{4}$, can A and B be disjoint?

A: 

$P(A) = \frac{1}{3} \rightarrow P(A^{c}) = 1 - \frac{1}{3} = \frac{2}{3}$
and
$P(B^{c}) = \frac{1}{4} \rightarrow P(B) = 1 - \frac{1}{4} = \frac{3}{4}$

Let us consider $P(A) + P(B) = \frac{1}{3} + \frac{3}{4} = \frac{4}{12} + \frac{9}{12} = \frac{13}{12}$  

For two events to be disjoint, then $P(A \cap B) = 0$, or they cannot occur jointly. Furthermore, the following condition must hold: 

$P(A \cup B) = P(A) + P(B)$

However, as shown, $P(A) + P(B) = \frac{13}{12} > 1$

However, the probability over the sample space of all possible events must be at most 1 (between 0 and 1). Thus, if we assume events A and B are disjoint, we arrive at a contradiction. 

> 3. 

Suppose a family has 4 children, named a, b, c and d, who take turns washing 4 plates denoted $p_1, p_2, p_3, p_4$. These children are not so careful in their work, so, over time, each of the plates will be broken. Suppose any child could break any plate and that the ways in which plates $p_1, p_2, p_3, p_4$ could be broken by children a, b, c, d are equally likely. 

We may frame this problem as a problem of urns and balls, where each child is an urn and each broken plate is a ball. 
So, framing these problems as such, (a) is the probability that 3 balls are put into one particular urn, and (b) is the probability that 3 balls are put into any one urn.

With this framework, we have the following relevant calculations for (a) and (b): 

(1): $4\choose{3}$, the number of events where one particular child breaks 3 plates. 
(2): $4\choose{1}$, the number of events where one particular child breaks 1 plate. 
(3): $\frac{1}{4}$, the probability that one particular child breaks a plate
(4): $\frac{3}{4}$, the probability that one particular child does not break a plate.

(a) Find the probability that child a breaks 3 plates. 

We have (1), (3), and (4) from the above. 

Because we are looking at the probability that a particular child (child a) breaks 3 plates we have: 

$P(\text{child a breaks 3 plates}) = {{4}\choose{1}} (\frac{1}{4})^3 (\frac{3}{4})^1 = 4(\frac{1}{64})(\frac{3}{4}) = \frac{3}{64}$

(b) Find the probability that one of the four children breaks 3 plates. 

We have (1), (2), (3), and (4) from the above. 

Because we are looking at the probability that any particular child breaks 3 plates, we have exactly 4 possible scenarios from part (a), or said differently we multiple the probability of part (a) by 4. 

$P(\text{child a breaks 3 plates}) = 4{{4}\choose{1}} (\frac{1}{4})^3 (\frac{3}{4})^1 = 16(\frac{1}{64})(\frac{3}{4}) = \frac{1}{4}(\frac{3}{4}) = \frac{3}{16}$

\newpage 

> 4. 1.34 

Two litters of a particular rodent species have been born, one with two brown-haired and one gray-haired (litter 1), and the other with three brown-haired and two gray-haired (litter 2). We select a litter at random and then select an  offspring at random from the selected litter. 

(a) What is the probability that the animal chosen is brown-haired? 

$P(\text{brown-haired}) = P(\text{brown-haired} | \text{litter 1})P(\text{litter1}) + P(\text{brown-haired} | \text{litter 2}) P(\text{litter 2})$

$P(\text{brown-haired}) = (\frac{2}{3})\frac{1}{2} + (\frac{3}{5})\frac{1}{2}$

$P(\text{brown-haired}) = \frac{2}{6} + \frac{3}{10} = \frac{10}{30} + \frac{9}{30}$

$P(\text{brown-haired}) = \frac{19}{30}$

(b) Give that a brown-haired offspring was selected, what is the probability that the sampling was from litter 1? 

$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$

where $P(A) = P(\text{litter 1}) = \frac{1}{2}$, 
$P(B) = P(\text{brown-haired}) = \frac{19}{30}$, and 
$P(B|A) = P(\text{brown-haired} | \text{litter 1}) = \frac{2}{3}$

$P(\text{litter 1} | \text{brown-haired}) = \frac{\frac{2}{3}\frac{1}{2}}{\frac{19}{30}}$

$P(\text{litter 1} | \text{brown-haired}) = \frac{\frac{2}{6}}{\frac{19}{30}}$

$P(\text{litter 1} | \text{brown-haired}) = \frac{1}{3} * \frac{30}{19}$

$P(\text{litter 1} | \text{brown-haired}) = \frac{10}{19}$

\newpage 

> 5. 1.38 

Prove each of the following statements. (Assume that any conditioning event has positive probability.)

(a) If $P(B) = 1$, then $P(A|B) = P(A)$ for any A. 

$P(B) = 1 \rightarrow P(B^{c}) = 1 - 1 = 0$

By definition: 
$P(A|B) = \frac{P(A \cap B)}{P(B)}$

So we find $P(A \cap B)$ through: 
$P (A \cap B^{c}) = P(A) - P(A \cap B)$,
$P(A \cap B) = P(A) - P (A \cap B^{c})$.

However, as $P(B^{c}) = 0$, 
$P(A \cap B) = P(A) - 0 = P(A)$

Giving us: 

$P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{P(A)}{P(B)} = \frac{P(A)}{1}$

Thus, for $P(B) = 1$, $P(A|B) = P(A)$.//

(b) If $A \subset B$, then $P(B|A) = 1$ and $P(A|B) = \frac{P(A)}{P(B)}$

By definition: 
$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$

Thus for $P(B|A) = 1$, we have: 

$P(A|B) = \frac{1*P(A)}{P(B)} = \frac{P(A)}{P(B)}$

(c) If A and B are mutually exclusive, then: 

$P(A | A  \cup B) = \frac{P(A)}{P(A) + P(B)}$

If A and B are mutually exclusive, then: 

$P(A \cup B) = P(A) + P(B)$

and $P(A \cap B) = 0$

So using the formula for conditional probability we have: 

$P(A|B) = \frac{P(A\cap B)}{P(B)} \rightarrow P(A | A \cup B) = \frac{P(A \cup (A \cap B))}{P(A \cup B)}$

Using Distributive Laws so have: 

$P(A \cup (A \cap B)) = P(A \cup A) \cap P(A \cup B) = P(A) \cap P(A \cup B) = P(A)$

So we have: 

$P(A | A \cup B) = \frac{P(A)}{P(A) + P(B)}$

(d) $P(A \cap B \cap C) = P(A | B \cup C) P(B | C)P(C)$

Base Formula: $P(A|B) = \frac{P(A\cap B)}{P(B)} \rightarrow P(A|B)P(B) = P(A \cap B)$

$P(A \cap B \cap C) = P(A \cap (B \cap C))$  

$P(A \cap B \cap C) = P(A|B \cap C)P(B \cap C)$ 

$P(A \cap B \cap C) = P(A | B \cap C)(P(B | C)P(C))$

\newpage 

> 6. 1.39 

A pair of events A and B cannot be simultaneously mutually exclusive and independent. Prove that $P(A) > 0$ and $P(B) > 0$ then: 

(1) If events A and B are mutually exclusive, then: $P(A \cap B) = 0$

(2) If events A and B are independent, then: $P(A | B) = P(A)$

(a) 

If A and B are mutually exclusive, they cannot be independent. 

Let A and B be mutually exclusive events. 

Let us then assume A and be are independent and $P(A) > 0$ and $P(B) > 0$. 

By their mutual exclusivity, (1) $P(A \cap B) = 0$
Let us then consider 
$P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{0}{P(B)} = 0$
This implies that $P(A|B) = 0$, 
However, due to independence, $P(A|B) = P(A) > 0$. 

We thus have a contradiction via our assumption of independence. 

(b)

If A and B are independent, they cannot be mutually exclusive. 

Let A and B now be independent events.

Furthermore define $P(A) > 0$ and $P(B) > 0$ and assume A and B are mutually exclusive. 

By independence (2), $P(A|B) = P(A)$ and $P(A \cap B) = P(A) P(B)$. 

However, by presuming mutual exclusivity (1), we have 

$P(A \cap B) = 0 \rightarrow P(A)P(B) = 0$

However, this requires $P(A), P(B)$, or both to be zero! 
Thus we're reached a contradiction from our presumption of mutual exclusivity. 

\newpage 

> 7. 1.47 Prove that the following functions are cdfs. 

To prove a function is a (valid, proper) cdf, for our purposes we must verify: 

(1): $\lim_{x \rightarrow -\infty}{F(x) = 0}$
(2): $\lim_{x \rightarrow \infty} F(x) = 1$
(3): $F(x)$ is a nondecreasing function of x, i.e. $\frac{d}{dx}F(x) > 0$

>> 1.47 (c)

$F_{X}(x) = e^{-e^{-x}}$ for $x \in (-\infty, \infty)$

(1): $\lim_{x \rightarrow -\infty}{F(x)} = \lim_{x \rightarrow -\infty} e^{-e^{-x}} = e^{-\infty} = 0$
Note: $e^{-\infty} = \frac{1}{e^{\infty}} = \frac{1}{\infty} = 0$

So we conclude that $\lim_{x \rightarrow -\infty}{F(x)} = 0$

(2): $\lim_{x \rightarrow \infty}{F(x)} =  e^{-e^{-x}} = e^{0} = 1$

So we conclude that $\lim_{x \rightarrow \infty}{F(x)} = 1$

(3): $\frac{d(e^{-e^{-x}})}{dx}= e^{-x - e^{-x}}$
Note: For $x \in (-\infty, \infty)$, $e^{-x - e^{-x}} > 0$, 
So we conclude: $\frac{d}{dx}F(x) > 0$

>> 1.47 (d) 

$F_{X}(x) = 1 - e^{-x}$ for $x \in (0, \infty)$

(1): For, $x \in (0, \infty)$, 
$\lim_{x \rightarrow -\infty}{F(x)} = \lim_{x \rightarrow 0}{F(x)} = \lim_{x \rightarrow 0} {1 - e^{-x}} = \lim_{x \rightarrow 0} 1 - \frac{1}{e^x} = 1 - 1 = 0$

So we conclude that $\lim_{x \rightarrow -\infty}{F(x)} = 0$

(2): $\lim_{x \rightarrow \infty} = \lim_{x \rightarrow -\infty} {1 - e^{-x}} = 1 - 0$

So we conclude that $\lim_{x \rightarrow \infty}{F(x)} = 1$

(3): $\frac{d(1 - e^{-x})}{dx}= 0 - (-e^{-x}) = e^{-x}$
Note: For $x \in (0, \infty)$, $e^{-x} > 0$ so this condition is satisfied. 

So we conclude $\frac{d}{dx}F(x) > 0$

\newpage 

> 8. 1.54 

For each of the following, determine the value of c that makes $f(x)$ a pdf. 

To prove a function is a (valid, proper) pdf (for the continuous cases) we must verify: 

(1): $f_{X}(x) \geq 0$ for all x. 

(2): $\int\limits_{-\infty}^{\infty}{f_{X}(x)dx = 1}$

>> (a)

$f(x) = c * sin(x)$ for $0 < x < \frac{\pi}{2}$

$\int c*sin(x) dx = c \int sin(x) dx = c (-cos(x))$

So evaluating over the range we have: 

$\int\limits_{0}^{\frac{\pi}{2}} c*sin(x) dx = c * (-cos(x)\big|_{0}^{\frac{\pi}{2}})$

$\int\limits_{0}^{\frac{\pi}{2}} c*sin(x) dx = c(-cos(\frac{\pi}{2}) + cos(0)) = c (1) = c$

So for the integral to evaluate to 1 over the support, we have 

$c = 1$

>> (b) 

$f(x) = c*e^{-|x|}$ for $-\infty < x < \infty$

As the absolute value does not have a defined derivative, we must separate $f(x)$ into two distinct integrals. 

$\int\limits_{-\infty}^{\infty}{e^{-|x|}} = \int\limits_{-\infty}^{0}{e^{x}}  + \int\limits_{0}^{\infty}{e^{-x}}$

So we have: 

(1): $\int e^x dx = e^{x}$

(2): $\int e^-x dx = -e^{-x}$

Over the respective range of x values, we may evaluate: 

(1): $\int\limits_{-\infty}^{0}{e^{x}} = e^{x} \big|_{-\infty}^{0} = 1 - 0 = 1$

(2): $\int\limits_{0}^{\infty}{e^{-x}} = -e^{-x}\big|_{0}^{\infty} = 0 - (-1) = 1$

Combining (1) and (2) we have: 

$\int\limits_{-\infty}^{\infty}{e^{-|x|}} = 1 + 1 = 2$

So if we include the constant c (which can be factored out of the integral), we have: 

$\int\limits_{-\infty}^{\infty}c * {e^{-|x|}} = c * \int\limits_{-\infty}^{\infty} {e^{-|x|}}  = c(1 + 1) = c(2)$

To have this evaluate to 1, we must set $c = \frac{1}{2}$

\newpage 

> 9. 

From the axioms of probability, it follows that probability functions $P(\cdot)$ exhibit "monotone continuity from above (mcfa)" (which you don't have to worry about showing), meaning that for any decreasing sequence of sets/events  

$A_1 \supset A_2 \supset A_3 \supset \space ...$, 

$$\lim_{n \rightarrow \infty}{P(A_n)} = P(\bigcap\limits_{i=1}^{\infty}A_i)$$

By using/applying the mcfa property, show that the cdf $F$ of a random variable $X$ must be right continuous for any $x \in \mathbb{R}$, 

$\lim_{n \rightarrow \infty}{F(x + n^{-1})} = F(x)$

holds. 

>> A: 

Notes: https://math.stackexchange.com/questions/480042/prove-that-the-cdf-of-a-random-variable-is-always-right-continuous

Let $(x_n)$ be a sequence such that $x_n \to y$ and $x_n \geq y$ for every $n$.

Furthermore, let $A_n = (-\infty,x_n]$ $\forall n$. 
Now from the sequence $(x_n)$ we can extract a monotonically decreasing subsequence, say $(x_{n_k})$. 
As the new subsequence is monotonically decreasing, it follows from our definition of $A_n$'s that for all $k$ in $\mathbb{N}$, $A_{n_{k+1}} \subseteq A_{n_k}$. 
So $X^{-1}(A_{n_{k+1}}) \subseteq X^{-1}(A_{n_k})$, and 
$$F_X(x_{n_{k+1}}) = P(X\leq x_{n_{k+1}}) = P(X^{-1}(A_{n_{k+1}}))$$
And
$$P(X^{-1}(A_{n_{k+1}})) \leq P(X^{-1}(A_{n_{k}})) = P(X\leq x_{n_{k}}) = F_X(x_{n_{k}})$$

Now since $(X^{-1}(A_{n_k}))_k$ is a nested sequence of events, we know 
$$\lim_{k\to\infty}P(X^{-1}(A_{n_k})) = P(\cap_{k=1}^{\infty}(X^{-1}(A_{n_k})))$$
$$\lim_{k\to\infty}P(X^{-1}(A_{n_k})) = P(X^{-1}(\cap_{k=1}^{\infty}A_{n_k}))$$

$$\lim_{k\to\infty}P(X^{-1}(A_{n_k})) = P(X^{-1}(-\infty,y])$$

And we conclude: 
$$\lim_{k\to\infty}F_X(x_{n_k}) = P(X\leq y) = F_X(y).$$

>> Alt:

Lemma: If a sequence of events ${\{ A_n\}}$ is decreasing, and $n \geq 1$, 
then 
$$\lim_{n \rightarrow \infty}{(A_n)} = (\bigcap\limits_{i=1}^{\infty}A_i)$$
and their associated probabilities 
$$\lim_{n \rightarrow \infty}{P(A_n)} = P(\bigcap\limits_{i=1}^{\infty}A_i)$$

The cdf $F$ is right continuous at some point a iff $\forall n \geq 1, {\{ x_n\}}$, such that $x_n \downarrow a$, gives $F(x_n) \downarrow F(a)$

Let the events $A_n$ be defined as follows: 

$A_n = {\{ \omega: X(\omega) \leq x_n\}}\space \forall n \geq 1$

We need to prove that: 

$\bigcap\limits_{i=1}^{\infty}A_i = {\{ \omega: X(\omega) \leq a\}}$

One Direction: 
if $X(\omega) \leq x_n \space \forall n \geq 1$ 
since $x_n \downarrow a$, $X(\omega) \leq a$

Other Direction: 
If $X(\omega) \leq a$, as $a \leq x_n \space \forall n \geq 1$
Giving us $X(\omega) \leq x_n \space\forall n \geq 1$

End Result: 

$F(X_n) = P(X \leq x_n) = P(A_n) \downarrow P(\bigcap\limits_{i=1}^{\infty}A_i) = P(X \leq a) = F(a)$

>> Note

$\downarrow$ means a decrease or a negaative change in a value or function, or "approaching from the right" 

>> My Attempt: 

Let us take the mcfa property as a given. Let us then define the cdf $F$ of a random variable $X$. 

Let us then define a decreasing sequence of sets/events, ${\{ A_n\}}$ with $n \geq 1$. 

By the mcfa property, we know:
$$\lim_{n \rightarrow \infty}{P(A_n)} = P(\bigcap\limits_{i=1}^{\infty}A_i)$$

