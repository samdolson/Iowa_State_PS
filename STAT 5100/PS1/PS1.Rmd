---
title: "Stat 5100 Assignment 1"
author: "Samuel Olson"
date: "Due: Wednesday, January 29th 11:59PM in Gradescope"
output: pdf_document
---

# Outline
  
  - Q1: DONE
  - Q2: DONE 
  - Q3: DONE
  - Q4: DONE, Lingering Q's on f), g) regarding commutivity 
  - Q5: WIP, just part e) 
  - Q6: DONE
  - Q7: DONE, Lingering Q's on the role of the zero vector in the proof 
  - Q8: DONE

# Instructions

- Please note that a template for the assignment will be provided, but its use is not required.
- When submitting your assignment in Gradescope, ensure that every problem set (3â€“8) starts on a new page.

# Purpose

The main purpose of this assignment is to familiarize yourself with the Preliminary Knowledge on Linear Algebra and Statistics posted for Lecture 1. Aside from Question 2, all questions are related to Linear Algebra. Questions on statistical concepts will follow on Homework 2.

\newpage

# Problem 1

Search the online catalog of Parks Library for a Linear Algebra book specifically for Statistics. I found at least one that is available online through your ISU account. Feel free to search elsewhere. I am not asking you to purchase any books, but I want you to have access to at least one as a resource.

Understood! 

\newpage

# Problem 2

Read through the notes posted for Lecture 1 (15-page document). Post any questions you have on the discussion board in the designated space. Grant and I, or your peers, will answer your questions.

Understood! 

\newpage

# Problem 3

Let $\mathbf{A}$ be an $m \times m$ idempotent matrix. Show that:

a) $\underset{m \times m}{\mathbf{I}} - \mathbf{A}$ is idempotent.

Definition of idempotent: $$\mathbf{A}\mathbf{A} = \mathbf{A}$$

Let $\mathbf{B} = \mathbf{I} - \mathbf{A}$. Then:

$$\mathbf{B}\mathbf{B} = (\mathbf{I} - \mathbf{A})^2 = \mathbf{B}^2 = \mathbf{I}^2 - 2\mathbf{I}\mathbf{A} + \mathbf{A}$$

Note the identity matrix, $\mathbf{I}$, is also idempotent, such that we may simplify, noting our initial assumption of $\mathbf{A}$ is idempotent: 

$$(\mathbf{I} - \mathbf{A})(\mathbf{I} - \mathbf{A}) = \mathbf{B}\mathbf{B} = \mathbf{I} - 2\mathbf{A} + \mathbf{A} = \mathbf{I} - \mathbf{A}$$

And we conclude that $\mathbf{I} - \mathbf{A}$ is idempotent.

b) $\mathbf{B A B}^{-1}$ is idempotent, where $\mathbf{B}$ is any $m \times m$ nonsingular matrix.

Our goal is to to show:

$$(\mathbf{B A B}^{-1})(\mathbf{B A B}^{-1}) = \mathbf{B A B}^{-1}$$

Let us start by assuming that the matrices $\mathbf{A}$ and $\mathbf{B}$ are compatible matrices. 

Noting associativity of matrix multiplication, we have:

$$(\mathbf{B A B}^{-1})(\mathbf{B A B}^{-1}) = \mathbf{B A (\mathbf{B}^{-1} \mathbf{B}) A B}^{-1}$$

By the definition of an inverse matrix, and given our assumption that $\mathbf{B}$ is a nonsingular matrix, $\mathbf{B}^{-1} \mathbf{B} = \mathbf{I}$:

$$(\mathbf{B A B}^{-1})(\mathbf{B A B}^{-1}) = \mathbf{B A (\mathbf{I}) A B}^{-1} = \mathbf{B A A B}^{-1}$$

Then with note of $\mathbf{A}$ being idempotent, we have: 

$$(\mathbf{B A B}^{-1})(\mathbf{B A B}^{-1}) = \mathbf{B A B}^{-1}$$

And we conclude that $\mathbf{B A B}^{-1}$ is idempotent.

\newpage

# Problem 4

A matrix $\mathbf{A}$ is symmetric if $\mathbf{A} = \mathbf{A}^\top$. Determine the truth of the following statements:

a) If $\mathbf{A}$ and $\mathbf{B}$ are symmetric, then their product $\mathbf{AB}$ is symmetric.

Let  

$$\mathbf{A} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}, \text{ and } \mathbf{B} = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}.$$

Note, both $\mathbf{A}$ and $\mathbf{B}$ are symmetric.

But, 

$$\mathbf{AB} = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}, \text{ and } (\mathbf{AB})^\top = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$$

Such that, as defined, $\mathbf{AB} \neq (\mathbf{AB})^\top$

As we have identified a counterexample, the statement given is false. 

b) If $\mathbf{A}$ is not symmetric, then $\mathbf{A}^{-1}$ is not symmetric.

Given the definition of an inverse, we have: 

$$
\mathbf{A} \mathbf{A}^{-1} = \mathbf{I}
$$

From the property of transposes, we then may write:

$$
(\mathbf{A} \mathbf{A}^{-1})^\top = \mathbf{I}^\top
$$

Assuming conformal for post-multiplication, we may write this: 

$$
(\mathbf{A}^{-1})^\top (\mathbf{A}^\top) = \mathbf{I}
$$

This implies that:

$$
(\mathbf{A}^\top)^{-1} = (\mathbf{A}^{-1})^\top
$$

Which we will then turn to investigate. To that end, 

Let us consider: If $\mathbf{A}^{-1}$ were symmetric, then clearly:

$$
\mathbf{A}^{-1} = (\mathbf{A}^{-1})^\top
$$

However, if we assume that $\mathbf{A}$ is **not** symmetric, which means $\mathbf{A} \neq \mathbf{A}^\top$, then it would still follow from the above relation that: 

$$
(\mathbf{A}^\top)^{-1} = \mathbf{A}^{-1}
$$

If we then apply the inverse (or take the inverse of both sides) of the above relation, with note that $(\mathbf{A}^{-1})^{-1} = A$, we would then have: 

$$\mathbf{A} = \mathbf{A}^\top$$

However, this would be a contradiction! This means that if $\mathbf{A}$ is not symmetric, then $\mathbf{A}^{-1}$ cannot be symmetric. This means that the statement is true. 

c) When $\mathbf{A, B, C}$ are symmetric, the transpose of $\mathbf{ABC}$ is $\mathbf{CBA}$.

Using the transpose property:

$$(\mathbf{ABC})^\top = \mathbf{C}^\top \mathbf{B}^\top \mathbf{A}^\top$$

Let $\mathbf{D} = \mathbf{AB}$, such that we may write the above as: 

$$(\mathbf{ABC})^\top = (\mathbf{DC})^\top$$

Then via our typical matrix arithmetic of transposes, we have: 

$$(\mathbf{DC})^\top = \mathbf{C}^{\top} \mathbf{D}^{\top}$$

Simplifying further we have: 

Since $\mathbf{A}, \mathbf{B}, \mathbf{C}$ are symmetric, this simplifies to:

$$(\mathbf{ABC})^\top = \mathbf{C}^{\top} (\mathbf{AB})^{\top} = \mathbf{C}^{\top} \mathbf{B}^{\top} \mathbf{A}^{\top}$$

However, as the matrices are all respectively symmetric, we then have: 

$$(\mathbf{ABC})^\top = \mathbf{C}^{\top} \mathbf{B}^{\top} \mathbf{A}^{\top} = \mathbf{C}^{} \mathbf{B}^{} \mathbf{A}^{}$$

And the original statement is indeed true. 

## Section Break 
If $\mathbf{A} = \mathbf{A}^\top$ and $\mathbf{B} = \mathbf{B}^\top$, which of these matrices are certainly symmetric?

Again, for each of the following we will assume necessarily that all matrices involved are compatible for the purposes of matrix multiplication. 

d) $\mathbf{A}^2 - \mathbf{B}^2$:  

Note the properties of summing/subtracting two matrices, and the property that $\mathbf{A}$ and $\mathbf{B}$ being symmetric implies their square (multiplied by itself) is also symmetric: 

$$(\mathbf{A}^2 - \mathbf{B}^2)^\top = (\mathbf{A}^2)^{\top} - (\mathbf{B}^2)^\top = \mathbf{A}^2 - \mathbf{B}^2$$

So we conclude that this matrix is certainly symmetric.  

e) $\mathbf{ABA}$:  

With note of the results of the above problem, part c), we may simplify this as: 

$$(\mathbf{ABA})^\top = \mathbf{A}^\top \mathbf{B}^\top \mathbf{A}^\top = \mathbf{ABA}$$

And with note of the symmetry of matrices $\mathbf{A}$ and $\mathbf{B}$, 

we conclude that this matrix is certainly symmetric.  

f) $\mathbf{ABAB}$:  

Again with note of the results of the above problem, part c), we may extend these results and write: 

$$(\mathbf{ABAB})^\top = \mathbf{B}^\top \mathbf{A}^\top \mathbf{B}^\top \mathbf{A}^\top = \mathbf{BABA}$$

However, to say that 

$$(\mathbf{ABAB})^\top = \mathbf{BABA} = \mathbf{ABAB}$$ and conclude this matrix is certainly symmetric, we would require that the matrices $\mathbf{A}$ and $\mathbf{B}$ are commutative, which we do not have a guarantee of. So we cannot conclude this matrix is certainly symmetric. 

g) $(\mathbf{A} + \mathbf{B})(\mathbf{A} - \mathbf{B})$:  

$$(\mathbf{A} + \mathbf{B})(\mathbf{A} - \mathbf{B}) = \mathbf{A}^2 + \mathbf{BA} - \mathbf{AB} + \mathbf{B}^2$$ 

And: 

$$\left((\mathbf{A} + \mathbf{B})(\mathbf{A} - \mathbf{B})\right)^{\top} = \left( \mathbf{A}^2 + \mathbf{BA} - \mathbf{AB} + \mathbf{B}^2 \right) ^{\top} = (\mathbf{A}^2)^{\top} + (\mathbf{BA})^{\top} - (\mathbf{AB})^{\top} + (\mathbf{B}^2) ^{\top}$$ 

However, to say that: 

$$\mathbf{A}^2 + \mathbf{BA} - \mathbf{AB} + \mathbf{B}^2 = (\mathbf{A}^2)^{\top} + (\mathbf{BA})^{\top} - (\mathbf{AB})^{\top} + (\mathbf{B}^2) ^{\top}$$

Which is to say: 

$$(\mathbf{A} + \mathbf{B})(\mathbf{A} - \mathbf{B}) = \left((\mathbf{A} + \mathbf{B})(\mathbf{A} - \mathbf{B})\right)^{\top}$$

and conclude this matrix is certainly symmetric, we would require that the matrices $\mathbf{A}$ and $\mathbf{B}$ are commutative, such that $\mathbf{AB} = \mathbf{BA} \rightarrow (\mathbf{AB})^{\top} = (\mathbf{BA})^{\top}$

However, we do not have a guarantee or presumption of commutivity, so we cannot conclude this matrix is certainly symmetric. 

\newpage

# Problem 5

Consider the matrix 

$$
\mathbf{X} = \begin{bmatrix}
1 & -3 & 0 & -3 \\
1 & -2 & -1 & 2 \\
2 & -5 & -1 & -1
\end{bmatrix}
$$

a) Show that the columns of $\mathbf{X}$ are linearly dependent.

To prove linear dependence, we must find some $\mathbf{a} \in \mathbb{R}^4$ that satisfies the following relation:

$$
\mathbf{X} \mathbf{a} = \sum_{i=1}^4 a_i \mathbf{x}_i = 0
$$

where $a_i$ is the $i$-th element of $\mathbf{a}$. 

We have the following system of equations: 

$$
\begin{cases}
a_1 1 + a_2 (-3) + a_3 (0) + a_4 (-3) = 0, \\
a_1 1 + a_2 (-2) + a_3 (-1) + a_4 2 = 0, \\
a_1 2 + a_2 (-5) + a_3 (-1) + a_4 (-1) = 0
\end{cases}
$$

Solving this system yields:

$$
a_1 = -12t + 3s, \text{ } a_2 = -5t + s, \text{ } a_3 = s, \text{ and } a_4 = t
$$

where $s, t \in \mathbb{R}$ (some real-valued scalars). 

Then, for the above, if we set $s = 0, t = 1$, 

the associated solution for $\mathbf{a}$ is: 

$$
\mathbf{a} = \begin{bmatrix} -12 \\ -5 \\ 0 \\ 1 \end{bmatrix}
$$

Which we may write as: 

$$
-12 \mathbf{x}_1 -5 \mathbf{x}_2 + 0 \mathbf{x}_3 + 1 \mathbf{x}_4 = \mathbf{0}
$$

b) Find the rank of $\mathbf{X}$.

Via row reduction of $\mathbf{X}$, it follows:

$$
\mathbf{X} =
\begin{bmatrix}
1 & -3 & 0 & -3 \\
1 & -2 & -1 & 2 \\
2 & -5 & -1 & -1
\end{bmatrix}
\rightarrow
\begin{bmatrix}
1 & -3 & 0 & -3 \\
0 & 1 & -1 & 5 \\
0 & 1 & -1 & 5
\end{bmatrix}
\rightarrow
\begin{bmatrix}
1 & 0 & -3 & 12 \\
0 & 1 & -1 & 5 \\
0 & 0 & 0 & 0
\end{bmatrix}
$$

Since rank is the maximum number of linearly independent rows or columns of the matrix $\mathbf{X}$, is follows that the rank of $\mathbf{X}$ is 2. 

c) Use the generalized inverse algorithm in Slide Set 1 to find a generalized inverse of $\mathbf{X}$.

(1): Find any $n \times n$ nonsingular submatrix of $\mathbf{X}$, where $n = \text{rank}(\mathbf{X}) = 2$ and call if $\mathbf{W}$. 

$$
W =
\begin{bmatrix}
x_{11} & x_{12} \\
x_{21} & x_{22}
\end{bmatrix}
=
\begin{bmatrix}
1 & -3 \\
1 & -2
\end{bmatrix}
$$

To verify $\mathbf{W}$ is nonsingular, I calculated: 

$det(\mathbf{W}) = 1$, which is nonsingular (not zero). 

(2): Invert and transpose $\mathbf{W}$, i.e. compute $(W^{-1})^\top$:

$$
W^{-1} =
\begin{bmatrix}
-2 & 3 \\
-1 & 1
\end{bmatrix}
$$

$$
(W^{-1})^\top =
\begin{bmatrix}
-2 & -1 \\
3 & 1
\end{bmatrix}
$$

(3): Replace the elements of $W$ in $\mathbf{X}$ with the corresponding elements of $(W^{-1})^\top$. Then:

$$
\mathbf{X} =
\begin{bmatrix}
-2 & -1 & 0 & -3 \\
3 & 1 & -1 & 2 \\
2 & -5 & -1 & -1
\end{bmatrix}
$$

(4): Replace all other elements in $\mathbf{X}$ with zeros:

$$
\mathbf{X} =
\begin{bmatrix}
-2 & -1 & 0 & 0 \\
3 & 1 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix}
$$

(5): Transpose the matrix to obtain $\mathbf{G}$, a generalized inverse of $\mathbf{X}$:

$$
\mathbf{G} =
\begin{bmatrix}
-2 & 3 & 0 \\
-1 & 1 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix}
$$

d) Use the R function `ginv` in the `MASS` package to find a generalized inverse of $\mathbf{X}$. 

- To load the `MASS` package into your R workspace, use the command `library(MASS)`. 
- If the `MASS` package is not already installed, use `install.packages("MASS")` to install it.

```{r}
library(MASS)
X <- matrix(c(1,1,2,
              -3,-2,-5,
              0,-1,-1,
              -3,2,-1), ncol = 4)
massX <- MASS::ginv(X)
massX
```

e) Provide one matrix $\mathbf{X}^*$ that satisfies both of the following characteristics:
   - $\mathbf{X}^*$ has full-column rank.
   - $\mathbf{X}^*$ has column space equal to the column space of $\mathbf{X}$.

The rank of $\mathbf{X}$ is 2 (from part (b)). 

Since $\mathbf{x}_1$ and $\mathbf{x}_3$ are linearly independent, and $\mathbf{x}_2$ and $\mathbf{x}_4$ can be generated by linear combinations of $\mathbf{x}_1$ and $\mathbf{x}_3$ (i.e., $\mathbf{x}_2 = 3 \cdot \mathbf{x}_1 + \mathbf{x}_3$ and $\mathbf{x}_4 = -3 \cdot \mathbf{x}_1 - 5 \cdot \mathbf{x}_3$), we have:

$$
C([\mathbf{x}_1, \mathbf{x}_3]) = C([\mathbf{x}_1, \mathbf{x}_2, \mathbf{x}_3, \mathbf{x}_4])
$$

Thus, we can construct:

$$
\mathbf{X}^* =
\begin{bmatrix}
1 & 0 \\
1 & -1 \\
2 & -1
\end{bmatrix}
$$

Note that we can pick any two linearly independent columns of $\mathbf{X}$ to form $\mathbf{X}^*$. It is not necessary for $\mathbf{X}^*$ to include columns of $\mathbf{X}$. For example, $\mathbf{X}^*$ could also be:

$$
\mathbf{X}^* =
\begin{bmatrix}
1 & 0 \\
0 & 1 \\
1 & 1
\end{bmatrix}
$$

Any column of $\mathbf{X}^*$ can be written as a linear combination of the columns of $\mathbf{X}$, and any column of $\mathbf{X}$ can be written as a linear combination of the columns of $\mathbf{X}^*$. Thus:

$$
C(\mathbf{X}) = C(\mathbf{X}^*)
$$

\newpage

# Problem 6

Prove the following result:

Suppose the set of $m \times 1$ vectors $\mathbf{x}_1, \ldots,\mathbf{x}_n$ is a basis for the vector space $\mathcal{S}$. Then any vector $\mathbf{x} \in \mathcal{S}$ has a unique representation as a linear combination of the vectors $\mathbf{x}_1, \ldots,\mathbf{x}_n$.

Since $\mathbf{x}_1, \ldots, \mathbf{x}_n$ is a basis for $\mathcal{S}$, we know:

(1): The vectors $\mathbf{x}_1, \ldots, \mathbf{x}_n$ are linearly independent.

(2): The span of $\mathbf{x}_1, \ldots, \mathbf{x}_n$ equals $\mathcal{S}$, written: 

$$
\mathcal{S} = \text{span}\{\mathbf{x}_1, \ldots, \mathbf{x}_n\}
$$

Bearing the above in mind, let $\mathbf{x} \in \mathcal{S}$. 

By definition, $\mathbf{x}$ can be written as a linear combination of $\mathbf{x}_1, \ldots, \mathbf{x}_n$ (the vector space generated by $\mathbf{x}_1, \ldots, \mathbf{x}_n$):

$$
\mathbf{x} = \sum_{i=1}^{n} c_i\mathbf{x}_i
$$

For some $c_1, ..., c_n \in \mathbb{R}$. 

Suppose there exists another representation of $\mathbf{x}$:

$$
\mathbf{x} = \sum_{i=1}^{n} d_i\mathbf{x}_i
$$

For some $d_1, ..., d_n \in \mathbb{R}$. 

Then by subtracting the two, we have: 

$$
\sum_{i=1}^{n} (c_i\mathbf{x}_i) - (d_i\mathbf{x}_i) = \sum_{i=1}^{n} (c_i- d_i)\mathbf{x}_i = \mathbf{x} - \mathbf{x} = \mathbf{0}
$$

However, as $\mathbf{x}_1, \ldots, \mathbf{x}_n$ are linearly independent of one another, the only solution to this equation is:

$$
(c_i- d_i) = 0 \text{, } \forall i
$$

Which is to say, $\forall i$, $c_i- d_i$, implying uniqueness. 

Therefore, the representation of $\mathbf{x}$ as a linear combination of $\mathbf{x}_1, \ldots, \mathbf{x}_n$ is unique.

\newpage

# Problem 7

Am I a vector space? (The basic question here is whether every linear combination is in the space. If there is no zero, then Iâ€™m for sure not a vector space.)

a) All vectors in $\mathbb{R}^n$ whose entries sum to 0.

Let $\mathbf{v} \in \mathbb{R}^n$ satisfy $\sum_{i=1}^n v_i = 0$, and let $\mathbf{w} \in \mathbb{R}^n$ satisfy $\sum_{i=1}^n w_i = 0$. 

We then consider a linear combination:

$$
\mathbf{u} = a \mathbf{v} + b \mathbf{w}
$$

where $a, b \in \mathbb{R}$ (some real-valued scalars). 

It follows then, that:

$$
\sum_{i=1}^n u_i = \sum_{i=1}^n (a v_i + b w_i) = a \sum_{i=1}^n v_i + b \sum_{i=1}^n w_i = a(0) + b(0) = 0
$$

Thus, $\mathbf{u} \in \mathbb{R}^n$ also satisfies $\sum_{i=1}^n u_i = 0$, so the set is closed under linear combinations, and this set is a vector space (as the set of all vectors in $\mathbb{R}^n$ whose entries sum to 0 is a vector space).

Additionally, the zero vector $\mathbf{0} \in \mathbb{R}^n$ also satisfies $\sum_{i=1}^n 0 = 0$, so the set contains the zero vector.

b) All matrices in $\mathbb{R}^{m \times n}$ whose entries, when squared, sum to 1.

Define matrices as follows: $\mathbf{A}, \mathbf{B} \in \mathbb{R}^{m \times n}$, which satisfy:

$$
\sum_{i=1}^m \sum_{j=1}^n A_{ij}^2 = 1 \text{ and } \sum_{i=1}^m \sum_{j=1}^n B_{ij}^2 = 1
$$

Let us then consider a linear combination:

$$
\mathbf{C} = a \mathbf{A} + b \mathbf{B}
$$

where $a, b \in \mathbb{R}$, again some real-valued scalars. 

It then follows that:

$$
\sum_{i=1}^m \sum_{j=1}^n C_{ij}^2 = \sum_{i=1}^m \sum_{j=1}^n (a A_{ij} + b B_{ij})^2 = a^2 \sum_{i=1}^m \sum_{j=1}^n A_{ij}^2 + b^2 \sum_{i=1}^m \sum_{j=1}^n B_{ij}^2 + 2ab \sum_{i=1}^m \sum_{j=1}^n A_{ij} B_{ij}
$$

Using the satisfying conditions of $\mathbf{A}$ and $\mathbf{B}$, we know that:

$$
\sum_{i=1}^m \sum_{j=1}^n A_{ij}^2 = 1 \quad \sum_{i=1}^m \sum_{j=1}^n B_{ij}^2 = 1
$$

Such that we may simplfy the above relation as: 

$$
\sum_{i=1}^m \sum_{j=1}^n C_{ij}^2 = a^2(1) + b^2(1)+ 2ab \sum_{i=1}^m \sum_{j=1}^n A_{ij} B_{ij} = a^2 + b^2+ 2ab \sum_{i=1}^m \sum_{j=1}^n A_{ij} B_{ij}
$$

However, we cannot simplify the entirety of this term, $2ab \sum_{i=1}^m \sum_{j=1}^n A_{ij} B_{ij}$. 

As such, we do not have a guarentee that $\mathbf{C}$ sum to 1, which is to say we do not guarantee $\mathbf{C}$ to remain in the set.

Furthermore, the zero matrix $\mathbf{0} \in \mathbb{R}^{m \times n}$ satisfies:

$$
\sum_{i=1}^m \sum_{j=1}^n 0^2 = 0 \neq 1
$$

Such that we know that the zero matrix is not in the set.

Taken together, this is evidence that the set of all matrices in $\mathbb{R}^{m \times n}$ whose entries, when squared, sum to 1, is not a vector space.

\newpage

# Problem 8

Let $\mathbf{A}$ represent any $m \times n$ matrix, and let $\mathbf{B}$ represent any $n \times q$ matrix. Prove that for any choices of generalized inverses $\mathbf{A}^{-}$ and $\mathbf{B}^{-}$, $\mathbf{B}^{-}\mathbf{A}^{-}$ is a generalized inverse of $\mathbf{AB}$ if and only if $\mathbf{A}^{-}\mathbf{ABB}^{-}$ is idempotent.

Structure of Proof: Iff $\iff$ means we must provide proof of both directions of the argument. To that end: 

## Direction 1
generalized inverse $\rightarrow$ idempotent

Let us then assume that $\mathbf{B}^{-}\mathbf{A}^{-}$ is a generalized inverse of $\mathbf{AB}$. 

Generally, a matrix $\mathbf{C}$ is a generalized inverse of $\mathbf{D}$ if:

$$
\mathbf{DCD} = \mathbf{D}
$$

By definition then, we may write:

$$
\mathbf{AB} (\mathbf{B}^{-} \mathbf{A}^{-}) \mathbf{AB} = \mathbf{AB}
$$

We may then consider that: 

$$
\mathbf{AB} (\mathbf{B}^{-} \mathbf{A}^{-}) \mathbf{AB} = \mathbf{AB} = \mathbf{A} (\mathbf{B} \mathbf{B}^{-}) (\mathbf{A}^{-} \mathbf{A}) \mathbf{B} = \mathbf{AB}
$$

Multiplying terms on both sides of the equation above gives us: 

$$
(\mathbf{A}^{-} \mathbf{A} \mathbf{B} \mathbf{B}^{-})(\mathbf{A}^{-} \mathbf{A} \mathbf{B} \mathbf{B}^{-}) = \mathbf{A}^{-} \mathbf{A} \mathbf{B} \mathbf{B}^{-}
$$

Such that we may conclude that $\mathbf{A}^{-}\mathbf{ABB}^{-}$ is idempotent.

## Direction 2
idempotent $\rightarrow$ generalized inverse 

We start by assuming that $\mathbf{A}^{-} \mathbf{A} \mathbf{B} \mathbf{B}^{-}$ is idempotent. 

By definition, this means:

$$
(\mathbf{A}^{-} \mathbf{A} \mathbf{B} \mathbf{B}^{-}) (\mathbf{A}^{-} \mathbf{A} \mathbf{B} \mathbf{B}^{-}) = \mathbf{A}^{-} \mathbf{A} \mathbf{B} \mathbf{B}^{-}
$$

Our goal is to show that $\mathbf{B}^{-} \mathbf{A}^{-}$ satisfies the conditions for being a generalized inverse of $\mathbf{AB}$. 

To that end, let us consider:

$$
\mathbf{AB} (\mathbf{B}^{-} \mathbf{A}^{-}) \mathbf{AB}
$$

Via associativity of matrix multiplication, we may write:

$$
\mathbf{AB} (\mathbf{B}^{-} \mathbf{A}^{-}) \mathbf{AB} = \mathbf{A} \big(\mathbf{B} \mathbf{B}^{-} (\mathbf{A}^{-} \mathbf{A}) \mathbf{B}\big)
$$

Taking advantage of our assumption that $\mathbf{A}^{-} \mathbf{A} \mathbf{B} \mathbf{B}^{-}$ is idempotent, we may note:

$$
\mathbf{B} \mathbf{B}^{-} \mathbf{A}^{-} \mathbf{A} = \mathbf{A}^{-} \mathbf{A} \mathbf{B} \mathbf{B}^{-}
$$

Such that our initial expression may be written: 

$$
\mathbf{AB} (\mathbf{B}^{-} \mathbf{A}^{-}) \mathbf{AB} = \mathbf{A} (\mathbf{A}^{-} \mathbf{A} \mathbf{B} \mathbf{B}^{-}) \mathbf{B}
$$

Finally, since $\mathbf{A}^{-} \mathbf{A} \mathbf{B} \mathbf{B}^{-}$ is idempotent, we may then write:

$$
\mathbf{A} (\mathbf{A}^{-} \mathbf{A} \mathbf{B} \mathbf{B}^{-}) \mathbf{B} = (\mathbf{A} \mathbf{A}^{-} \mathbf{A}) (\mathbf{B} \mathbf{B}^{-} \mathbf{B}) = \mathbf{AB}
$$

So, we have shown that: 

$$
\mathbf{AB} (\mathbf{B}^{-} \mathbf{A}^{-}) \mathbf{AB} = \mathbf{AB}
$$

and conclude that $\mathbf{B}^{-} \mathbf{A}^{-}$ satisfies the properties of a generalized inverse for $\mathbf{AB}$ given the assumption that $\mathbf{A}^{-} \mathbf{A} \mathbf{B} \mathbf{B}^{-}$ is idempotent.

## Conclusion 

Taken together, for any $\mathbf{A}^{-}$ and $\mathbf{B}^{-}$, $\mathbf{B}^{-}\mathbf{A}^{-}$ is a generalized inverse of $\mathbf{AB}$ if and only if $\mathbf{A}^{-} \mathbf{ABB}^{-}$ is idempotent.
