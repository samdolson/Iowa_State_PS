---
title: "Stat 5100 Assignment 1"
author: "Samuel Olson"
date: "Due: Wednesday, January 29th 11:59PM in Gradescope"
output: pdf_document
---

# Instructions

- Please note that a template for the assignment will be provided, but its use is not required.
- When submitting your assignment in Gradescope, ensure that every problem set (3â€“8) starts on a new page.

# Purpose

The main purpose of this assignment is to familiarize yourself with the Preliminary Knowledge on Linear Algebra and Statistics posted for Lecture 1. Aside from Question 2, all questions are related to Linear Algebra. Questions on statistical concepts will follow on Homework 2.

\newpage

# Problem 1

Search the online catalog of Parks Library for a Linear Algebra book specifically for Statistics. I found at least one that is available online through your ISU account. Feel free to search elsewhere. I am not asking you to purchase any books, but I want you to have access to at least one as a resource.

Understood! I have Harville and Zimmerman textbooks. 

\newpage

# Problem 2

Read through the notes posted for Lecture 1 (15-page document). Post any questions you have on the discussion board in the designated space. Grant and I, or your peers, will answer your questions.

Understood! 

\newpage

# Problem 3

Let $\mathbf{A}$ be an $m \times m$ idempotent matrix. Show that:

a) $\underset{m \times m}{\mathbf{I}} - \mathbf{A}$ is idempotent.

An $m \times m$ matrix $\mathbf{A}$ is idempotent if:

$$\mathbf{A}^2 = \mathbf{A}$$

We aim to show that $\mathbf{I} - \mathbf{A}$ is also idempotent.

Let $\mathbf{B} = \mathbf{I} - \mathbf{A}$. Then:

$$\mathbf{B}^2 = (\mathbf{I} - \mathbf{A})^2.$$

Expanding the square:

$$\mathbf{B}^2 = (\mathbf{I} - \mathbf{A})(\mathbf{I} - \mathbf{A}) = \mathbf{I} - \mathbf{A} - \mathbf{A} + \mathbf{A}^2.$$

Using the fact that $\mathbf{A}$ is idempotent ($\mathbf{A}^2 = \mathbf{A}$), we substitute $\mathbf{A}^2$ with $\mathbf{A}$:

$$\mathbf{B}^2 = \mathbf{I} - \mathbf{A} - \mathbf{A} + \mathbf{A} = \mathbf{I} - \mathbf{A}.$$

Thus:

$$\mathbf{B}^2 = \mathbf{B}$$

Therefore, $\mathbf{I} - \mathbf{A}$ is idempotent.

b) $\mathbf{B A B}^{-1}$ is idempotent, where $\mathbf{B}$ is any $m \times m$ nonsingular matrix.

To prove idempotence of $\mathbf{B A B}^{-1}$, we need to show:

$$(\mathbf{B A B}^{-1})^2 = \mathbf{B A B}^{-1}$$

Start with $\mathbf{B A B}^{-1}$:

$$(\mathbf{B A B}^{-1})^2 = (\mathbf{B A B}^{-1})(\mathbf{B A B}^{-1}).$$

Using associativity of matrix multiplication:

$$(\mathbf{B A B}^{-1})^2 = \mathbf{B A (\mathbf{B}^{-1} \mathbf{B}) A B}^{-1}$$

Since $\mathbf{B}^{-1} \mathbf{B} = \mathbf{I}$:

$$(\mathbf{B A B}^{-1})^2 = \mathbf{B A A B}^{-1}$$

Using the idempotence of $\mathbf{A}$ ($\mathbf{A}^2 = \mathbf{A}$):

$$(\mathbf{B A B}^{-1})^2 = \mathbf{B A B}^{-1}.$$

Thus, $\mathbf{B A B}^{-1}$ is idempotent.

\newpage

# Problem 4

A matrix $\mathbf{A}$ is symmetric if $\mathbf{A} = \mathbf{A}^\top$. Determine the truth of the following statements:

a) If $\mathbf{A}$ and $\mathbf{B}$ are symmetric, then their product $\mathbf{AB}$ is symmetric.

Let  
$$\mathbf{A} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}, \quad \mathbf{B} = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}.$$

Both $\mathbf{A}$ and $\mathbf{B}$ are symmetric. However,  

$$\mathbf{AB} = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}, \quad (\mathbf{AB})^\top = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}.$$

Since $(\mathbf{AB})^\top \neq \mathbf{AB}$, the product $\mathbf{AB}$ is not symmetric, so False.

b) If $\mathbf{A}$ is not symmetric, then $\mathbf{A}^{-1}$ is not symmetric.


Let:

$$\mathbf{A} = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix}.$$

The transpose of $\mathbf{A}$ is:  
$$\mathbf{A}^\top = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix},$$  
which is not equal to $\mathbf{A}$.  
Therefore, $\mathbf{A}$ is not symmetric.

The inverse of $\mathbf{A}$ is:  
$$\mathbf{A}^{-1} = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}.$$

The transpose of $\mathbf{A}^{-1}$ is:  
$$(\mathbf{A}^{-1})^\top = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix},$$  
which is equal to $\mathbf{A}^{-1}$.  
Therefore, $\mathbf{A}^{-1}$ is symmetric.

So overall, the statement is FALSE, as shown by the counterexample provided. 

c) When $\mathbf{A, B, C}$ are symmetric, the transpose of $\mathbf{ABC}$ is $\mathbf{CBA}$.

Using the transpose property:

$$(\mathbf{ABC})^\top = \mathbf{C}^\top \mathbf{B}^\top \mathbf{A}^\top.$$

Since $\mathbf{A}, \mathbf{B}, \mathbf{C}$ are symmetric, this simplifies to:

$$(\mathbf{ABC})^\top = \mathbf{CBA}$$, so True.

If $\mathbf{A} = \mathbf{A}^\top$ and $\mathbf{B} = \mathbf{B}^\top$, which of these matrices are certainly symmetric?

d) $\mathbf{A}^2 - \mathbf{B}^2$:  

$(\mathbf{A}^2 - \mathbf{B}^2)^\top = \mathbf{A}^2 - \mathbf{B}^2$, so it is symmetric.  

e) $\mathbf{ABA}$:  

$(\mathbf{ABA})^\top = \mathbf{A}^\top \mathbf{B}^\top \mathbf{A}^\top = \mathbf{ABA}$, so it is symmetric.    

f) $\mathbf{ABAB}$:  

$(\mathbf{ABAB})^\top = \mathbf{B}^\top \mathbf{A}^\top \mathbf{B}^\top \mathbf{A}^\top = \mathbf{ABAB}$, so it is symmetric.    

g) $(\mathbf{A} + \mathbf{B})(\mathbf{A} - \mathbf{B})$:  

Expanding: $(\mathbf{A} + \mathbf{B})(\mathbf{A} - \mathbf{B}) = \mathbf{A}^2 - \mathbf{B}^2$, so it is symmetric.  

\newpage

# Problem 5

Consider the matrix 

$$
\mathbf{X} = \begin{bmatrix}
1 & -3 & 0 & -3 \\
1 & -2 & -1 & 2 \\
2 & -5 & -1 & -1
\end{bmatrix}
$$

a) Show that the columns of $\mathbf{X}$ are linearly dependent.

To prove linear dependence, we find a non-zero vector $\mathbf{a} \in \mathbb{R}^4$ such that:

$$
\mathbf{X} \mathbf{a} = \sum_{i=1}^4 a_i \mathbf{x}_i = 0,
$$

where $a_i$ is the $i$-th element of $\mathbf{a}$. For example:

$$
\mathbf{a} = \begin{bmatrix} 3 \\ 1 \\ 1 \\ 0 \end{bmatrix},
$$

since:

$$
3 \mathbf{x}_1 + 1 \mathbf{x}_2 + 1 \mathbf{x}_3 + 0 \mathbf{x}_4 = 0.
$$

To find such a solution, solve the system:

$$
\begin{cases}
a_1 \cdot 1 + a_2 \cdot (-3) + a_3 \cdot 0 + a_4 \cdot (-3) = 0, \\
a_1 \cdot 1 + a_2 \cdot (-2) + a_3 \cdot (-1) + a_4 \cdot 2 = 0, \\
a_1 \cdot 2 + a_2 \cdot (-5) + a_3 \cdot (-1) + a_4 \cdot (-1) = 0.
\end{cases}
$$

Solving this system yields:
$$
a_1 = -12t + 3s, \quad a_2 = -5t + s, \quad a_3 = s, \quad a_4 = t,
$$

where $s, t \in \mathbb{R}$. By choosing $s = 1, t = 0$, we recover $\mathbf{a} = \begin{bmatrix} 3 \\ 1 \\ 1 \\ 0 \end{bmatrix}$.

b) Find the rank of $\mathbf{X}$.

The row reduction of $\mathbf{X}$ is as follows:

$$
\mathbf{X} =
\begin{bmatrix}
1 & -3 & 0 & -3 \\
1 & -2 & -1 & 2 \\
2 & -5 & -1 & -1
\end{bmatrix}
\to
\begin{bmatrix}
1 & -3 & 0 & -3 \\
0 & 1 & -1 & 5 \\
0 & 1 & -1 & 5
\end{bmatrix}
\to
\begin{bmatrix}
1 & 0 & -3 & 12 \\
0 & 1 & -1 & 5 \\
0 & 0 & 0 & 0
\end{bmatrix}.
$$

Since the row echelon form has two non-zero rows, the rank of $\mathbf{X}$ is 2. Additionally, the third row is the sum of the first two rows, meaning the rank can be at most 2. The first two rows are linearly independent, confirming that the rank is 2.

c) Use the generalized inverse algorithm in Slide Set 1 to find a generalized inverse of $\mathbf{X}$.

1. Let $W$ be an $r \times r$ nonsingular submatrix of $\mathbf{X}$, where $r = \text{rank}(\mathbf{X}) = 2$. For example:

$$
W =
\begin{bmatrix}
x_{11} & x_{13} \\
x_{21} & x_{23}
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 \\
1 & -1
\end{bmatrix}.
$$

2. Compute $(W^{-1})^\top$:

$$
(W^{-1})^\top =
\begin{bmatrix}
1 & 1 \\
0 & -1
\end{bmatrix}.
$$

3. Replace the elements of $W$ in $\mathbf{X}$ with the corresponding elements of $(W^{-1})^\top$. Then:

$$
\mathbf{X} =
\begin{bmatrix}
1 & -3 & 1 & -3 \\
0 & -2 & -1 & 2 \\
2 & -5 & -1 & -1
\end{bmatrix}.
$$

4. Replace all other elements in $\mathbf{X}$ with zeros. The resulting matrix is:

$$
\mathbf{X} =
\begin{bmatrix}
1 & 0 & 1 & 0 \\
0 & 0 & -1 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix}.
$$

5. Transpose the matrix to obtain $\mathbf{G}$, a generalized inverse of $\mathbf{X}$:

$$
\mathbf{G} =
\begin{bmatrix}
1 & 0 & 0 \\
0 & 0 & 0 \\
1 & -1 & 0 \\
0 & 0 & 0
\end{bmatrix}.
$$

d) Use the R function `ginv` in the `MASS` package to find a generalized inverse of $\mathbf{X}$. 

- To load the `MASS` package into your R workspace, use the command `library(MASS)`. 
- If the `MASS` package is not already installed, use `install.packages("MASS")` to install it.

```{r}
library(MASS)
X <- matrix(c(1,1,2,-3,-2,-5,0,-1,-1,-3,2,-1), ncol = 4)
massX <- MASS::ginv(X)
massX
```

e) Provide one matrix $\mathbf{X}^*$ that satisfies both of the following characteristics:
   - $\mathbf{X}^*$ has full-column rank.
   - $\mathbf{X}^*$ has column space equal to the column space of $\mathbf{X}$.

The rank of $\mathbf{X}$ is 2 (from part (b)). Since $\mathbf{x}_1$ and $\mathbf{x}_3$ are linearly independent, and $\mathbf{x}_2$ and $\mathbf{x}_4$ can be generated by linear combinations of $\mathbf{x}_1$ and $\mathbf{x}_3$ (i.e., $\mathbf{x}_2 = 3 \cdot \mathbf{x}_1 + \mathbf{x}_3$ and $\mathbf{x}_4 = -3 \cdot \mathbf{x}_1 - 5 \cdot \mathbf{x}_3$), we have:

$$
C([\mathbf{x}_1, \mathbf{x}_3]) = C([\mathbf{x}_1, \mathbf{x}_2, \mathbf{x}_3, \mathbf{x}_4]).
$$

Thus, we can construct:

$$
\mathbf{X}^* =
\begin{bmatrix}
1 & 0 \\
1 & -1 \\
2 & -1
\end{bmatrix}.
$$

Note that we can pick any two linearly independent columns of $\mathbf{X}$ to form $\mathbf{X}^*$. It is not necessary for $\mathbf{X}^*$ to include columns of $\mathbf{X}$. For example, $\mathbf{X}^*$ could also be:

$$
\mathbf{X}^* =
\begin{bmatrix}
1 & 0 \\
0 & 1 \\
1 & 1
\end{bmatrix}.
$$

Any column of $\mathbf{X}^*$ can be written as a linear combination of the columns of $\mathbf{X}$, and any column of $\mathbf{X}$ can be written as a linear combination of the columns of $\mathbf{X}^*$. Thus:

$$
C(\mathbf{X}) = C(\mathbf{X}^*).
$$

\newpage

# Problem 6

Prove the following result:

Suppose the set of $m \times 1$ vectors $\mathbf{x}_1, \ldots,\mathbf{x}_n$ is a basis for the vector space $\mathcal{S}$. Then any vector $\mathbf{x} \in \mathcal{S}$ has a unique representation as a linear combination of the vectors $\mathbf{x}_1, \ldots,\mathbf{x}_n$.

Since $\mathbf{x}_1, \ldots, \mathbf{x}_n$ is a basis for $\mathcal{S}$, the following two properties hold:
1. The vectors $\mathbf{x}_1, \ldots, \mathbf{x}_n$ are linearly independent.
2. The span of $\mathbf{x}_1, \ldots, \mathbf{x}_n$ equals $\mathcal{S}$, i.e.,

$$
\mathcal{S} = \text{span}\{\mathbf{x}_1, \ldots, \mathbf{x}_n\}.
$$

Let $\mathbf{x} \in \mathcal{S}$. By definition of span, $\mathbf{x}$ can be written as a linear combination of $\mathbf{x}_1, \ldots, \mathbf{x}_n$:

$$
\mathbf{x} = c_1 \mathbf{x}_1 + c_2 \mathbf{x}_2 + \cdots + c_n \mathbf{x}_n,
$$

where $c_1, c_2, \ldots, c_n \in \mathbb{R}$ are scalars.

To prove uniqueness, suppose there exists another representation of $\mathbf{x}$:

$$
\mathbf{x} = d_1 \mathbf{x}_1 + d_2 \mathbf{x}_2 + \cdots + d_n \mathbf{x}_n,
$$

where $d_1, d_2, \ldots, d_n \in \mathbb{R}$ are scalars. Subtracting the two representations gives:

$$
(c_1 - d_1) \mathbf{x}_1 + (c_2 - d_2) \mathbf{x}_2 + \cdots + (c_n - d_n) \mathbf{x}_n = \mathbf{0}.
$$

Since $\mathbf{x}_1, \ldots, \mathbf{x}_n$ are linearly independent, the only solution to this equation is:

$$
c_1 - d_1 = 0, \quad c_2 - d_2 = 0, \quad \ldots, \quad c_n - d_n = 0.
$$

Thus:

$$
c_1 = d_1, \quad c_2 = d_2, \quad \ldots, \quad c_n = d_n.
$$

Therefore, the representation of $\mathbf{x}$ as a linear combination of $\mathbf{x}_1, \ldots, \mathbf{x}_n$ is unique.

Any vector $\mathbf{x} \in \mathcal{S}$ has a unique representation as a linear combination of the basis vectors $\mathbf{x}_1, \ldots, \mathbf{x}_n$.

\newpage

# Problem 7

Am I a vector space? (The basic question here is whether every linear combination is in the space. If there is no zero, then Iâ€™m for sure not a vector space.)

a) All vectors in $\mathbb{R}^n$ whose entries sum to 0.

Let $\mathbf{v} \in \mathbb{R}^n$ satisfy $\sum_{i=1}^n v_i = 0$, and let $\mathbf{w} \in \mathbb{R}^n$ satisfy $\sum_{i=1}^n w_i = 0$. Consider a linear combination:

$$
\mathbf{u} = a \mathbf{v} + b \mathbf{w},
$$

where $a, b \in \mathbb{R}$. Then:

$$
\sum_{i=1}^n u_i = \sum_{i=1}^n (a v_i + b w_i) = a \sum_{i=1}^n v_i + b \sum_{i=1}^n w_i = a \cdot 0 + b \cdot 0 = 0.
$$

Thus, $\mathbf{u} \in \mathbb{R}^n$ also satisfies $\sum_{i=1}^n u_i = 0$, so the set is closed under linear combinations.

- The zero vector $\mathbf{0} \in \mathbb{R}^n$ also satisfies $\sum_{i=1}^n 0 = 0$, so the set contains the zero vector.
- Since addition and scalar multiplication preserve the property, this set is a vector space.

The set of all vectors in $\mathbb{R}^n$ whose entries sum to 0 is a vector space.

b) All matrices in $\mathbb{R}^{m \times n}$ whose entries, when squared, sum to 1.

Let $\mathbf{A}, \mathbf{B} \in \mathbb{R}^{m \times n}$ satisfy:

$$
\sum_{i=1}^m \sum_{j=1}^n A_{ij}^2 = 1 \quad \text{and} \quad \sum_{i=1}^m \sum_{j=1}^n B_{ij}^2 = 1.
$$

Consider a linear combination:

$$
\mathbf{C} = a \mathbf{A} + b \mathbf{B},
$$

where $a, b \in \mathbb{R}$. Then:

$$
\sum_{i=1}^m \sum_{j=1}^n C_{ij}^2 = \sum_{i=1}^m \sum_{j=1}^n (a A_{ij} + b B_{ij})^2.
$$

Expanding this:

$$
\sum_{i=1}^m \sum_{j=1}^n C_{ij}^2 = a^2 \sum_{i=1}^m \sum_{j=1}^n A_{ij}^2 + b^2 \sum_{i=1}^m \sum_{j=1}^n B_{ij}^2 + 2ab \sum_{i=1}^m \sum_{j=1}^n A_{ij} B_{ij}.
$$

Using the conditions on $\mathbf{A}$ and $\mathbf{B}$, we know:

$$
\sum_{i=1}^m \sum_{j=1}^n A_{ij}^2 = 1, \quad \sum_{i=1}^m \sum_{j=1}^n B_{ij}^2 = 1,
$$

but the result depends on the cross term $2ab \sum_{i=1}^m \sum_{j=1}^n A_{ij} B_{ij}$. In general, this does not sum to 1, so $\mathbf{C}$ is not guaranteed to remain in the set.

Furthermore, the zero matrix $\mathbf{0} \in \mathbb{R}^{m \times n}$ satisfies:

$$
\sum_{i=1}^m \sum_{j=1}^n 0^2 = 0,
$$

which is not equal to 1. Therefore, the zero matrix is not in the set.

The set of all matrices in $\mathbb{R}^{m \times n}$ whose entries, when squared, sum to 1, is not a vector space.

\newpage

# Problem 8

Let $\mathbf{A}$ represent any $m \times n$ matrix, and let $\mathbf{B}$ represent any $n \times q$ matrix. Prove that for any choices of generalized inverses $\mathbf{A}^{-}$ and $\mathbf{B}^{-}$, $\mathbf{B}^{-}\mathbf{A}^{-}$ is a generalized inverse of $\mathbf{AB}$ if and only if $\mathbf{A}^{-}\mathbf{ABB}^{-}$ is idempotent.

A matrix $\mathbf{C}$ is a generalized inverse of $\mathbf{D}$ if:

$$
\mathbf{DCD} = \mathbf{D}.
$$

Assume $\mathbf{B}^{-}\mathbf{A}^{-}$ is a generalized inverse of $\mathbf{AB}$. Then by definition:

$$
\mathbf{AB} (\mathbf{B}^{-} \mathbf{A}^{-}) \mathbf{AB} = \mathbf{AB}.
$$

Expanding and rearranging terms:

$$
\mathbf{A} (\mathbf{B} \mathbf{B}^{-}) (\mathbf{A}^{-} \mathbf{A}) \mathbf{B} = \mathbf{AB}.
$$

Now multiply $\mathbf{A}^{-}$ on the left and $\mathbf{B}^{-}$ on the right:

$$
(\mathbf{A}^{-} \mathbf{A} \mathbf{B} \mathbf{B}^{-})(\mathbf{A}^{-} \mathbf{A} \mathbf{B} \mathbf{B}^{-}) = \mathbf{A}^{-} \mathbf{A} \mathbf{B} \mathbf{B}^{-}.
$$

Thus, $\mathbf{A}^{-}\mathbf{ABB}^{-}$ is idempotent.

Assume $\mathbf{A}^{-} \mathbf{ABB}^{-}$ is idempotent. By the idempotence property:

$$
(\mathbf{A}^{-} \mathbf{A} \mathbf{B} \mathbf{B}^{-}) (\mathbf{A}^{-} \mathbf{A} \mathbf{B} \mathbf{B}^{-}) = \mathbf{A}^{-} \mathbf{A} \mathbf{B} \mathbf{B}^{-}.
$$

Now consider $\mathbf{B}^{-} \mathbf{A}^{-}$ as a candidate for the generalized inverse of $\mathbf{AB}$. Verify:

$$
\mathbf{AB} (\mathbf{B}^{-} \mathbf{A}^{-}) \mathbf{AB} = \mathbf{A} \mathbf{B} \mathbf{B}^{-} \mathbf{A}^{-} \mathbf{A} \mathbf{B}.
$$

Substitute $\mathbf{A}^{-} \mathbf{A} \mathbf{B} \mathbf{B}^{-}$ for idempotence:

$$
\mathbf{A} (\mathbf{A}^{-} \mathbf{A} \mathbf{B} \mathbf{B}^{-}) \mathbf{B} = \mathbf{AB}.
$$

Thus, $\mathbf{B}^{-} \mathbf{A}^{-}$ satisfies the generalized inverse property for $\mathbf{AB}$.

For any $\mathbf{A}^{-}$ and $\mathbf{B}^{-}$, $\mathbf{B}^{-}\mathbf{A}^{-}$ is a generalized inverse of $\mathbf{AB}$ if and only if $\mathbf{A}^{-} \mathbf{ABB}^{-}$ is idempotent.
