---
title: "PS6"
output: pdf_document
author: "Sam Olson"
---

# Outline 

  - Q1: Edited, iffy on part f) 
  - Q2: Edited, G2G 
  - Q3: Edited, G2G

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Q1 

Consider the dataset `pigs` provided in the R package `emmeans`. The data can be accessed in R with the following commands.

```{r, warning = F, message = F}
library(emmeans)
names(pigs)
```

To learn a more about the data, type `?pigs` at the R prompt. For the purposes of this problem, use the natural logarithm of the variable `conc` as the response. Consider both `source` and `percent` as categorical factors. Assume the cell-means model with one unrestricted treatment mean for each combination of `source` and `percent`.

```{r, warning = F, message = F}
lnConc <- log(pigs$conc)
pigs$percent <- factor(pigs$percent)
class(pigs$percent)
```

\newpage 

### a) 

Generate an ANOVA table with Type I (sequential) sums of squares for `source`, `percent`, `source × percent`, `error`, and `corrected total`. In addition to sums of squares, your ANOVA table should include degrees of freedom, mean squares, F statistics, and p-values where appropriate.

Type 1 Sums of Squares are the default given when using the `anova` function in R. Hence: 

```{r, warning = F, message = F}
baseDat <- lm(lnConc ~ source + percent + source*percent, data=pigs)
type1Dat <- anova(baseDat)
type1_df <- data.frame(type1Dat)

total <- c(sum(type1_df$`Df`), sum(type1_df$`Sum.Sq`), NA, NA, NA)
type1_df <- rbind(type1_df, total) 
rownames(type1_df)[nrow(type1_df)] <- "Total"

print(type1_df, digits = 6)
```

\newpage 

### b) 

Generate an ANOVA table with Type II sums of squares for `source`, `percent`, `source × percent`, `error`, and `corrected total`. In addition to sums of squares, your ANOVA table should include degrees of freedom, mean squares, F statistics, and p-values where appropriate.

The `car` package allows more flexible usage of Types of Suns of Squares used for anova. Hence: 

```{r, warning = F, message = F}
library(car)
type2Dat <- car::Anova(baseDat, type = 2)

type2_df <- as.data.frame(type2Dat)

# Compute Mean Square (MS) = SS / df
type2_df$MeanSq <- type2_df$`Sum Sq` / type2_df$Df

# Reorder columns to match anova() output
type2_df <- type2_df[, c("Df", "Sum Sq", "MeanSq", "F value", "Pr(>F)")]

total <- c(sum(type2_df$`Df`), sum(type2_df$`Sum Sq`), NA, NA, NA)
type2_df <- rbind(type2_df, total) 
rownames(type2_df)[nrow(type2_df)] <- "Total"
# Print with higher precision
print(type2_df, digits = 6)
```

\newpage 

### c) 

Generate an ANOVA table with Type III sums of squares for `source`, `percent`, `source × percent`, `error`, and `corrected total`. In addition to sums of squares, your ANOVA table should include degrees of freedom, mean squares, F statistics, and p-values where appropriate.

Quick note on the continued use of `car::Anova` for this problem: By default R uses the baseline constraint when making these calculations, and in R the baseline is the combination of the first factor levels. So effectively we just need to change the type of contrast being used when calculating the Anova table from treatment aka `contr.treatment` to sums aka `contra.sum` (which corresponds to a sum-to-zero constraint) and proceed, bearing in mind Type 3 Sums of Squares are 'marginal' Sums of Squares, hence the reason for this adjustment. 

```{r, warning = F, message = F}
options(digits = 6)  

# car Anova function needs some adjustment for Type 3
contrasts(pigs$source) <- contr.sum
contrasts(pigs$percent) <- contr.sum

type3Dat <- car::Anova(baseDat, type = 3)
type3_df <- as.data.frame(type3Dat)

# Add MSE
type3_df$MeanSq <- type3_df$`Sum Sq` / type2_df$Df
type3_df <- type3_df[, c("Df", "Sum Sq", "MeanSq", "F value", "Pr(>F)")]

type3_df <- type3_df[-1, ]

total <- c(sum(type3_df$`Df`), sum(type3_df$`Sum Sq`), NA, NA, NA)
type3_df <- rbind(type3_df, total) 
rownames(type3_df)[nrow(type3_df)] <- "Total"

print(type3_df, digits = 6)
```

\newpage 

### d) 

Find LSMeans for `source` and `percent`.

```{r, warning = F, message = F}
library(tidyverse)

summary_table <- pigs |> 
  group_by(source, percent) |> 
  mutate(count = n()) 

print(summary_table, digits = 6)
```

Our data is not balanced, so need to explicitly calculate LSmeans (they are not the same as the OLS estimates). 

```{r, warning = F, message = F}
library(tidyverse)
library(emmeans)

modDat <- lm(lnConc ~ source * percent, data = pigs)
lsmeans_table <- emmeans(modDat, ~ source * percent)
lsmeans_df <- as.data.frame(lsmeans_table)

lsmeans_wide <- lsmeans_df |> 
  select(source, percent, emmean) |> 
  pivot_wider(names_from = percent, values_from = emmean)

lsmeans_wide 
```

We then sum across row, column for the `source`, `percent` LSmeans respectively. 

```{r, warning = F, message = F}
lsmeans_wide <- lsmeans_wide |> 
  mutate(Row_Avg = rowMeans(select(lsmeans_wide, -source), na.rm = TRUE))

lsmeans_wide[-c(2:5)]
```

```{r, warning = F, message = F}
# Compute column-wise averages (across sources)
col_avg <- lsmeans_wide |> 
  summarise(across(-source, mean, na.rm = TRUE)) |> 
  mutate(source = "Column_Avg")

col_avg
```

In Summary, our LSmeans are: 

Percent: 

  - 9%: 3.448
  - 12%: 3.625
  - 15%: 3.670
  - 18%: 3.775
	
Source: 

  - Fish: 3.396
  - Soy: 3.662	
  - Skim: 3.830
  
\newpage 

### e) 

Consider simplifying the model so that `percent` is treated like a quantitative variable with linear effects on `log(conc)` and linear interactions; i.e.,

```{r, eval = F}
lm(y ~ source + percent + source:percent)
```

where `y=log(conc)` and `percent` is numeric. Does such a model fit adequately relative to the cell-means model? Conduct a lack of fit test and report the results.

Back to the land of vanilla `anova`. We can just put the two models in and compare directly. 

```{r, warning = F, message = F}
# Need to readjust based on messing with contrasts for Type 3 Sums of Squares 
contrasts(pigs$source) <- contr.sum
contrasts(pigs$percent) <- contr.sum
# Reduced model: percent as numeric
reduced <- lm(lnConc ~ source + as.numeric(percent) + source:as.numeric(percent), data = pigs)

# Full model: percent as factor
full <- lm(lnConc ~ source * factor(percent), data = pigs)

anova(reduced, full)
# order is important for positive df, but statistical tests yield same results (F stat and p-value)
# anova(full, reduced)
```

We have evidence to support the reduced model being adequate in lieu of the more complex, cell means model. 

\newpage 

### f) 

The reduced model fit in part e) implies that, for each `source`, there is a linear relationship between the expected log concentration and percentage. Based on the fit of the reduced model in part e), provide the estimated linear relationship for each `source`.

General Form of Model: 

$$
y_{ijk} = \mu + \alpha_i + \beta x_{ij} + \gamma_i x_{ij} + \epsilon_{ijk}
$$
Where: 

$$
E[y_{ijk}] = \mu + \alpha_i + \beta x_{ij} + \gamma_i x_{ij}
$$

And where we use the following as our estimated effects: 

```{r, eval = FALSE, echo = FALSE}
# troubleshooting discrepancies 
contrasts(pigs$source) <- contr.sum
contrasts(pigs$percent) <- contr.sum

reduced <- lm(lnConc ~ source + as.numeric(percent) + source:as.numeric(percent), data = pigs)
summary(reduced)
```

```{r}
summary(reduced)
```

Checking Reference Levels for source: 

```{r}
levels(pigs$source)
contrasts(pigs$source)
```

Based on the above output, we have "fish" as our baseline, giving us the estimates of: 

#### Fish (Baseline Category)

$$
\hat{y} 
= \hat{\mu} + \hat{\beta} x_{1j}
= 3.37953 + 0.10081 x_{1j}
$$

#### Soy (`source1`)

$$
\hat{y} 
= (\hat{\mu} + \hat{\alpha}_1) + (\hat{\beta} + \hat{\gamma}_1) x_{2j}
= (3.37953 - 0.13651) + (0.10081 - 0.03751) x_{2j}
= 3.24302 + 0.0633 x_{2j}
$$

#### Skim (`source2`)

$$
\hat{y} 
= (\hat{\mu} + \hat{\alpha}_2) + (\hat{\beta} + \hat{\gamma}_2) x_{3j}
= (3.37953 + 0.11907) + (0.10081 - 0.03556) x_{3j}
= 3.4986 + 0.06525 x_{3j}
$$

\newpage

## Q2

Consider the plant density example discussed in slide set 6.

```{r, eval = T, echo=FALSE, fig.cap="CocoMelon"}
knitr::include_graphics("Density.png")
```

### a) 

For each of the tests in the ANOVA table on slide 38, provide a vector $c$ so that a test of

$$H_0 : c^T \beta = 0$$

would yield the same statistic and p-value as the ANOVA test. (You can use R to help you with the computations like we did on slides 45 and 46 of slide set 6.) Label these vectors $c_1$, $c_2$, $c_3$, and $c_4$ for the linear, quadratic, cubic, and quartic tests, respectively.

```{r}
proj <- function(x) {
  x %*% MASS::ginv(t(x) %*% x) %*% t(x)
}

# Had to dig through Canvas to find the supplemental R handout to find this. Smh 
# Or, I guess not, there's a lot there to borrow for this problem
d <- read.delim("https://dnett.github.io/S510/PlantDensity.txt")
names(d) <- c("x","y")

n <- nrow(d)
x <- (d$x-mean(d$x))/10
# Iteratively, we just need to add a higher order term to the prior design matrix 
x1 <- matrix(1, nrow = n, ncol = 1)
x2 <- cbind(x1,x)
x3 <- cbind(x2,x^2)
x4 <- cbind(x3,x^3)
x5 <- matrix(model.matrix(~0+factor(x)),nrow <- n)
```

```{r}
p1 <- proj(x1)
p2 <- proj(x2)
p3 <- proj(x3)
p4 <- proj(x4)
p5 <- proj(x5)

# Contrasts 
((p2-p1)%*%x5)[1,] * 5 
((p3-p2)%*%x5)[1,] * 7 
((p4-p3)%*%x5)[1,] * 10 
((p5-p4)%*%x5)[1,] * 70 
```

From the above output, we have: 

$$
\mathbf{c}_1^{\top} = [2, 1, 0, -1, -2]
$$

$$
\mathbf{c}_2^{\top} = [2, -1, -2, -1, 2]
$$

$$
\mathbf{c}_3^{\top} = [1, -2, 0, 2, -1]
$$

$$
\mathbf{c}_4^{\top} = [1, -4, 6, -4, 1]
$$

\newpage 

### b) 

Are $c^T_1 \beta$, $c^T_2 \beta$, $c^T_3 \beta$, and $c^T_4 \beta$ contrasts? Explain.

All $\mathbf{c}_i ^{\top} \boldsymbol{\beta}$ are contrasts because:

$$
\mathbf{c}_i ^{\top} \mathbf{1} = 0 \quad \text{for } i = 1,2,3,4
$$

Explicitly: 

$$
\mathbf{c}_1^{\top} \mathbf{1}
= 2 + 1 + 0 - 1 - 2 = 0
$$

$$
\mathbf{c}_2^{\top} \mathbf{1}
= 2 -1 -2 -1 + 2 = 0
$$

$$ 
\mathbf{c}_3^{\top} \mathbf{1}
= 1 -2 + 0 +2 -1 = 0
$$

$$ 
\mathbf{c}_4^{\top} \mathbf{1}
= 1 -4 + 6 -4 + 1 = 0
$$

And not only are these contrasts, but they are orthogonal contrasts because they satisfy:  

$$
\mathbf{c}_i ^{\top} \mathbf{c}_j = 0, \quad \text{for } i \neq j
$$

\newpage 

### c) 

Are $c^T_1 \beta$, $c^T_2 \beta$, $c^T_3 \beta$, and $c^T_4 \beta$ orthogonal? Explain.

Preempted this question a bit at the end of part b), but yes, these contrasts are orthogonal, and here is more detail on why beyond the expression they satisfy. 

So, the initial expression to check is: 

$$
\mathbf{c}_i ^{\top} \mathbf{c}_j = 0, \quad \text{for } i \neq j
$$

Explicitly: 

4 choose 2 cases to check, 6 total cases: 

$$
\mathbf{c}_1^{\top} \mathbf{c}_2
= (2)(2) + (1)(-1) + (0)(-2) + (-1)(-1) + (-2)(2)
= 4 - 1 + 0 + 1 - 4 = 0
$$

$$
\mathbf{c}_1^{\top} \mathbf{c}_3
= (2)(1) + (1)(-2) + (0)(0) + (-1)(2) + (-2)(-1)
= 2 - 2 + 0 - 2 + 2 = 0
$$

$$ 
\mathbf{c}_1^{\top} \mathbf{c}_4
= (2)(1) + (1)(-4) + (0)(6) + (-1)(-4) + (-2)(1)
= 2 - 4 + 0 + 4 - 2 = 0
$$

$$ 
\mathbf{c}_2^{\top} \mathbf{c}_3
= (2)(1) + (-1)(-2) + (-2)(0) + (-1)(2) + (2)(-1)
= 2 + 2 + 0 - 2 - 2 = 0
$$

$$ 
\mathbf{c}_2^{\top} \mathbf{c}_4
= (2)(1) + (-1)(-4) + (-2)(6) + (-1)(-4) + (2)(1)
= 2 + 4 - 12 + 4 + 2 = 0
$$

$$
\mathbf{c}_3^{\top} \mathbf{c}_4
= (1)(1) + (-2)(-4) + (0)(6) + (2)(-4) + (-1)(1)
= 1 + 8 + 0 - 8 - 1 = 0
$$

Note: The above is a simplification that works for the example given because of the design matrix used in this problem. 

The "base" definition we reference for testing whether something is orthogonal is: Any two estimable linear combinations $c_i^T \boldsymbol{\beta}$ and $c_j^T \boldsymbol{\beta}$ are orthogonal if and only if:

$$
\mathbf{c}_i^T (\mathbf{X^T X})^{-} \mathbf{c}_j = 0 \quad \text{for } i \neq j
$$

We use the full design matrix (`x5` from above) for the calculations. Since $\mathbf{X^T X}$ is full rank, we can compute its inverse directly rather than using a generalized inverse. Specifically, because $\mathbf{X^T X}$ is a scalar multiple of the identity matrix, we ensure it is invertible, meaning its inverse is unique and can be computed using the `solve` function in R.

Evaluating the above, we get the following matrices: 

```{r}
require(MASS)
x5
t(x5) %*% x5
fractions(solve(t(x5) %*% x5))
```

Thus, in this case,

$$
\mathbf{c}_i^T (\mathbf{X^T X})^{-1} \mathbf{c}_j = \mathbf{c}_i^T \mathbf{c}_j /3
$$

so that linear combinations $\mathbf{c}_i^T \boldsymbol{\beta}$ and $\mathbf{c}_j^T \boldsymbol{\beta}$ are orthogonal if and only if $\mathbf{c}_i^T \mathbf{c}_j = 0$. (Multiplying each side by 3 for simplicity). 

The rest of the results (that our contrasts are orthogonal) follow from the explicit derivations. 

\newpage 

## Q3 

Suppose $\mathbf{H}$ is a symmetric matrix. Prove that $H$ is nonnegative definite if and only if all its eigenvalues are nonnegative. (If you wish, you may use the Spectral Decomposition Theorem in your proof.)

I do want to use Spectral Decomposition, thank you! 

### Spectral Decomposition Theorem:

For $\mathbf{H}$ is a symmetric matrix, then: 

$$
\mathbf{H} = \mathbf{P} \Lambda \mathbf{P}^{\top} = \sum_{i=1}^{n} \lambda_i \mathbf{p}_i \mathbf{p}_i^{\top}
$$

where $\mathbf{p}_i$ are orthonormal eigenvectors of $\mathbf{H}$.

The general approach for solving an iff proof is to prove both directions hold. To that end: 

### Direction 1

Assume we have $\mathbf{H}$, a symmetric, nonnegative definite matrix. 

By definition, $\mathbf{H}$ being nonnegative definite, means the following holds:

$$
\mathbf{p}_i^{\top} \mathbf{H} \mathbf{p}_i \geq 0
$$

for any $\mathbf{p}_i$, where $i = 1, \dots, n$.

Unfurling $\mathbf{H}$, we may rewrite: 

$$
\mathbf{p}_i^{\top} \mathbf{H} \mathbf{p}_i = \mathbf{p}_i^{\top} \left( \sum_{j=1}^{n} \lambda_j \mathbf{p}_j \mathbf{p}_j^{\top} \right) \mathbf{p}_i
= \sum_{j=1}^{n} \lambda_j \mathbf{p}_i^{\top} \mathbf{p}_j \mathbf{p}_j^{\top} \mathbf{p}_i
$$

By Special Decomposition, we know $\mathbf{p}_i$ are orthonormal, meaning:

$$
\mathbf{p}_i^{\top} \mathbf{p}_j = 0 \quad \text{for all } i \neq j, \quad \text{and} \quad \mathbf{p}_i^{\top} \mathbf{p}_i = 1
$$

Allowing us to further simplify: 

$$
\mathbf{p}_i^{\top} \mathbf{H} \mathbf{p}_i 
= \lambda_i \mathbf{p}_i^{\top} \mathbf{p}_i \mathbf{p}_i^{\top} \mathbf{p}_i
= \lambda_i
$$

And because $\mathbf{H}$ is nonnegative definite, it then follows that:

$$
\lambda_i \geq 0, \quad \forall i = 1, \dots, n
$$

Direction one complete! 

### Direction 2

We start by assuming that for a symmetric matrix $\mathbf{H}$, all its eigenvalues are nonnegative. 

Given our assumption, we may restate as $\lambda_i \geq 0$ for $i = 1, \dots, n$. 

By the Spectral Decomposition Theorem:

$$
\mathbf{H} = \mathbf{P} \operatorname{diag}(\lambda_1, \dots, \lambda_n) \mathbf{P}^{\top}
$$

where $\mathbf{P} = [\mathbf{p}_1, \dots, \mathbf{p}_n]$ and:

$$
\mathbf{P} \mathbf{P}^{\top} = \mathbf{P}^{\top} \mathbf{P} = \mathbf{I}
$$

For $j = 1, \dots, n$, Expressing $\mathbf{y}$ in terms of the eigenvectors of $\mathbf{H}$, we have: 

$$
\mathbf{y} = \sum_{j=1}^{n} x_j \mathbf{p}_j \rightarrow x_j = \mathbf{p}_j^{\top} \mathbf{y}
$$

By Spectral Decomposition, and then simplifying: 

$$
\mathbf{y}^{\top} \mathbf{H} \mathbf{y} 
= \mathbf{y}^{\top} \mathbf{P} \Lambda \mathbf{P}^{\top} \mathbf{y}
= \mathbf{y}^{\top} \left( \sum_{j=1}^{n} \lambda_j \mathbf{p}_j \mathbf{p}_j^{\top} \right) \mathbf{y}
= \sum_{j=1}^{n} \lambda_j \mathbf{y}^{\top} \mathbf{p}_j \mathbf{p}_j^{\top} \mathbf{y}
= \sum_{j=1}^{n} \lambda_j x_j^2
$$

Individually, we know $x_j^2$ is non-negative. And as given, we know $\lambda_j$ is non-negative. Taken together, we know that each term $\lambda_j x_j^2$ is non-negative, such that their sum is non-negative as well. This means we have shown:

$$
\mathbf{y}^{\top} \mathbf{H} \mathbf{y} \geq 0
$$

Making the matrix $\mathbf{H}$ nonnegative definite.

Second direction complete! 

### Iff

Taken together, for $H$ is a symmetric matrix, $H$ is nonnegative definite $\iff$ all its eigenvalues are nonnegative.