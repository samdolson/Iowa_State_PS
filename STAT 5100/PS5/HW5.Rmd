---
title: "HW5"
output: pdf_document
author: "Sam Olson"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment 5

# Q1

Cell Means versus Additive Model

In class, we talked extensively about the two types of models when analyzing two treatment factors. For this question, consider your audience to be a first-year graduate student in a different discipline. Their understanding of statistics includes what you learned in Stat 5000 but they have not seen much Stat 5100 materials. The student is asking you for help trying to better understand:

## a) 

The two types of statistical models themselves.

At a high-level, first bear in mind nothing is fundamentally changing about the underlying data. Instead, these are different parametrizations of the data. So these are not diametrically opposed, and will actually provide the same estimates under certain conditions. Just different ways of modelling the relationship between variables/factors and the response/thing you're trying to predict. 

Cell Means: 

  - We are focused on estimating the average response for each unique combination of factor levels. 
  - Every combination of factors has their own mean, with a focus on estimating individual effects.  

Additive Model: 

  - Instead of estimating for each combination of factor levels, we are more interested in estimating the effects of the factors themselves. 
  - To do that, we assume the effect of a factor is consistent across levels, i.e. no unique interactions, interactions are "additive", hence the name. 

## b) 

The difference between both types of statistical models.

Cell Means: 

  - Directly estimates separate means for each combination of factor levels, i.e. the "individual, cell means".  
  - Estimating parameters equal to the number of unique factor-level combinations. 
  - No assumptions about the relationship between treatment means.
  - "More flexible", as we do not impose constraints like the Additive Model.

Additive Model: 

  - Decomposes what you're estimating (the $\beta$'s) into an overall mean, plus main effects (plus interactions effects too). 
  - Estimating fewer parameters due to imposed structure (constraints).
  - Assumes additive relationship between factors and their interactions. 
  - "Less flexible" (requires constraints such as sum-to-zero constraints). 

## c) 

Which one they should use for their own experiment that they plan on carrying out studying the effect of two treatments on some response $y$.

Both *can* work, at least in theory! They *can* also lead to very similar results, especially if the researcher is concerned with a more general, *"is there an effect?"* style questions, and if some assumptions are met (consistent interaction effects, for example). This does not mean that the two different models are naturally interchangeable though!  

There are some key distinctions to think about to determine if one seems more appropriate for the study. Also bear in mind, that this is a "study" rather vaguely. There is inherently a lot of subject matter expertise to consider, not the least of which being how the study is designed and whether the things being measured or treatments being used make sense or are interpretable. 

That being said, some considerations for the researcher, a.k.a. questions I'd ask: 

  - Prior to collecting any data or running any experiment, what you believe about the underlying relationship you're trying to test?
  - Do any of the underlying assumptions of the two models readily incorrect? 
  - Are you hoping to predict things, e.g. we expect an individual with particular characteristics to have y response, or are you more interested in the broad characteristics of your population of interest?

\newpage

# Q2

The Surprising Power of Reflection

## a) 

Watch the following video:
[The surprising power of reflection](https://thelearnerlab.us4.list-manage.com/track/click?u=650effaf591ee7171e0472541&id=ba9ae9d9c6&e=552083b364)

## b) 

Statistical Flaw in the First Study

The video showcases two studies: one immediately in the beginning within the first minute of the video and a second one introduced in the last 45 seconds of minute two. Listen carefully to the descriptions of each study. What *flaw*, statistically speaking, does the first study suffer from that does not show up in the second study? Briefly explain.

Overall, I think the statistical concept is a *randomized study design*, which does not exist in the first and is being used in the second. 

Details: 

In the study shown in the beginning of the video participants selected which group they belong to, i.e. chose to practice or to reflect. By contrast, participants in the second one were assigned their group, i.e. "treatments" were assigned and the sample under study was divided into a "control" group and an "experimental" group. 

## c) 

Reflection on Learning

In 2012, I took a workshop at ISU with Dr. Jan Wiersema called Project LEA/RN. Dr. Wiersema shared the following advice with us during the workshop -- it has stuck with me ever since:

> **It's the thinking about the doing that does the learning.**

Reflect on, and briefly summarize how *you* best learn new things. Share one or two tips on how you deal with challenging course material to ensure you learn it. You can reference experiences you have made as a student since joining our program but you can also reference an experience at some other time in your life.

Overall: I do best by having a mix of "alone" and "together" time. My general approach, if I have one, is to first give it a try without any assistance, references, or guidance. After that, I give myself some time to process, identify loose-ends/dead-ends, or walls I came up against and potential resources that could help, e.g. *"Wasn't there a formula on a slide we recently covered for that?"*. After reflecting, I then revisit, give it another go. Only after then do I feel comfortable discussing/sharing with others, with a focus on trying to *"knowing what I don't know"*. Sometimes I "rubber duck" it too, where my collaboration is to try explaining a topic, question, solution to an inanimate object. 

None of this is prescriptive though, not even for myself. Had I to think of an approach I use and tends more often than not to work for me, the above would be it. 

Tips: 

  - Have a constructive relationship with failure. I remember more problems I got wrong than problems I got right. 
  - Confusion and difficulty are the norm, especially when learning something considered "advanced" like grad school. 
  - Related, remember: A minority of people know how to calculate a derivative. I'd also wager even more people were ecstatic when they finished their last math class...in high school, sometimes undergrad.  
  - Sometimes the best thing to do is step away. It doesn't mean you're abandoning it, just that you need some time before trying again. 

\newpage

# Q3 

MS Exam Repository

Go to the old **MS exam repository** and look at the Methods I and II questions. Familiarize yourself with the questions in Methods I -- you should have an idea on how to answer most questions that are part of Methods I. Familiarize yourself with the questions in Methods II -- these you cannot answer yet, for the most part. Select one Methods II question you find intriguing and download the question document.

If you are in the PhD program in Statistics, in addition, pick and download a Methods II question from the old **PhD exam repository.**

## a) 

Selected Questions

Which questions did you pick? Answer by following this format:

*YEAR Methods II MS repository* (and *YEAR Methods II PhD repository* if you are a PhD student).

*MS: 2020* Part IV

*PhD: 2020* Part I 

## b) 

Submission of Selected Questions

Submit your question(s) as part of the homework. Note that I am not asking you to solve the question(s) (*yet*); I just want you to familiarize with them so you have an idea about expectations. For the new PhD qualifying exam, expect the level of difficulty to fall approximately in between old MS exam and old PhD exam questions.

```{r, eval = T, echo=FALSE, fig.cap="MS", out.width="100%"}
knitr::include_graphics("MethodsMS.png")
```

```{r, eval = T, echo=FALSE, fig.cap="PhD Pt1", out.width="100%"}
knitr::include_graphics("MethodsPhDPt1.png")
```

```{r, eval = T, echo=FALSE, fig.cap="PhD Pt2", out.width="100%"}
knitr::include_graphics("MethodsPhDPt2.png")
```

\newpage 

## c) 

Reflection on Learning

Reflect on your Fall semester; what has helped you learn and why? Note that it is the *why-part* of your answer that I am most interested in.

What helped: Sharing my work, asking for feedback, and proactively starting discussions with members of the cohort. I have a number of people I email drafts of my problemset, with the hopes of seeing how they compare. 

Why: I think there are two components to the why of this. Incomplete as an answer, as it will ever be. 

(1): In order to explain an answer, I need to know more than it takes to have *an answer*. I sometimes forget a key concept, have a typo causing massive downstream issues, or simply enough just misunderstood what a question was asking. Improving is collaborative and goes both ways. Getting other perspectives helps me not only with the end goal, getting an answer, but also with the journey, trite as I feel it may be to say. The process affirms my own knowledge and my confidence. 

(2): My primary struggle, an ongoing one, is having started graduate school "late". I was in industry for roughly 7 years after completing undergrad. I don't regret having spent that time away from academia as it was formative, but that is a source of imposter syndrome and FOMO. These anxieties are not unique to me though, and realizing that has been very relieving. Sharing something as direct as, *"Major hand waving at Q5, anyone have any better ideas, or whether I'm wrong there?"*, and hearing back...well, anything, lets me know I'm not alone. I'm not the only one struggling, and I'm not the only one looking for help. 

\newpage 

## References

- Di Stefano, Giada and Gino, Francesca and Pisano, Gary and Staats, Bradley R., Learning by Thinking: How Reflection Can Spur Progress Along the Learning Curve (February 6, 2023). Harvard Business School NOM Unit Working Paper No. 14-093, Kenan Institute of Private Enterprise Research Paper No. 2414478, Available at [https://dx.doi.org/10.2139/ssrn.2414478](https://dx.doi.org/10.2139/ssrn.2414478)
- [Learning by Thinking: How Reflection Can Spur Progress Along the Learning Curve](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2414478)