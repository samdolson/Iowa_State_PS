---
title: "HW6"
output: pdf_document
author: "Sam Olson"
---

# Outline 
  - Q5: Skeleton

# Q1

An ecologist takes data $$(x_i, Y_i), i = 1, \dots, n,$$ where $x_i > 0$ is the size of an area and $Y_i$ is the number of moss plants. The data are modeled assuming $x_1, \dots, x_n$ are fixed; $Y_1, \dots, Y_n$ are independent; and:

$$
Y_i \sim \text{Poisson}(\theta x_i)
$$

with parameter $\theta x_i$. Suppose that:

$$
\sum_{i=1}^n x_i = 5
$$

is known. Find an exact form of the most powerful (MP) test of size $\alpha = 9e^{-10}$ for testing:

$$
H_0 : \theta = 2 \quad \text{vs} \quad H_1 : \theta = 1.
$$

## Answer 

To start, we consider the likelihood ratio test statistic. The likelihood function under a general $\theta$ is:

$$
L(\theta) = \prod_{i=1}^n \frac{e^{-\theta x_i} (\theta x_i)^{Y_i}}{Y_i!}
$$

The likelihood ratio for testing $H_0: \theta = 2$ vs $H_1: \theta = 1$ is then given as a ratio of the likelihood under the alternative over the likelihood over the null:

$$
\Lambda = \frac{L(\theta = 1)}{L(\theta = 2)}
= \frac{\prod_{i=1}^n e^{-x_i}x_i^{Y_i}/Y_i!}{\prod_{i=1}^n e^{-2x_i}(2x_i)^{Y_i}/Y_i!} 
= e^{\sum_{i} x_i} \cdot 2^{-\sum_{i} Y_i} 
= e^{5} \cdot 2^{-T}
$$

where $T = \sum_{i=1}^n Y_i$ and substituting in other known quantities.

Then, via Neyman-Pearson, the MP test rejects $H_0$ when $\Lambda$ is large, which corresponds to small values of $T$ (since $\Lambda$ decreases as $T$ increases). 

Thus, the rejection region is of the form:

$$
R = \{T \leq c\}
$$

for some critical value $c$.

Under $H_0: \theta = 2$, we have:

$$
T \sim \text{Poisson}(2 \cdot \sum_{i} x_i) = \text{Poisson}(10)
$$

We need to find $c$ such that:

$$
P_{H_0}(T \leq c) \leq \alpha = 9 \cdot 10^{-10}
$$

We can compute these probabilities for $T \in \mathbb{Z}_{0}$:

  - $P(T = 0) = e^{-10} \approx 4.54 \times 10^{-5}$
  - $P(T = 1) = e^{-10} \cdot 10 \approx 4.54 \times 10^{-4}$
  - $P(T \leq 1) = P(T=0) + P(T=1) \approx 4.99 \times 10^{-4}$

Since $\alpha = 9 \times 10^{-10}$ is much smaller than $P(T \leq 1)$, we see that only $T = 0$ satisfies the size requirement in this problem, i.e.:

$$
P(T \leq 0) = e^{-10} \approx 4.54 \times 10^{-5} < \alpha
$$

However, $P(T \leq 0) \neq \alpha$, so we must find a suitable $\gamma \in [0,1]$ to satisfy equality. 

To that end, we would need to use a randomized test when $T = 1$, i.e. our test is of the form:

  - Reject with probability 1 if $T = 0$
  - Reject with probability $\gamma$ if $T = 1$
  - Never reject if $T \geq 2$

Where $\gamma$ solves:

$$
P(T=0) + \gamma P(T=1) = \alpha
$$

$$
e^{-10} + \gamma \cdot 10e^{-10} = 9e^{-10}
$$

Solving for $\gamma$: 

$$
\gamma = \frac{9e^{-10} - e^{-10}}{10e^{-10}} = 0.8
$$

So we may write the full form of the test now: 

$$
\phi_{H_0}(X) = 
\begin{cases}
1 & \text{if } T = \sum Y_i = 0 \\
\gamma = 0.8 & \text{if } T = \sum Y_i = 1 \\
0 & \text{otherwise}
\end{cases}
$$

\newpage 

# Q2

Problem 8.19: 

The random variable $X$ has pdf:

$$
f(x) = e^{-x}, \quad x > 0.
$$

One observation is obtained on the random variable:

$$
Y = X^\theta,
$$

and a test of:

$$
H_0 : \theta = 1 \quad \text{versus} \quad H_1 : \theta = 2
$$

needs to be constructed.

Find the UMP level $\alpha = 0.10$ test and compute the Type II Error probability.

## Hint 

Show that the form of the MP test involves rejecting $H_0$ if:

$$
e^{y - \sqrt{y}} / \sqrt{y} > k
$$

for some $k > 1$. 

(Skip the part involving $\alpha = 0.1$ or the Type II error part.)

## Answer 

Under the transformation $Y = X^\theta$, the inverse is $X = Y^{1/\theta}$, and:

$$
\frac{dx}{dy} = \frac{1}{\theta} y^{(1/\theta) - 1}
$$

The above Jacobian we will need for a change of variables, specifically, using the pdf of $X$, we have the pdf of $Y$ given by:

$$
f_Y(y|\theta) = f_X(y^{1/\theta}) \cdot \left| \frac{dx}{dy} \right| = e^{-y^{1/\theta}} \cdot \frac{1}{\theta} y^{(1/\theta) - 1}
$$

Where: $y > 0$

Via Neyman-Pearson, the MP test rejects $H_0$ for large values of the likelihood ratio, given by:

$$
\Lambda = \frac{f_Y(y|2)}{f_Y(y|1)}
$$

Substituting the pdfs, and simplifying:

$$
\Lambda = \frac{\frac{1}{2} y^{-1/2} e^{-y^{1/2}}}{e^{-y}} = \frac{1}{2} y^{-1/2} e^{y - \sqrt{y}}
$$

The rejection region is of the form:

$$
\Lambda > k \rightarrow \frac{e^{y - \sqrt{y}}}{\sqrt{y}} > 2k = k_{1}
$$

Where $k_{1} > 1$

Let $g(y) = \frac{e^{y - \sqrt{y}}}{\sqrt{y}}$. Take derivative, with the intent to show monotonicity (Spoiler: Non-monotonicity due to two distinct rejection regions) and also noting log is a monotonic transformation:

$$
\frac{d}{dy} \ln g(y) = \frac{d}{dy} \left( y - \sqrt{y} - \frac{1}{2} \ln y \right) = 1 - \frac{1}{2\sqrt{y}} - \frac{1}{2y}
$$

Note then: 
  
  - For $y \to 0^+$: The derivative $\approx -\frac{1}{2y} \to -\infty$ (decreasing).
  - For $y \to \infty$: The derivative $\approx 1$ (increasing).
  - $g(y)$ is a convex function, such that setting the derivative to zero is a minimum. (Has strictly positive second derivative.)

To that end: 

$$
1 - \frac{1}{2\sqrt{y}} - \frac{1}{2y} = 0 \implies y = 1
$$

So at $y = 1$, $g(y)$ has a minimum. Thus, $\Lambda(y) > k_1$ corresponds to:

$$
Y \leq c_0 \quad \text{or} \quad Y \geq c_1
$$
where $c_0 < 1 < c_1$.

The UMP level-$\alpha$ test rejects $H_0$ if:

$$
Y \leq c_0 \quad \text{or} \quad Y \geq c_1
$$

where $c_0, c_1$ are chosen such that:

$$
P_{H_0}(Y \leq c_0) + P_{H_0}(Y \geq c_1) = \alpha
$$

Under $H_0$ ($\theta = 1$), $Y = X \sim \text{Exp}(1)$, so, these are probabilities we can explicitly calculate::

$$
P_{H_0}(Y \leq c_0) = 1 - e^{-c_0}
$$

And

$$
P_{H_0}(Y \geq c_1) = e^{-c_1}
$$

Taken together then, the UMP test for $H_0: \theta = 1$ vs $H_1: \theta = 2$ rejects $H_0$ if:

$$
Y \leq c_0 \quad \text{or} \quad Y \geq c_1
$$

Written: 

$$
\phi_{H_0} (Y) = 
\begin{cases}
1 & Y \leq c_0 \quad \text{or} \quad Y \geq c_1 \\
0 & \text{otherwise}
\end{cases}
$$

Noting that $\gamma = 0$ in writing the above test function due to Y being a continuous random variable. 

Where $c_0, c_1$ satisfy:

$$
(1 - e^{-c_0}) + e^{-c_1} = \alpha
$$

And noting to *"Skip the part involving $\alpha = 0.1$ or the Type II error part."*

\newpage 

# Q3

Problem 8.20, Casella and Berger (2nd Edition).

Let $X$ be a random variable whose pmf under $H_0$ and $H_1$ is given by:

| $x$          | 1    | 2    | 3    | 4    | 5    | 6    | 7    |
|--------------|------|------|------|------|------|------|------|
| $f(x|H_0)$   | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.94 |
| $f(x|H_1)$   | 0.06 | 0.05 | 0.04 | 0.03 | 0.02 | 0.01 | 0.79 |

Use the Neyman–Pearson Lemma to find the most powerful test for $H_0$ versus $H_1$ with size:

$$
\alpha = 0.04.
$$

Compute the probability of Type II Error for this test.

## Hint:

It holds that:

$$
\frac{f(x | H_1)}{f(x | H_0)} = 7 - x + \frac{79}{94} I(x = 7)
$$

over the support $x = 1, 2, \dots, 7$, where $I(\cdot)$ denotes the indicator function.

## Answer

The likelihood ratio is given by the Hint:

$$
\Lambda = \frac{f(x|H_1)}{f(x|H_0)} = 7 - x + \frac{79}{94} I(x = 7),
$$

where $I(\cdot)$ is the indicator function. 

Notably: 

  - For $x = 1, \dots, 6$, the LR simplifies to $\Lambda(x) = 7 - x$.
  - For $x = 7$, $\Lambda(7) = \frac{79}{94} \approx 0.84$.
  - The likelihood ratio is decreasing in $x$, so the MP test rejects $H_0$ for the smallest values of $x$.
  - The smaller the $x$, the larger the likelihood ratio. 

Using the above information, we can directly calculate the following: 

| $x$ | LR $\Lambda(x)$ | $f(x|H_0)$ | Cumulative $P_{H_0}$ |
|---------|----------------------|----------------|--------------------------|
| 1       | 6.00                 | 0.01           | 0.01                     |
| 2       | 5.00                 | 0.01           | 0.02                     |
| 3       | 4.00                 | 0.01           | 0.03                     |
| 4       | 3.00                 | 0.01           | 0.04                     |
| 5       | 2.00                 | 0.01           | 0.05                     |
| 6       | 1.00                 | 0.01           | 0.06                     |
| 7       | 0.84                 | 0.94           | 1.00                     |


To achieve the desired size, $\alpha = 0.04$, we consider where the cumulative probability, $P_{H_0}$, achieves $\alpha$, which is at 4. As this is cumulative then, we have the rejection region given by:

$$
R = \{1, 2, 3, 4\}
$$

The Type II error probability $\beta$ is the probability of not rejecting $H_0$ when $H_1$ is true:

$$
\beta = P_{H_1}(X \notin R) = P_{H_1}(X = 5, 6, 7) = f(5|H_1) + f(6|H_1) + f(7|H_1) = 0.02 + 0.01 + 0.79 = 0.82
$$

Giving us a Type II Error Probability of $\beta = 0.82$.

\newpage 

# Q4

Recall Method I for finding Uniformly Most Powerful (UMP) tests:

To find a UMP size $\alpha$ test for $H_0 : \theta \in \Theta_0$ vs $H_1 : \theta \notin \Theta_0$, suppose we can fix $\theta_0 \in \Theta_0$ suitably and then use the Neyman–Pearson lemma to find an MP size $\alpha$ test $\varphi(\tilde{X})$ for:

$$
H_0 : \theta = \theta_0 \quad \text{vs} \quad H_1 : \theta = \theta_1,
$$

where:

## a) 

$\varphi(\tilde{X})$ does not depend on $\theta_1 \notin \Theta_0$, and

## b) 

$\max_{\theta \in \Theta_0} E_\theta \varphi(\tilde{X}) = \alpha.$

## Proof

Show that if a) and b) both hold, then $\varphi(\tilde{X})$ must be a UMP size $\alpha$ test for $H_0 : \theta \in \Theta_0$ vs $H_1 : \theta \notin \Theta_0$.

## Hint:

From b), the size of the test rule $\varphi(\tilde{X})$ is correct. So, by definition of a UMP test, it is necessary to prove that if $\bar{\varphi}(\tilde{X})$ is any other test of $H_0 : \theta \in \Theta_0$ vs $H_1 : \theta \notin \Theta_0$ with size:

$$
\max_{\theta \in \Theta_0} E_\theta \bar{\varphi}(\tilde{X}) \leq \alpha,
$$

then $\varphi(\tilde{X})$ has more power over the parameter subspace of $H_1$ than $\bar{\varphi}(\tilde{X})$, i.e.,

$$
E_{\theta} \varphi(\tilde{X}) \geq E_{\theta} \bar{\varphi}(\tilde{X}) \quad \text{for any } \theta \notin \Theta_0.
$$

In other words, pick/fix some $\theta_1 \notin \Theta_0$ and argue that:

$$
E_{\theta_1} \varphi(\tilde{X}) \geq E_{\theta_1} \bar{\varphi}(\tilde{X})
$$

must hold. The way to do this is to take the test $\bar{\varphi}(\tilde{X})$ and apply it to testing $H_0 : \theta = \theta_0$ vs. $H_1 : \theta = \theta_1$.

### Answer 

Assume a) and b) hold. The goal then is to show that $\varphi(\tilde{X})$ is UMP for $H_0: \theta \in \Theta_0$ vs. $H_1: \theta \notin \Theta_0$.

Consider testing:

$$
H_0: \theta = \theta_0 \quad \text{vs.} \quad H_1: \theta = \theta_1
$$

Where $\theta_0$ and $\theta_1$ are suitable parameters belonging to $\Theta_0$ and $\theta_1$ respectively. 

By Neyman Pearson, $\varphi(\tilde{X})$ is MP at size $\alpha$ for this test.

Let $\bar{\varphi}(\tilde{X})$ be another test with:

$$
\sup_{\theta \in \Theta_0} E_\theta \bar{\varphi}(\tilde{X}) \leq \alpha
$$

In particular, $E_{\theta_0} \bar{\varphi}(\tilde{X}) \leq \alpha$.

Since $\varphi(\tilde{X})$ is MP for $\theta = \theta_0$ vs. $\theta = \theta_1$, it satisfies:

$$
E_{\theta_1} \varphi(\tilde{X}) \geq E_{\theta_1} \bar{\varphi}(\tilde{X})
$$

Given condition a) holds then, we know that $\varphi(\tilde{X})$ does not depend on $\theta_1$. Thus, the inequality holds for all $\theta_1 \notin \Theta_0$, proving $\varphi(\tilde{X})$ is UMP.

### An Alternative Approach 

I believe there is also another approach via a proof by contradiction. To that end: 

To start, assume (for contradiction) that $\varphi(\tilde{X})$ is not UMP of size $\alpha$, yet still meets conditions a) and b).  

Then $\exists$ a test $\bar{\varphi}(\tilde{X})$ such that:  

  - $\sup_{\theta \in \Theta_0} E_\theta[\bar{\varphi}(\tilde{X})] \leq \alpha$ (level $\alpha$),  
  - $\exists \theta_1 \notin \Theta_0$ with $E_{\theta_1}[\bar{\varphi}(\tilde{X})] > E_{\theta_1}[\varphi(\tilde{X})]$.  

Then, fix $\theta_0 \in \Theta_0$ where size $\alpha$ is attained:  

  - By condition b), $E_{\theta_0}[\varphi(\tilde{X})] = \alpha$.  
  - Additionally, we know $E_{\theta_0}[\bar{\varphi}(\tilde{X})] \leq \alpha$.  

Via Neyman-Pearson for $H_0: \theta = \theta_0$ vs $H_1: \theta = \theta_1$:  

  - $\varphi(\tilde{X})$ is MP of size $\alpha$ for this test (via condition a) and Neyman-Pearson).  

However, $\bar{\varphi}(\tilde{X})$ has:  
  
  - Size $\leq \alpha$ (since $E_{\theta_0}[\bar{\varphi}] \leq \alpha$),  
  - Higher power at $\theta_1$ (since $E_{\theta_1}[\bar{\varphi}] > E_{\theta_1}[\varphi]$).  

This is a contradiction, as Neyman-Pearson guarantees no such $\bar{\varphi}$ can exist (any other MP test with the same size cannot have higher power!)  

Thus, we conclude that $\varphi(\tilde{X})$ is UMP of size $\alpha$.  

\newpage 

# Q5

Problem 8.23, Casella and Berger (2nd Edition).

Suppose $X$ is one observation from a population with $\text{Beta}(\theta, 1)$ pdf.

## a) 

For testing:

$$
H_0 : \theta \leq 1 \quad \text{versus} \quad H_1 : \theta > 1,
$$

find the size and sketch the power function of the test that rejects $H_0$ if:

$$
X > \frac{1}{2}.
$$

## b) 

Find the most powerful level-$\alpha$ test of:

$$
H_0 : \theta = 1 \quad \text{versus} \quad H_1 : \theta = 2.
$$

## c) 

Is there a UMP test of:

$$
H_0 : \theta \leq 1 \quad \text{versus} \quad H_1 : \theta > 1?
$$

If so, find it. If not, prove so.