---
title: "HW6"
output: pdf_document
author: "Sam Olson"
---

# Q1

An ecologist takes data $$(x_i, Y_i), i = 1, \dots, n,$$ where $x_i > 0$ is the size of an area and $Y_i$ is the number of moss plants. The data are modeled assuming $x_1, \dots, x_n$ are fixed; $Y_1, \dots, Y_n$ are independent; and:

$$
Y_i \sim \text{Poisson}(\theta x_i)
$$

with parameter $\theta x_i$. Suppose that:

$$
\sum_{i=1}^n x_i = 5
$$

is known. Find an exact form of the most powerful (MP) test of size $\alpha = 9e^{-10}$ for testing:

```{r}
alpha <- 9 * exp(-10)
alpha
```

$$
H_0 : \theta = 2 \quad \text{vs} \quad H_1 : \theta = 1.
$$

## Answer 

To start, we consider the likelihood ratio test statistic. The likelihood function under a parameter $\theta$ is:

$$
L(\theta) = \prod_{i=1}^n \frac{e^{-\theta x_i} (\theta x_i)^{Y_i}}{Y_i!}
$$

The likelihood ratio for testing $H_0: \theta = 2$ vs $H_1: \theta = 1$ is then given as a ratio of the likelihood functions where the alternative is the numerator and the null is the denominator, which is: 

$$
\Lambda = \frac{L(\theta = 1)}{L(\theta = 2)}
= \frac{\prod_{i=1}^n e^{-x_i}x_i^{Y_i}/Y_i!}{\prod_{i=1}^n e^{-2x_i}(2x_i)^{Y_i}/Y_i!} 
= e^{\sum_{i} x_i} \cdot 2^{-\sum_{i} Y_i} 
= e^{5} \cdot 2^{-T}
$$

For simplification, we let $T = \sum_{i=1}^n Y_i$ and cancel terms when applicable.

Then, via Neyman-Pearson, the MP test rejects $H_0$ when $\Lambda$ is large, which corresponds to small values of $T$ (since $\Lambda$ decreases as $T$ increases). 

Thus, the rejection region is of the form:

$$
R = \{T \leq c\}
$$

for some critical value $c$.

Under $H_0: \theta = 2$, we have:

$$
T \sim \text{Poisson}(2 \cdot \sum_{i} x_i) = \text{Poisson}(10)
$$

We need to find $c$ such that:

$$
P_{H_0}(T \leq c) \leq \alpha = 9 \cdot 10^{-10}
$$

We can compute these probabilities for $T \in \mathbb{Z}_{0}$:

```{r}
alpha > exp(-10)
alpha > 10 * exp(-10)
```

  - $P(T = 0) = e^{-10} \approx 4.54 \cdot 10^{-5}$
  - $P(T = 1) = e^{-10} \cdot 10 \approx 4.54 \cdot 10^{-4}$
  - $P(T \leq 1) = P(T=0) + P(T=1) \approx 4.99 \cdot 10^{-4}$

Since $\alpha = 9 \times 10^{-10}$ is much smaller than $P(T \leq 1)$, we see that only $T = 0$ satisfies the size requirement in this problem.

However, $P(T \leq 0) \neq \alpha$, so we must find a suitable $\gamma \in [0,1]$ to satisfy equality. 

To that end, we would need to use a randomized test when $T = 1$, i.e. our test is of the form:

  - Reject with probability 1 if $T = 0$
  - Reject with probability $\gamma$ if $T = 1$
  - Never reject if $T \geq 2$

We then need to calculate $\gamma \in [0,1]$, using:

$$
P(T=0) + \gamma P(T=1) = \alpha
$$

Solving for $\gamma$, substituting known values: 

$$
e^{-10} + \gamma \cdot 10e^{-10} = 9e^{-10}
$$

$$
\gamma = \frac{9e^{-10} - e^{-10}}{10e^{-10}} = 0.8
$$

With special note of distinguishing between `e-10` as $10^{-10}$ vs. $e^{-10}$ = `exp(-10)`

So we may write the full form of the test as: 

$$
\varphi_{H_0}(X) = 
\begin{cases}
1 & \text{if } T = \sum Y_i = 0 \\
\gamma = 0.8 & \text{if } T = \sum Y_i = 1 \\
0 & \text{otherwise}
\end{cases}
$$

\newpage 

# Q2

Problem 8.19: 

The random variable $X$ has pdf:

$$
f(x) = e^{-x}, \quad x > 0.
$$

One observation is obtained on the random variable:

$$
Y = X^\theta,
$$

and a test of:

$$
H_0 : \theta = 1 \quad \text{versus} \quad H_1 : \theta = 2
$$

needs to be constructed.

Find the UMP level $\alpha = 0.10$ test and compute the Type II Error probability.

## Hint 

Show that the form of the MP test involves rejecting $H_0$ if:

$$
e^{y - \sqrt{y}} / \sqrt{y} > k
$$

for some $k > 1$. 

(Skip the part involving $\alpha = 0.1$ or the Type II error part.)

## Answer

Under the transformation $Y = X^\theta$, the inverse is $X = Y^{1/\theta}$, and:

$$
\frac{dx}{dy} = \frac{1}{\theta} y^{(1/\theta) - 1}
$$

Using the change-of-variables formula, the pdf of $Y$ is:

$$
f_Y(y|\theta) = f_X(y^{1/\theta}) \cdot \left| \frac{dx}{dy} \right| = e^{-y^{1/\theta}} \cdot \frac{1}{\theta} y^{(1/\theta) - 1}
$$

for $y > 0$.

By the Neyman–Pearson, the MP test rejects $H_0$ for large values of the likelihood ratio, given by:

$$
\Lambda = \frac{f_Y(y|2)}{f_Y(y|1)} = \frac{\frac{1}{2} y^{-1/2} e^{-y^{1/2}}}{e^{-y}} = \frac{1}{2} y^{-1/2} e^{y - \sqrt{y}}
$$

Thus, the rejection region has the form:

$$
\Lambda > k  \iff \frac{e^{y - \sqrt{y}}}{\sqrt{y}} > 2k = k_1
$$

for some $k_1 > 1$. This last condition is included to ensure the test has size less than 1, i.e. to ensure that we do not always reject the null hypothesis (reject with probability 1, which would occur for values of $k_1 \leq 1$). 

Noting Y is a continuous random variable, we again may set the value $\gamma = 0$, such that our UMP test function focuses solely on two unique cases. 

To that end, the UMP test is given by:

$$
\varphi(y) =
\begin{cases}
1 & \text{if } \dfrac{e^{y - \sqrt{y}}}{\sqrt{y}} > k_1 \\
0 & \text{otherwise}
\end{cases}
$$

for some $k_1 > 1$.

And noting to, *"(Skip the part involving $\alpha = 0.1$ or the Type II error part.)"*

\newpage 

# Q3

Problem 8.20, Casella and Berger (2nd Edition).

Let $X$ be a random variable whose pmf under $H_0$ and $H_1$ is given by:

| $x$          | 1    | 2    | 3    | 4    | 5    | 6    | 7    |
|--------------|------|------|------|------|------|------|------|
| $f(x|H_0)$   | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.94 |
| $f(x|H_1)$   | 0.06 | 0.05 | 0.04 | 0.03 | 0.02 | 0.01 | 0.79 |

Use the Neyman–Pearson Lemma to find the most powerful test for $H_0$ versus $H_1$ with size:

$$
\alpha = 0.04.
$$

Compute the probability of Type II Error for this test.

## Hint:

It holds that:

$$
\frac{f(x | H_1)}{f(x | H_0)} = 7 - x + \frac{79}{94} I(x = 7)
$$

over the support $x = 1, 2, \dots, 7$, where $I(\cdot)$ denotes the indicator function.

## Answer

The likelihood ratio is given by the Hint:

$$
\Lambda = \frac{f(x|H_1)}{f(x|H_0)} = 7 - x + \frac{79}{94} I(x = 7),
$$

where $I(\cdot)$ is the indicator function. 

Notably: 

  - The likelihood ratio is decreasing in $x$, so the MP test rejects $H_0$ for the smallest values of $x$.
  - The smaller the $x$, the larger the likelihood ratio. 
  - For $x = 1, \dots, 6$, the LR simplifies to $\Lambda = 7 - x$.
  - For $x = 7$, $\Lambda = \frac{0.79}{0.94} \approx 0.84$.

\newpage

Using the above information, we can directly calculate the following: 

| $x$     | $\Lambda$            | $f(x|H_0)$     | Cumulative  |
|---------|----------------------|----------------|--------------------------|
| 1       | 6.00                 | 0.01           | 0.01                     |
| 2       | 5.00                 | 0.01           | 0.02                     |
| 3       | 4.00                 | 0.01           | 0.03                     |
| 4       | 3.00                 | 0.01           | 0.04                     |
| 5       | 2.00                 | 0.01           | 0.05                     |
| 6       | 1.00                 | 0.01           | 0.06                     |
| 7       | 0.84                 | 0.94           | 1.00                     |


To achieve the desired size, $\alpha = 0.04$, we consider where the cumulative probability, $P_{H_0}$, achieves $\alpha$, which is at 4. As this is cumulative then, we have the rejection region given by:

$$
R = \{1, 2, 3, 4\}
$$

The Type II error probability $\beta$ is the probability of not rejecting $H_0$ when $H_1$ is true:

$$
\beta = P_{H_1}(X \notin R) = P_{H_1}(X = 5, 6, 7) = f(5|H_1) + f(6|H_1) + f(7|H_1) = 0.02 + 0.01 + 0.79 = 0.82
$$

Giving us a Type II Error Probability of $\beta = 0.82$.

\newpage 

# Q4

Recall Method I for finding Uniformly Most Powerful (UMP) tests:

To find a UMP size $\alpha$ test for $H_0 : \theta \in \Theta_0$ vs $H_1 : \theta \notin \Theta_0$, suppose we can fix $\theta_0 \in \Theta_0$ suitably and then use the Neyman–Pearson lemma to find an MP size $\alpha$ test $\varphi(\tilde{X})$ for:

$$
H_0 : \theta = \theta_0 \quad \text{vs} \quad H_1 : \theta = \theta_1,
$$

where:

## a) 

$\varphi(\tilde{X})$ does not depend on $\theta_1 \notin \Theta_0$, and

## b) 

$\max_{\theta \in \Theta_0} E_\theta \varphi(\tilde{X}) = \alpha.$

## Proof

Show that if a) and b) both hold, then $\varphi(\tilde{X})$ must be a UMP size $\alpha$ test for $H_0 : \theta \in \Theta_0$ vs $H_1 : \theta \notin \Theta_0$.

## Hint:

From b), the size of the test rule $\varphi(\tilde{X})$ is correct. So, by definition of a UMP test, it is necessary to prove that if $\bar{\varphi}(\tilde{X})$ is any other test of $H_0 : \theta \in \Theta_0$ vs $H_1 : \theta \notin \Theta_0$ with size:

$$
\max_{\theta \in \Theta_0} E_\theta \bar{\varphi}(\tilde{X}) \leq \alpha,
$$

then $\varphi(\tilde{X})$ has more power over the parameter subspace of $H_1$ than $\bar{\varphi}(\tilde{X})$, i.e.,

$$
E_{\theta} \varphi(\tilde{X}) \geq E_{\theta} \bar{\varphi}(\tilde{X}) \quad \text{for any } \theta \notin \Theta_0.
$$

In other words, pick/fix some $\theta_1 \notin \Theta_0$ and argue that:

$$
E_{\theta_1} \varphi(\tilde{X}) \geq E_{\theta_1} \bar{\varphi}(\tilde{X})
$$

must hold. The way to do this is to take the test $\bar{\varphi}(\tilde{X})$ and apply it to testing $H_0 : \theta = \theta_0$ vs. $H_1 : \theta = \theta_1$.

### Answer 

Assume a) and b) hold. The goal then is to show that $\varphi(\tilde{X})$ is UMP for $H_0: \theta \in \Theta_0$ vs. $H_1: \theta \notin \Theta_0$.

We consider fixing the null and alternative hypotheses respectively by:

$$
H_0: \theta = \theta_0 \quad \text{vs.} \quad H_1: \theta = \theta_1
$$

Where $\theta_0$ and $\theta_1$ are suitable parameters belonging to $\Theta_0$ and $\theta_1$, again resp.  

By Neyman Pearson, $\varphi(\tilde{X})$ is MP at size $\alpha$ for this test.

Let $\bar{\varphi}(\tilde{X})$ be another test with:

$$
\sup_{\theta \in \Theta_0} E_\theta \bar{\varphi}(\tilde{X}) \leq \alpha
$$

In particular, $E_{\theta_0} \bar{\varphi}(\tilde{X}) \leq \alpha$.

Since $\varphi(\tilde{X})$ is MP for $\theta = \theta_0$ vs. $\theta = \theta_1$, it satisfies:

$$
E_{\theta_1} \varphi(\tilde{X}) \geq E_{\theta_1} \bar{\varphi}(\tilde{X})
$$

Given condition a) holds then, we know that $\varphi(\tilde{X})$ does not depend on $\theta_1$. Thus, the inequality holds for all $\theta_1 \notin \Theta_0$, proving $\varphi(\tilde{X})$ is UMP.

### An Alternative Approach 

I believe there is also another approach via a proof by contradiction. To that end: 

To start, assume (for contradiction) that $\varphi(\tilde{X})$ is not UMP of size $\alpha$, yet still meets conditions a) and b).  

Then $\exists$ a test $\bar{\varphi}(\tilde{X})$ such that:  

  - $\sup_{\theta \in \Theta_0} E_\theta[\bar{\varphi}(\tilde{X})] \leq \alpha$ (level $\alpha$),  
  - $\exists \theta_1 \notin \Theta_0$ with $E_{\theta_1}[\bar{\varphi}(\tilde{X})] > E_{\theta_1}[\varphi(\tilde{X})]$.  

Fix $\theta_0 \in \Theta_0$ where size $\alpha$ is attained:  

  - By condition b), $E_{\theta_0}[\varphi(\tilde{X})] = \alpha$.  
  - We also know $E_{\theta_0}[\bar{\varphi}(\tilde{X})] \leq \alpha$.  

Via Neyman-Pearson for $H_0: \theta = \theta_0$ vs $H_1: \theta = \theta_1$, $\varphi(\tilde{X})$ is MP of size $\alpha$ for this test (via condition a) and Neyman-Pearson).  

However, $\bar{\varphi}(\tilde{X})$ has:  
  
  - Size $\leq \alpha$ (since $E_{\theta_0}[\bar{\varphi}] \leq \alpha$),  
  - Higher power at $\theta_1$ (since $E_{\theta_1}[\bar{\varphi}] > E_{\theta_1}[\varphi]$).  

This is a contradiction, as Neyman-Pearson guarantees no such $\bar{\varphi}$ can exist (any other MP test with the same size cannot have higher power!)  

Thus, we conclude that $\varphi(\tilde{X})$ is UMP of size $\alpha$.  

\newpage 

# Q5

Problem 8.23, Casella and Berger (2nd Edition).

Suppose $X$ is one observation from a population with $\text{Beta}(\theta, 1)$ pdf.

## a) 

For testing:

$$
H_0 : \theta \leq 1 \quad \text{versus} \quad H_1 : \theta > 1,
$$

find the size and sketch the power function of the test that rejects $H_0$ if:

$$
X > \frac{1}{2}.
$$

### Answer

The power function, $\beta(\theta)$, is by definition the probability of rejecting $H_0$ under a given $\theta$:

$$
\beta(\theta) = P_\theta \left( X > \frac{1}{2} \right) = \int_{1/2}^1 \theta x^{\theta - 1} \, dx=  \left. x^\theta \right|_{1/2}^1 = 1 - \left( \frac{1}{2} \right)^\theta = 1 - \frac{1}{2^\theta}
$$

Then, the size, is by definition the supremum of $\beta(\theta)$ under $H_0$ ($\theta \leq 1$).  

Since $\beta(\theta)$ is strictly increasing in $\theta$ (because $\beta'(\theta) = \ln(2) \cdot 2^{-\theta} > 0$), the maximum occurs at $\theta = 1$, which is:

$$
\sup_{\theta \leq 1} \beta(\theta) = \beta(1) = 1 - \frac{1}{2} = \frac{1}{2}
$$

Expectation of the sketch: ($\theta = 1$, $\beta(1) = 0.5$).   

### A Sketch

```{r}
# setup
theta_vals <- seq(0.01, 5, length.out = 300)
power_vals <- 1 - (1 / 2^theta_vals)

# plotting 
plot(theta_vals, power_vals, type = "l", lwd = 2,
     xlab = expression(theta), ylab = expression(beta(theta)),
     main = "Power Function: Reject if X > 1/2")

abline(h = 0.5, col = "red", lty = 2)

legend("bottomright", legend = c("Size at expression(theta) = 1"),
       col = c("red"), lty = 2, bty = "n")
```

## b) 

Find the most powerful level-$\alpha$ test of:

$$
H_0 : \theta = 1 \quad \text{versus} \quad H_1 : \theta = 2.
$$

### Answer

We find the MP test via Neyman-Pearson: The MP test rejects $H_0$ when the likelihood ratio exceeds a threshold k.

Via our now typical formulation, our likelihood ratio is given by: 

$$
\Lambda = \frac{f(x \mid \theta = 2)}{f(x \mid \theta = 1)} = \frac{2x^{2-1}}{1x^{1-1}} = 2x
$$

The test rejects $H_0$ when $\Lambda = 2x > k \rightarrow x > \frac{k}{2} = t$, where t denotes our critical value.

Using the above, the size constraint requires:  

$$
P_{\theta=1}(X > t) = \alpha
$$

For $\theta = 1$, $X \sim \text{Uniform}(0,1)$, and the probability can be explicitly evaluated and solved for t:

$$
P(X > t) = 1 - t = \alpha \rightarrow t = 1 - \alpha
$$

Taken together, the most powerful level-$\alpha$ test is given by:

$$
\varphi_{H_0}(X) = 
\begin{cases}
1 &  X > 1 - \alpha \\
0 & \text{otherwise}
\end{cases}
$$

Note: The above takes advantage of dealing with a continuous random variable, meaning we can have $\gamma = 0$ for the "coin toss" scenario $\gamma$ in the test function, further simplifying the number of unique cases for our test function. 

## c) 

Is there a UMP test of:

$$
H_0 : \theta \leq 1 \quad \text{versus} \quad H_1 : \theta > 1
$$

If so, find it. If not, prove so.

### Answer 

$\text{Beta}(\theta, 1)$ is in the exponential family since its likelihood function may be written as: 

$$
f_{X}(x) = \frac{x^{\theta - 1}(1 - x)^{1 - 1}}{B(\theta, 1)} = \frac{1}{B(\theta, 1)} \exp(\ln(x^{\theta - 1})) = \left(\frac{1}{B(\theta, 1)} \cdot 1 \right) \cdot \exp\left[ (\theta - 1) \ln(x) \right]
$$

Where 

$$
c(\theta) = \frac{1}{B(\theta, 1)}, \quad h(x) = 1, \quad q_1(\theta) = \theta - 1, \quad t_1(x) = \ln(x)
$$

Since $q_1(\theta)$ is a nondecreasing function, $\{f(x|\theta): \theta \in \Theta\}$ has MLR in $T = t_1(x)$. 

Taking advantage of MLR, we know that a size $\alpha$ UMP test for:

$$
H_0: \theta \leq \theta_0 \quad \text{vs} \quad H_1: \theta > \theta_0
$$

is given by:

$$
\varphi(x) = 
\begin{cases}
1 & \text{if } t_1(x) > k \\
\gamma & \text{if } t_1(x) = k \\
0 & \text{if } t_1(x) < k
\end{cases}
$$

with size $E_{\theta_0}(\varphi(x)) = \alpha$.

Because the exponential family is continuous (i.e., $X$ is a continuous random variable), we may choose $\gamma = 0$, simplifying the test function to:

$$
\varphi(x) = 
\begin{cases}
1 & \text{if } \ln(x) > k \\
0 & \text{otherwise}
\end{cases}
$$

We then solve for the critical value $k$ using the size condition:

$$
\alpha = E_{\theta_0}[\varphi(x)] = P(\ln(x) > k) = P(x > e^k)
$$

Since $X \sim \text{Beta}(1,1) = \text{Uniform}(0,1)$ under $\theta_0 = 1$, we have:

$$
P(x > e^k) = 1 - e^k = \alpha \quad \rightarrow \quad e^k = 1 - \alpha \quad \rightarrow \quad k = \ln(1 - \alpha)
$$

Thus, the MP test is:

$$
\varphi(x) = 
\begin{cases}
1 & \text{if } x > 1 - \alpha \\
0 & \text{otherwise}
\end{cases}
$$

### Alternative Method

Note: The following is based on usage of language/ideas from Casella. After discussing with Dr. Nordman, the method used is correct for this problem, but unnecessary compared to the first approach used above, particularly regarding the usage of Karlin-Rubin.

We start by checking whether the likelihood ratio is monotonic. To that end, for $\theta_2 > \theta_1$, the likelihood ratio is given by:

$$
\Lambda = \frac{f(x|\theta_2)}{f(x|\theta_1)} = \frac{\theta_2}{\theta_1}x^{\theta_2-\theta_1}
$$

Since $\theta_2 - \theta_1 > 0$ and $x \in (0,1)$, the function $x^{\theta_2 - \theta_1}$ is increasing in $x$, which in turn means the likelihood ratio $\Lambda$ is increasing in $x$, which in turn means the family has a monotone likelihood ratio in $x$.

Importantly, the family having a MLR in $x$ allows us to utilize Karlin-Rubin, i.e. the test that rejects for large values of $X$ is UMP for $H_0: \theta \leq 1$ vs $H_1: \theta > 1$.

We choose $t$ such that:

$$
\sup_{\theta \leq 1} P_\theta(X > t) = \alpha
$$

Under $\theta = 1$ (where the sup is attained), $X \sim \text{Uniform}(0,1)$, so:

$$
P(X > t) = 1 - t = \alpha \rightarrow t = 1 - \alpha
$$

From part b), the MP test for $\theta = 1$ vs $\theta = 2$ was given by:  

$$
\varphi_{H_0}(X) = 
\begin{cases}
1 &  X > 1 - \alpha \\
0 & \text{otherwise}
\end{cases}
$$

Which notably does not include the $\theta$ value! Because of this, it is UMP for all $\theta > 1$.  

So the UMP level-$\alpha$ test is the same as the MP test in part b):  

$$
\varphi(x) = 
\begin{cases}
1 & \text{if } x > 1 - \alpha \\
0 & \text{otherwise}
\end{cases}
$$