---
title: "HW6"
output: pdf_document
author: "Sam Olson"
---

# Outline 
  - Q1-Q4: Edit
  - Q5: Skeleton

# Q1

An ecologist takes data $$(x_i, Y_i), i = 1, \dots, n,$$ where $x_i > 0$ is the size of an area and $Y_i$ is the number of moss plants. The data are modeled assuming $x_1, \dots, x_n$ are fixed; $Y_1, \dots, Y_n$ are independent; and:

$$
Y_i \sim \text{Poisson}(\theta x_i)
$$

with parameter $\theta x_i$. Suppose that:

$$
\sum_{i=1}^n x_i = 5
$$

is known. Find an exact form of the most powerful (MP) test of size $\alpha = 9e^{-10}$ for testing:

$$
H_0 : \theta = 2 \quad \text{vs} \quad H_1 : \theta = 1.
$$

## Answer 

We begin by constructing the likelihood ratio test statistic. The likelihood function under a general $\theta$ is:

$$
L(\theta) = \prod_{i=1}^n \frac{e^{-\theta x_i} (\theta x_i)^{Y_i}}{Y_i!}
$$

The likelihood ratio for testing $H_0: \theta = 2$ vs $H_1: \theta = 1$ is:

$$
\Lambda(\mathbf{Y}) = \frac{L(\theta = 1)}{L(\theta = 2)}
= \frac{\prod_{i=1}^n e^{-x_i}x_i^{Y_i}/Y_i!}{\prod_{i=1}^n e^{-2x_i}(2x_i)^{Y_i}/Y_i!} 
= e^{\sum x_i} \cdot 2^{-\sum Y_i} 
= e^{5} \cdot 2^{-T}
$$

where $T = \sum_{i=1}^n Y_i$.

The Neyman-Pearson lemma tells us the most powerful test rejects $H_0$ when $\Lambda(\mathbf{Y})$ is large, which corresponds to small values of $T$ (since $\Lambda$ decreases as $T$ increases). 

Thus, the rejection region is of the form:

$$
R = \{T \leq c\}
$$

for some critical value $c$.

Under $H_0: \theta = 2$, we have:

$$
T \sim \text{Poisson}(2 \cdot \sum x_i) = \text{Poisson}(10)
$$

We need to find $c$ such that:

$$
P_{H_0}(T \leq c) \leq \alpha = 9 \times 10^{-10}
$$

Compute the Poisson CDF for $T \sim \text{Poisson}(10)$:

- $P(T = 0) = e^{-10} \approx 4.54 \times 10^{-5}$
- $P(T = 1) = e^{-10} \cdot 10 \approx 4.54 \times 10^{-4}$
- $P(T \leq 1) = P(T=0) + P(T=1) \approx 4.99 \times 10^{-4}$

Since $\alpha = 9 \times 10^{-10}$ is much smaller than $P(T \leq 1)$, we see that only $T = 0$ satisfies:

$$
P(T \leq 0) = e^{-10} \approx 4.54 \times 10^{-5} < \alpha
$$

The most powerful test of size $\leq \alpha$ is:

$$
\text{Reject } H_0 \text{ if and only if } T = 0
$$

The actual size of this test is $P_{H_0}(T = 0) = e^{-10} \approx 4.54 \times 10^{-5}$, which is less than $\alpha = 9 \times 10^{-10}$.

To achieve exactly $\alpha = 9 \times 10^{-10}$, we would need to use a randomized test when $T = 1$:

- Reject with probability 1 if $T = 0$
- Reject with probability $\gamma$ if $T = 1$
- Never reject if $T \geq 2$

Where $\gamma$ solves:

$$
P(T=0) + \gamma P(T=1) = \alpha \\
e^{-10} + \gamma \cdot 10e^{-10} = 9e^{-10} \\
\gamma = \frac{9e^{-10} - e^{-10}}{10e^{-10}} = 0.8
$$

However, since the problem asks for an exact form and doesn't specify that the size must be exactly $\alpha$, the non-randomized test that rejects only when $T = 0$ is sufficient.

### Conclusion

The most powerful test of size $\leq \alpha = 9 \times 10^{-10}$ is:

$$
\text{Reject } H_0 \text{ if } T = 0
$$

where $T = \sum_{i=1}^n Y_i$. This test has size $e^{-10} \approx 4.54 \times 10^{-5}$.

\newpage 

# Q2

Problem 8.19: 

The random variable $X$ has pdf:

$$
f(x) = e^{-x}, \quad x > 0.
$$

One observation is obtained on the random variable:

$$
Y = X^\theta,
$$

and a test of:

$$
H_0 : \theta = 1 \quad \text{versus} \quad H_1 : \theta = 2
$$

needs to be constructed.

Find the UMP level $\alpha = 0.10$ test and compute the Type II Error probability.

## Hint 

Show that the form of the MP test involves rejecting $H_0$ if:

$$
e^{y - \sqrt{y}} / \sqrt{y} > k
$$

for some $k > 1$. 

(Skip the part involving $\alpha = 0.1$ or the Type II error part.)

## Answer 

Under the transformation $Y = X^\theta$, the inverse is $X = Y^{1/\theta}$, and the Jacobian is:

$$
\frac{dx}{dy} = \frac{1}{\theta} y^{(1/\theta) - 1}.
$$

Thus, the pdf of $Y$ is:

$$
f_Y(y|\theta) = f_X(y^{1/\theta}) \cdot \left| \frac{dx}{dy} \right| = e^{-y^{1/\theta}} \cdot \frac{1}{\theta} y^{(1/\theta) - 1}, \quad y > 0.
$$

The Neyman-Pearson lemma states that the most powerful (MP) test rejects $H_0$ for large values of the likelihood ratio:

$$
\Lambda(y) = \frac{f_Y(y|2)}{f_Y(y|1)}.
$$

Substituting the pdfs:

$$
\Lambda(y) = \frac{\frac{1}{2} y^{-1/2} e^{-y^{1/2}}}{e^{-y}} = \frac{1}{2} y^{-1/2} e^{y - \sqrt{y}}.
$$

The rejection region is of the form:

$$
\Lambda(y) > k \implies \frac{e^{y - \sqrt{y}}}{\sqrt{y}} > 2k = k'.
$$

Let $g(y) = \frac{e^{y - \sqrt{y}}}{\sqrt{y}}$. To determine the shape of $g(y)$, compute its logarithmic derivative:

$$
\frac{d}{dy} \ln g(y) = \frac{d}{dy} \left( y - \sqrt{y} - \frac{1}{2} \ln y \right) = 1 - \frac{1}{2\sqrt{y}} - \frac{1}{2y}.
$$

- For $y \to 0^+$: The derivative $\approx -\frac{1}{2y} \to -\infty$ (decreasing).
- For $y \to \infty$: The derivative $\approx 1$ (increasing).
- Critical point: Setting the derivative to zero:

$$
1 - \frac{1}{2\sqrt{y}} - \frac{1}{2y} = 0 \implies y = 1.
$$

At $y = 1$, $g(y)$ has a minimum (verified by second derivative or numerical check).

- $g(y)$ is decreasing for $y < 1$ and increasing for $y > 1$.
- Thus, $\Lambda(y) > k'$ corresponds to:

$$
Y \leq c_0 \quad \text{or} \quad Y \geq c_1,
$$
where $c_0 < 1 < c_1$.

The UMP level-$\alpha$ test rejects $H_0$ if:

$$
Y \leq c_0 \quad \text{or} \quad Y \geq c_1,
$$

where $c_0, c_1$ are chosen such that:

$$
P_{H_0}(Y \leq c_0) + P_{H_0}(Y \geq c_1) = \alpha.
$$

Under $H_0$ ($\theta = 1$), $Y = X \sim \text{Exp}(1)$, so:

$$
P_{H_0}(Y \leq c_0) = 1 - e^{-c_0}, \quad P_{H_0}(Y \geq c_1) = e^{-c_1}.
$$

### Conclusion

The UMP test for $H_0: \theta = 1$ vs $H_1: \theta = 2$ rejects $H_0$ if:

$$
Y \leq c_0 \quad \text{or} \quad Y \geq c_1,
$$

where $c_0, c_1$ satisfy:

$$
(1 - e^{-c_0}) + e^{-c_1} = \alpha.
$$

Note: The exact values of $c_0, c_1$ and the Type II error probability require solving the above equation numerically (not requested here). The key insight is the non-monotonicity of the likelihood ratio, leading to a two-sided rejection region.

\newpage 

# Q3

Problem 8.20, Casella and Berger (2nd Edition).

Let $X$ be a random variable whose pmf under $H_0$ and $H_1$ is given by:

| $x$          | 1    | 2    | 3    | 4    | 5    | 6    | 7    |
|--------------|------|------|------|------|------|------|------|
| $f(x|H_0)$   | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.94 |
| $f(x|H_1)$   | 0.06 | 0.05 | 0.04 | 0.03 | 0.02 | 0.01 | 0.79 |

Use the Neyman–Pearson Lemma to find the most powerful test for $H_0$ versus $H_1$ with size:

$$
\alpha = 0.04.
$$

Compute the probability of Type II Error for this test.

## Hint:

It holds that:

$$
\frac{f(x | H_1)}{f(x | H_0)} = 7 - x + \frac{79}{94} I(x = 7)
$$

over the support $x = 1, 2, \dots, 7$, where $I(\cdot)$ denotes the indicator function.

## Answer

The likelihood ratio (LR) is given by:

$$
\Lambda(x) = \frac{f(x|H_1)}{f(x|H_0)} = 7 - x + \frac{79}{94} I(x = 7),
$$

where $I(\cdot)$ is the indicator function. 

1. For $x = 1, \dots, 6$, the LR simplifies to $\Lambda(x) = 7 - x$.
2. For $x = 7$, $\Lambda(7) = \frac{79}{94} \approx 0.84$.

The LR is decreasing in $x$, so the MP test rejects $H_0$ for the smallest values of $x$ (where the LR is largest).

We order the support points by decreasing LR and compute cumulative probabilities under $H_0$:

| $x$ | LR $\Lambda(x)$ | $f(x|H_0)$ | Cumulative $P_{H_0}$ |
|---------|----------------------|----------------|--------------------------|
| 1       | 6.00                 | 0.01           | 0.01                     |
| 2       | 5.00                 | 0.01           | 0.02                     |
| 3       | 4.00                 | 0.01           | 0.03                     |
| 4       | 3.00                 | 0.01           | 0.04                     |
| 5       | 2.00                 | 0.01           | 0.05                     |
| 6       | 1.00                 | 0.01           | 0.06                     |
| 7       | 0.84                 | 0.94           | 1.00                     |

- To achieve $\alpha = 0.04$, we include the smallest $x$ values until the cumulative probability under $H_0$ reaches $\alpha$.
- The rejection region is:

$$
R = \{1, 2, 3, 4\},
$$

since $P_{H_0}(X \in R) = 0.04$.

The Type II error probability $\beta$ is the probability of not rejecting $H_0$ when $H_1$ is true:

$$
\beta = P_{H_1}(X \notin R) = P_{H_1}(X = 5, 6, 7).
$$

Substituting the pmf under $H_1$:

$$
\beta = f(5|H_1) + f(6|H_1) + f(7|H_1) = 0.02 + 0.01 + 0.79 = 0.82.
$$

### Conclusion

- Most Powerful Test: Reject $H_0$ if $X \in \{1, 2, 3, 4\}$.
- Type II Error Probability: $\beta = 0.82$.
- Size: $P_{H_0}(X \in R) = 0.04$ (exactly $\alpha$).
- Power: $1 - \beta = 0.18$.

\newpage 

# Q4

Recall Method I for finding Uniformly Most Powerful (UMP) tests:

To find a UMP size $\alpha$ test for $H_0 : \theta \in \Theta_0$ vs $H_1 : \theta \notin \Theta_0$, suppose we can fix $\theta_0 \in \Theta_0$ suitably and then use the Neyman–Pearson lemma to find an MP size $\alpha$ test $\varphi(\tilde{X})$ for:

$$
H_0 : \theta = \theta_0 \quad \text{vs} \quad H_1 : \theta = \theta_1,
$$

where:

## a) 

$\varphi(\tilde{X})$ does not depend on $\theta_1 \notin \Theta_0$, and

### Answer 

Condition a) requires that the MP test $\varphi(\tilde{X})$ derived for $H_0: \theta = \theta_0$ vs. $H_1: \theta = \theta_1$ does not depend on the specific alternative $\theta_1 \notin \Theta_0$.

1. Universal Form: The test $\varphi(\tilde{X})$ has the same rejection region for all $\theta_1 \notin \Theta_0$. This typically arises when the likelihood ratio has a monotone structure (e.g., monotone likelihood ratio property).
2. Consistency: The test is not tailored to a single alternative but is valid for the entire alternative space $\theta \notin \Theta_0$.

For example, for exponential families with monotone likelihood ratios, the MP test rejects for large values of a sufficient statistic, regardless of $\theta_1$.

## b) 

$\max_{\theta \in \Theta_0} E_\theta \varphi(\tilde{X}) = \alpha.$

### Answer

Condition b) ensures that the test $\varphi(\tilde{X})$ has size exactly $\alpha$ over the composite null $H_0: \theta \in \Theta_0$:
$$
\max_{\theta \in \Theta_0} E_\theta \varphi(\tilde{X}) = \alpha.
$$

1. Calibration: The test is not conservative; the worst-case Type I error rate is exactly $\alpha$.
2. Sufficiency: The size condition for the simple null ($\theta = \theta_0$) extends to the composite null because $\theta_0$ is chosen to maximize $E_\theta \varphi(\tilde{X})$ over $\Theta_0$.

For many exponential families, the power function is monotone in $\theta$, so the maximum Type I error occurs at the boundary of $\Theta_0$.

## Extra 

Show that if a) and b) both hold, then $\varphi(\tilde{X})$ must be a UMP size $\alpha$ test for $H_0 : \theta \in \Theta_0$ vs $H_1 : \theta \notin \Theta_0$.

## Hint:

From b), the size of the test rule $\varphi(\tilde{X})$ is correct. So, by definition of a UMP test, it is necessary to prove that if $\bar{\varphi}(\tilde{X})$ is any other test of $H_0 : \theta \in \Theta_0$ vs $H_1 : \theta \notin \Theta_0$ with size:

$$
\max_{\theta \in \Theta_0} E_\theta \bar{\varphi}(\tilde{X}) \leq \alpha,
$$

then $\varphi(\tilde{X})$ has more power over the parameter subspace of $H_1$ than $\bar{\varphi}(\tilde{X})$, i.e.,

$$
E_{\theta} \varphi(\tilde{X}) \geq E_{\theta} \bar{\varphi}(\tilde{X}) \quad \text{for any } \theta \notin \Theta_0.
$$

In other words, pick/fix some $\theta_1 \notin \Theta_0$ and argue that:

$$
E_{\theta_1} \varphi(\tilde{X}) \geq E_{\theta_1} \bar{\varphi}(\tilde{X})
$$

must hold. The way to do this is to take the test $\bar{\varphi}(\tilde{X})$ and apply it to testing $H_0 : \theta = \theta_0$ vs. $H_1 : \theta = \theta_1$.

### Answer 

Assume a) and b) hold. We show $\varphi(\tilde{X})$ is UMP for $H_0: \theta \in \Theta_0$ vs. $H_1: \theta \notin \Theta_0$.

Consider testing:

$$
H_0: \theta = \theta_0 \quad \text{vs.} \quad H_1: \theta = \theta_1.
$$

By the NP lemma, $\varphi(\tilde{X})$ is MP at size $\alpha$ for this test.

Let $\bar{\varphi}(\tilde{X})$ be another test with:

$$
\sup_{\theta \in \Theta_0} E_\theta \bar{\varphi}(\tilde{X}) \leq \alpha.
$$

In particular, $E_{\theta_0} \bar{\varphi}(\tilde{X}) \leq \alpha$.

Since $\varphi(\tilde{X})$ is MP for $\theta = \theta_0$ vs. $\theta = \theta_1$, it satisfies:

$$
E_{\theta_1} \varphi(\tilde{X}) \geq E_{\theta_1} \bar{\varphi}(\tilde{X}).
$$

By a), $\varphi(\tilde{X})$ does not depend on $\theta_1$. Thus, the inequality holds for all $\theta_1 \notin \Theta_0$, proving $\varphi(\tilde{X})$ is UMP.

Under conditions a) and b):
1. $\varphi(\tilde{X})$ is UMP for $H_0: \theta \in \Theta_0$ vs. $H_1: \theta \notin \Theta_0$.
2. Size Control: $\max_{\theta \in \Theta_0} E_\theta \varphi(\tilde{X}) = \alpha$.
3. Power Dominance: For all $\theta \notin \Theta_0$, $\varphi(\tilde{X})$ has higher power than any other size-$\alpha$ test.

\newpage 

# Q5

Problem 8.23, Casella and Berger (2nd Edition).

Suppose $X$ is one observation from a population with $\text{Beta}(\theta, 1)$ pdf.

## a) 

For testing:

$$
H_0 : \theta \leq 1 \quad \text{versus} \quad H_1 : \theta > 1,
$$

find the size and sketch the power function of the test that rejects $H_0$ if:

$$
X > \frac{1}{2}.
$$

## b) 

Find the most powerful level-$\alpha$ test of:

$$
H_0 : \theta = 1 \quad \text{versus} \quad H_1 : \theta = 2.
$$

## c) 

Is there a UMP test of:

$$
H_0 : \theta \leq 1 \quad \text{versus} \quad H_1 : \theta > 1?
$$

If so, find it. If not, prove so.