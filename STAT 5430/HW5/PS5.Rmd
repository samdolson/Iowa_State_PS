---
title: "HW5"
output: pdf_document
author: "Sam Olson"
---

# 1. 

In the attached article by Prof. M. Ghosh, read pages 509-512 (including example 1), examples 4-6 of Section 3, and Section 5.2 up to and including Examples 17-18. (This is sort of a technical article, so to read a bit of this material is not easy. Also, Example 17 should look like an example from class regarding Basu’s theorem.)  
   
In example 18, show that $T$ is a complete and sufficient statistic, while $U$ is an ancillary statistic.

## Example 18.

Let $X_1, \dots, X_n$ ($n \geq 2$) be iid with common Weibull pdf

$$
f_{\theta}(x) = \exp(-x^p/\theta) (p/\theta)x^{p-1}; \quad 0 < x < \infty, \quad 0 < \theta < \infty,
$$

$p(>0)$ being known. In this case, $T = \sum_{i=1}^{n} X_i^p$ is complete sufficient for $\theta$, while $U = X_1^p / T$ is ancillary. Also, since $X_1^p, \dots, X_n^p$ are iid exponential with scale parameter $\theta$, $U \sim \text{Beta}(1, n-1)$. Hence, the UMVUE of $P_{\theta}(X_1 \leq x) = P_{\theta}(X_1^p \leq x^p)$ is given by

$$
k(T) =
\begin{cases} 
1 - x^{np}/T^n  & \text{if } T > x^p, \\
1 & \text{if } T \leq x^p.
\end{cases}
$$

## Answer

By definition, a statistic $T$ is sufficient if the joint pdf of $X_1, \dots, X_n$ can be factorized into the form:

$$
f_{\theta}(x_1, \dots, x_n) = g(T, \theta) h(x_1, \dots, x_n)
$$

where:

$g(T, \theta)$ depends on $\theta$,

$h(x_1, \dots, x_n)$ does not depend on $\theta$.

Given $X_1, ..., X_n$ ($n \geq 2$) are iid with Common Weibull pdf, the joint pdf of $X_1, \dots, X_n$ is:

$$
f_{\theta}(x_1, \dots, x_n) = \prod_{i=1}^{n} \left[ \exp(-x_i^p/\theta) \cdot \frac{p}{\theta} x_i^{p-1} \right]
$$

Where: 

$$
0 < x_i < \infty \quad \text{and} \quad 0 < \theta < \infty \quad \forall i
$$

We can simplify this expression, somewhat:

$$
f_{\theta}(x_1, \dots, x_n) = \left(\frac{p}{\theta}\right)^n \exp\left(-\frac{T}{\theta}\right) \prod_{i=1}^{n} x_i^{p-1}
$$

Of note: 

  - The function $g(T, \theta) = \left(\frac{p}{\theta}\right)^n \exp\left(-\frac{T}{\theta}\right)$ depends on $T$ and $\theta$.

  - The function $h(x_1, \dots, x_n) = \prod_{i=1}^{n} x_i^{p-1}$ does not depend on $\theta$.

Thus, by the Factorization Theorem, $T$ is sufficient for $\theta$.

We then need to address completeness: 

By definition, a statistic $T$ is complete if for any function $g(T)$:

$$
E_{\theta}[g(T)] = 0, \quad \forall \theta \quad \Rightarrow \quad P(g(T) = 0) = 1
$$

That is, if the expectation of $g(T)$ is zero for all $\theta$, then $g(T)$ must be the zero function.

Since $X_1^p, \dots, X_n^p$ are iid Exponential($\theta$), following from rescaling the original Weibull-distributed $X_i$'s, we have know the sum:

$$
T = \sum_{i=1}^{n} X_i^p \sim \text{Gamma}(n, \theta)
$$

We then note that the Gamma family is a specific instance of the Exponential family, so we note that the above result holds for "a complete exponential family in $\theta$", which implies that $T$ is a complete statistic for $\theta$. 

Specifically, this result follows because we treat n as known, meaning the only unknown in the above of the parameter $\theta$, making this an instance of "a one-parameter exponential family is complete in $\theta$", though the general description is we cannot find anything with expectation zero for all $\theta$ that is not the zero function itself.

Thus, $T$ is both sufficient and complete for $\theta$.

Finally, we address the ancillary statistic. By definition, a statistic $U$ is ancillary if its distribution does not depend on $\theta$.

We are given a hint to try:

$$
U = \frac{X_1^p}{T}
$$

Since $X_1^p, \dots, X_n^p$ are iid Exponential($\theta$), again following from the initial $X_i$'s being iid Weibull. At any rate, we can again rescale, but with a different linear combination, namely:

$$
\left( \frac{X_1^p}{\theta}, \dots, \frac{X_n^p}{\theta} \right) \sim \text{Exp}(1)
$$

And note the above are still iid. 

Taking the sum, we can express this as another sum of Exponential iid random variables, giving us:

$$
T / \theta \sim \text{Gamma}(n,1)
$$

Since:

$$
U = \frac{X_1^p}{T} = \frac{X_1^p / \theta}{T / \theta}
$$

By multiplying by a "cheeky one".

We then note that $\frac{X_1^p}{\theta} \sim Gamma(1, 1)$ and $T / \theta \sim \text{Gamma}(n,1)$, we know that U is a ratio of two Gamma distributions that are independent is by definition a Beta distribution (the numerator and denominator being independent). 

Specifically, $U$ is distributed: 

$$
U \sim \text{Beta}(1, n-1)
$$

Since the Beta(1, n-1) distribution does not depend on $\theta$, we know the statistic $U$ is ancillary.

### Extra Details

I'm pretty sure the above is "enough" (avoiding using the word sufficient explicitly in a non-maths context), but in the event that some more work would help: 

For finding the distribution of $U$: 

$$
U = \frac{X_1^p}{T} = \frac{X_1^p / \theta}{T / \theta}
$$

$$
f_{X_1^p/\theta}(x) = \frac{x^{1-1} e^{-x}}{\Gamma(1)} = e^{-x}
$$

$$
f_{T/\theta}(t) = \frac{t^{n-1} e^{-t}}{\Gamma(n)}
$$

Giving joint pdf: 

$$
f_{X_1^p/\theta, T/\theta}(x, t) = f_{X_1^p/\theta}(x) f_{T/\theta}(t)
= e^{-x} \cdot \frac{t^{n-1} e^{-t}}{\Gamma(n)}
$$

For:

$$
U = \frac{X_1^p}{T}
$$

Gives:

$$
X_1^p = U T
$$

$$
T = T
$$

With Jacobian:

$$
J = \left| \begin{matrix} \frac{\partial X_1^p}{\partial U} & \frac{\partial X_1^p}{\partial T} \\ \frac{\partial T}{\partial U} & \frac{\partial T}{\partial T} \end{matrix} \right|
= \left| \begin{matrix} T & U \\ 0 & 1 \end{matrix} \right|
= T
$$

Transformation of the prior joint pdf gives:

$$
f_{U, T}(u, t) = f_{X_1^p, T}(ut, t) \cdot |J|
= e^{-ut} \cdot \frac{t^{n-1} e^{-t}}{\Gamma(n)} \cdot T
= \frac{t^n e^{-t}}{\Gamma(n)} e^{-ut}
$$

Getting the marginal distribution of $U$, integrating over T, we have:

$$
f_U(u) 
= \int_0^\infty f_{U, T}(u, t) dt
= \int_0^\infty \frac{t^n e^{-t}}{\Gamma(n)} e^{-ut} dt
= \frac{1}{\Gamma(n)} \int_0^\infty t^n e^{-(1+u)t} dt
= \frac{\Gamma(n+1)}{\Gamma(n)} \cdot \frac{1}{(1+u)^{n+1}}
$$

As $\Gamma(n+1) = n\Gamma(n)$, we can simplify further:

$$
f_U(u) 
= n \frac{1}{(1+u)^{n+1}}
= \frac{u^{1-1} (1 - u)^{(n-1)-1}}{B(1, n-1)}
$$

Where B is the Beta function as is usually defined, and $u^{1-1} = u^0 = 0$ under the constraint 0<u<1 as a "cheeky one". 

This confirms:

$$
U \sim \text{Beta}(1, n-1)
$$

\newpage 

# 2. 

Problem 7.60, Casella and Berger and the following:

## Extra

Let $X_1, \dots, X_n$ be iid gamma$(\alpha, \beta)$ with $\alpha$ known. Find the best unbiased estimator of $1/\beta$.

### Answer

Since $X_i \sim \text{Gamma}(\alpha, \beta)$ are iid, the sum:

$$
S_n = \sum_{i=1}^{n} X_i \sim \text{Gamma}(n\alpha, \beta)
$$

Taking expectation of this statistic: 

$$
E_{\beta}(S_n) = n\alpha \beta
$$

To get $\frac{1}{\beta}$, consider:

$$
E_{\beta} \left[ \frac{n\alpha}{S_n} \right] = \frac{n\alpha}{E_{\beta}(S_n)} = \frac{n\alpha}{n\alpha \beta} = \frac{1}{\beta}
$$

This shows that $\frac{n\alpha}{S_n}$ is an unbiased estimator of $\frac{1}{\beta}$

Noting the work shown previously in [1.], via the Factorization Theorem, we know $S_n$ is a sufficient statistic for $\beta$ (scale parameter of the Gamma).

Similarly, we know that the Gamma family is a specific instance of a a complete one-parameter exponential family, meaning we know that $S_n$ is also complete.

Now, we can do something new! Via Lehmann-Scheffé, since $\delta(S_n) = \frac{n\alpha}{S_n}$ is an unbiased function of the complete sufficient statistic,$S_n$, we know that $\delta(S_n) = \frac{n\alpha}{S_n}$ is the UMVUE of $1/\beta$. Yippee! 

## a) 

Let $S_n = \sum_{i=1}^{n} X_i$. Using Basu’s theorem, show $X_1/S_n$ and $S_n$ are independent.

### Answer

By definition, Basu’s theorem: If $T$ is a complete sufficient statistic and $U$ is an ancillary statistic, then $T$ and $U$ are independent.

From the prior question, we know that $S_n$ is complete and sufficient for $\beta$.

We need to then find an ancillary statistic. 

To that end, let: 

$$
U = \frac{X_1}{S_n}
$$

Where: 

$$
X_1 \sim \text{Gamma}(\alpha, \beta)
$$

$$
S_n \sim \text{Gamma}(n\alpha, \beta)
$$

Using given information, we know U is a ratio of two Gamma random variables. However, this is complicated somewhat by $X_1$ and $S_n$ not being independent! So we need to do a bit of calculation to identify the underlying structure (distribution) of their ratios (though it will be Beta-distributed, the parameter values don't follow the typical formula, i.e. differences between numerator and denominator). To that end, using the known pdfs of each statistic: 

$$
f_{X_1}(x_1) = \frac{x_1^{\alpha - 1} e^{-x_1 / \beta}}{\beta^\alpha \Gamma(\alpha)}
$$

$$
f_{S_n}(s) = \frac{s^{n\alpha - 1} e^{-s / \beta}}{\beta^{n\alpha} \Gamma(n\alpha)}
$$

By the product rule (noting $X_1$ and $S_n$ are not independent):

$$
f_{X_1, S_n}(x_1, s) = f_{X_1 | S_n}(x_1 | s) f_{S_n}(s)
$$

Where: 

$$
f_{X_1 | S_n}(x_1 | s) = \frac{x_1^{\alpha - 1} (s - x_1)^{(n-1)\alpha - 1}}{s^{n\alpha - 1} B(\alpha, (n-1)\alpha)}
$$

Giving joint pdf: 

$$
f_{X_1, S_n}(x_1, s) = \frac{x_1^{\alpha - 1} (s - x_1)^{(n-1)\alpha - 1} e^{-s / \beta} s^{n\alpha - 1}}{\beta^{n\alpha} \Gamma(\alpha) \Gamma((n-1)\alpha)}
$$

For

$$
U = \frac{X_1}{S_n}
$$

We have:

$$
X_1 = U S_n
$$

$$
S = (1 - U) S_n
$$

Caluating the Jacobian determinant:

$$
J = \left| \begin{matrix} \frac{\partial X_1}{\partial U} & \frac{\partial X_1}{\partial S_n} \\ \frac{\partial S}{\partial U} & \frac{\partial S}{\partial S_n} \end{matrix} \right|
=
\left| \begin{matrix} S_n & U \\ -S_n & 1 - U \end{matrix} \right|
= S_n (1 - U) + S_n U = S_n
$$

Via transformation, we have:

$$
f_{U, S_n}(u, s) = f_{X_1, S_n}(us, s) |J|
= \frac{(us)^{\alpha - 1} ((1 - u)s)^{(n-1)\alpha - 1} e^{-s / \beta} s^{n\alpha - 1}}{\beta^{n\alpha} \Gamma(\alpha) \Gamma((n-1)\alpha)} s
= \frac{u^{\alpha - 1} (1 - u)^{(n-1)\alpha - 1} s^{n\alpha - 1} e^{-s / \beta}}{\beta^{n\alpha} \Gamma(\alpha) \Gamma((n-1)\alpha)}
$$

Getting the the marginal distribution of $f_U(u)$, our variable of interest in this problem:

$$
f_U(u) 
= \int_0^\infty f_{U, S_n}(u, s) ds
= \frac{u^{\alpha - 1} (1 - u)^{(n-1)\alpha - 1} \Gamma(n\alpha)}{\Gamma(\alpha) \Gamma((n-1)\alpha)}
$$

Thus, we can identify the distribution and parameters from the above! We know that U is Beta-distributed, specifically: 

$$
U \sim \text{Beta}(\alpha, (n-1)\alpha)
$$

Which does not depend on $\beta$ for any of its parameters! This means we have an ancillary statistic. 

As such, by Basu’s theorem, $U = X_1/S_n$ and $S_n$ are independent.

## b) 

Using the result in a) and $E_\theta(S_n) = n\alpha \beta$, find $E_\theta(X_1/S_n)$.

### Answer

Using the results in a):

$$
E_{\theta} \left( \frac{X_1}{S_n} \right)
=E_{\theta} \left( U \right)
$$

Where:

$$
U \sim \text{Beta}(\alpha, (n-1)\alpha)
$$

Using the properties of a known distribution (distribution of U), we know that: 

$$
E_{\theta} \left( \frac{X_1}{S_n} \right) = \frac{\alpha}{\alpha + (n-1)\alpha} = \frac{1}{n}
$$

\newpage 

# 3. 

Problem 8.13(a)-(c), Casella and Berger (2nd Edition) and, in place of Problem 8.13(d), consider the following test:

Let $X_1, X_2$ be iid uniform$(\theta, \theta + 1)$. For testing $H_0: \theta = 0$ versus $H_1: \theta > 0$, we have two competing tests:

$$
\phi_1(X_1) : \text{Reject } H_0 \text{ if } X_1 > 0.95,
$$

$$
\phi_2(X_1, X_2) : \text{Reject } H_0 \text{ if } X_1 + X_2 > C.
$$

## a) 

Find the value of $C$ so that $\phi_2$ has the same size as $\phi_1$.

### Answer

The size of $\phi_1$ is:

$$
\alpha_1 = P(X_1 > 0.95 \mid \theta = 0) = 0.05
$$

The size of $\phi_2$ is:

$$
\alpha_2 = P(X_1 + X_2 > C \mid \theta = 0)
$$

For $1 \leq C \leq 2$, the probability $P(X_1 + X_2 > C \mid \theta = 0)$ is:

$$
\alpha_2 = \int_{1-C}^{1} \int_{C-x_1}^{1} 1 \, dx_2 \, dx_1 = \frac{(2 - C)^2}{2}
$$

For $\alpha_2 = \alpha_1 = 0.05$, we solve for $C$:

$$
\frac{(2 - C)^2}{2} = 0.05 \implies (2 - C)^2 = 0.1 \implies C = 2 - \sqrt{0.1} \approx 1.68
$$

## b) 

Calculate the power function of each test. Draw a well-labeled graph of each power function.

### Answer

The power function of $\phi_1$ is:

$$
\beta_1(\theta) = P_\theta(X_1 > 0.95) =
\begin{cases} 
0 & \text{if } \theta \leq -0.05, \\ 
\theta + 0.05 & \text{if } -0.05 < \theta \leq 0.95, \\ 
1 & \text{if } \theta > 0.95
\end{cases}
$$

The distribution of $Y = X_1 + X_2$ is:

$$
f_Y(y \mid \theta) =
\begin{cases} 
y - 2\theta & \text{if } 2\theta \leq y < 2\theta + 1, \\ 
2\theta + 2 - y & \text{if } 2\theta + 1 \leq y < 2\theta + 2, \\ 
0 & \text{otherwise}
\end{cases}
$$

The power function of $\phi_2$ is:

$$
\beta_2(\theta) = P_\theta(Y > C) =
\begin{cases} 
0 & \text{if } \theta \leq \frac{C}{2} - 1, \\ 
\frac{(2\theta + 2 - C)^2}{2} & \text{if } \frac{C}{2} - 1 < \theta \leq \frac{C - 1}{2}, \\ 
1 - \frac{(C - 2\theta)^2}{2} & \text{if } \frac{C - 1}{2} < \theta \leq \frac{C}{2}, \\ 
1 & \text{if } \theta > \frac{C}{2}
\end{cases}
$$

For $C \approx 1.68$:

$$
\beta_2(\theta) =
\begin{cases} 
0 & \text{if } \theta \leq -0.16, \\ 
\frac{(2\theta + 0.32)^2}{2} & \text{if } -0.16 < \theta \leq 0.34, \\ 
1 - \frac{(1.68 - 2\theta)^2}{2} & \text{if } 0.34 < \theta \leq 0.84, \\ 
1 & \text{if } \theta > 0.84
\end{cases}
$$

```{r}
theta <- seq(0, 1.2, by = 0.01)
C <- 2 - sqrt(0.1)  

# Power function for phi_1
beta1 <- pmax(0, pmin(1, theta + 0.05))

# Power function for phi_2
beta2 <- ifelse(theta <= (C/2) - 1, 0,
         ifelse(theta <= (C - 1)/2, ((2*theta + 2 - C)^2)/2,
         ifelse(theta <= C/2, 1 - ((C - 2*theta)^2)/2, 1)))

plot(theta, beta1, type = "l", col = "black", lwd = 2, ylim = c(-0.1, 1.1),
     ylab = "Power", xlab = expression(theta), main = "Power Functions of Phi1 and Phi2")
lines(theta, beta2, col = "red", lwd = 2)
legend("bottomright", legend = c(expression(phi[1]), expression(phi[2])),
       col = c("black", "red"), lty = c(1, 2), lwd = 2)
```

## c) 

Prove or disprove: $\phi_2$ is a more powerful test than $\phi_1$.

### Answer

From the graph above, $\phi_1$ is more powerful for $\theta$ near 0, around 0 to 0.2, but $\phi_2$ is more powerful for larger values of $\theta$, particularly around 0.2 to 0.9.

To be a more powerful test, you must be uniformly more powerful than the reference test. We do not meet this condition, meaning $\phi_2$ is not a more powerful test than $\phi_1$.

## Extra 
   
$$
\phi_3(X_1, X_2) = \begin{cases} 
1 & \text{if } X_{(1)} > 1 - \sqrt{0.05} \text{ or } X_{(2)} > 1 \\
0 & \text{otherwise}
\end{cases}
$$
   
where $X_{(1)}, X_{(2)}$ are the min, max.
   
Find the size of this test and the power function for $\theta > 0$. Then, graph the power functions of $\phi_3$ and $\phi_2$ to determine which test is more powerful. (It’s enough to graph over the range $\theta \in [0, 1.2]$.)

### Answer

With the tests as defined, 

where $X_{(1)}$ and $X_{(2)}$ are the minimum and maximum of $X_1, X_2$, respectively. 

Under $H_0: \theta = 0$, $X_1, X_2 \sim \text{Uniform}(0, 1)$, and the order statistics $X_{(1)}$ and $X_{(2)}$ are random variables with distributions: 

$X_{(1)} \sim \text{Beta}(1, 2)$,

$X_{(2)} \sim \text{Beta}(2, 1)$.

Under $H_0: \theta = 0$, the size of $\phi_3$ is:

$$
\alpha_3 = P(X_{(1)} > 1 - \sqrt{0.05} \mid \theta = 0) = (1 - (1 - \sqrt{0.05}))^2 = 0.05
$$

This is because $X_{(1)} > 1 - \sqrt{0.05}$ requires both $X_1$ and $X_2$ to be greater than $1 - \sqrt{0.05}$, and the probability of this event is $(\sqrt{0.05})^2 = 0.05$.

Under $H_1: \theta > 0$, $X_1, X_2 \sim \text{Uniform}(\theta, \theta + 1)$. 

The minimum $X_{(1)}$ follows the CDF:

$$
P(X_{(1)} \leq x) = 1 - (1 - (x - \theta))^2 \quad \text{for } \theta \leq x \leq \theta + 1
$$

Thus, the power function of $\phi_3$ is:

$$
\beta_3(\theta) = P_\theta(X_{(1)} > 1 - \sqrt{0.05}) = (1 - (1 - \sqrt{0.05} - \theta))^2
$$

For $\theta > 1 - \sqrt{0.05}$, $\beta_3(\theta) = 1$ because $X_{(1)} > 1 - \sqrt{0.05}$ is always true.

The power function of $\phi_2$, as determined previously, is:

$$
\beta_2(\theta) =
\begin{cases} 
0 & \text{if } \theta \leq -0.16, \\ 
\frac{(2\theta + 0.32)^2}{2} & \text{if } -0.16 < \theta \leq 0.34, \\ 
1 - \frac{(1.68 - 2\theta)^2}{2} & \text{if } 0.34 < \theta \leq 0.84, \\ 
1 & \text{if } \theta > 0.84.
\end{cases}
$$

From the graph comparing the two tests, $\phi_2$ is more powerful for small values of $\theta$, roughly speaking less than 0.7, and $\phi_3$ is more powerful for larger values of $\theta$, roughly greater than 0.7.

```{r}
# setup
theta <- seq(0, 1.2, by = 0.01)

t_crit <- 1 - sqrt(0.05)

phi3_power <- function(theta) {
    ifelse(theta <= t_crit, (1 - (1 - sqrt(0.05) - theta))^2, 1)
}

beta3 <- sapply(theta, phi3_power)

C <- 2 - sqrt(0.1)  
beta2 <- ifelse(theta <= (C - 1)/2, ((2*theta + 2 - C)^2)/2,
         ifelse(theta <= C/2, 1 - ((C - 2*theta)^2)/2, 1))

plot(theta, beta2, type = "l", col = "black", lwd = 2, ylim = c(-0.1, 1.3),
     ylab = "Power", xlab = expression(theta), main = "Power Functions of Phi2 and Phi3")
lines(theta, beta3, col = "red", lwd = 2)

legend("bottomright", legend = c(expression(phi[2]), expression(phi[3])),
       col = c("black", "red"), lty = c(1, 1), lwd = 2)
```

\newpage

# 4. 

Problem 8.15, Casella and Berger (2nd Edition), though you can just assume the form given is most powerful (no need to show).

Show that for a random sample $X_1, \dots, X_n$ from a $\mathcal{N}(0, \sigma^2)$ population, the most powerful test of $H_0: \sigma = \sigma_0$ versus $H_1: \sigma = \sigma_1$, where $\sigma_0 < \sigma_1$, is given by  

$$
\phi \left( \sum X_i^2 \right) =
\begin{cases} 
1 & \text{if } \sum X_i^2 > c, \\
0 & \text{if } \sum X_i^2 \leq c.
\end{cases}
$$

For a given value of $\alpha$, the size of the Type I Error, show how the value of $c$ is explicitly determined.

## Answer

From the Neyman-Pearson lemma, the most powerful test rejects $H_0$ if the likelihood ratio exceeds a threshold $k$.

The likelihood ratio is given by:

$$
\Lambda = \frac{f(x \mid \sigma_1)}{f(x \mid \sigma_0)} = \left( \frac{\sigma_0}{\sigma_1} \right)^n \exp \left\{ \frac{1}{2} \sum_i x_i^2 \left( \frac{1}{\sigma_0^2} - \frac{1}{\sigma_1^2} \right) \right\} > k
$$

Taking the logarithm (a monotonic function):

$$
\log \Lambda = n \log\left(\frac{\sigma_0}{\sigma_1}\right) + \frac{1}{2} \sum_i x_i^2 \left( \frac{1}{\sigma_0^2} - \frac{1}{\sigma_1^2} \right) > \log(k)
$$

We isolate the term $\sum_i x_i^2$ to one side of the inequality:

$$
\frac{1}{2} \sum_i x_i^2 \left( \frac{1}{\sigma_0^2} - \frac{1}{\sigma_1^2} \right) > \log k - n \log\left(\frac{\sigma_0}{\sigma_1}\right)
$$

$$
\sum_i x_i^2 \left( \frac{1}{\sigma_0^2} - \frac{1}{\sigma_1^2} \right) > 2 \left( \log k - n \log\left(\frac{\sigma_0}{\sigma_1}\right) \right)
$$

Solving for $\sum_i x_i^2$:

$$
\sum_i x_i^2 > \frac{2 \left( \log k - n \log\left(\frac{\sigma_0}{\sigma_1}\right) \right)}{\frac{1}{\sigma_0^2} - \frac{1}{\sigma_1^2}} = c
$$

for some constant, real-valued $c$.

A couple notes: 

  - The above assumes $\sigma_1 > \sigma_0$, so that $\frac{1}{\sigma_0^2} - \frac{1}{\sigma_1^2} > 0$, is this switches, then the overall inequailty flips as well. 

  - Also, the inequality $\sum_i x_i^2 > c$ defines the rejection region for the uniformly most powerful (UMP) test.

That being said, now the critical value $c$ is determined such that the Type I error probability is $\alpha$:

$$
\alpha = P_{\sigma_0} \left( \sum_i X_i^2 > c \right)
$$

Under $H_0$, $\sum_i X_i^2 / \sigma_0^2$ follows a chi-squared distribution with $n$ degrees of freedom (squared standard normal, where we achieve a standard normal variable by scaling by $\sigma_0^2$):

$$
\sum_i X_i^2 / \sigma_0^2 \sim \chi_n^2
$$

Thus, we can rewrite the expression for $\alpha$ as:

$$
\alpha = P_{\sigma_0} \left( \sum_i X_i^2 > c \right) = P_{\sigma_0} \left( \sum_i X_i^2 / \sigma_0^2 > c / \sigma_0^2 \right) = P \left( \chi_n^2 > c / \sigma_0^2 \right)
$$

Solving for $c$:

$$
c = \sigma_0^2 \cdot \chi_{n, 1-\alpha}^2
$$

where $\chi_{n, 1-\alpha}^2$ is the $(1 - \alpha)$-quantile of the $\chi_n^2$ distribution.

The UMP test rejects $H_0$ if:

$$
\sum_i X_i^2 > c = \sigma_0^2 \cdot \chi_{n, 1-\alpha}^2
$$

This defines the rejection region for the most powerful test with Type I error probability $\alpha$.

Note: All the above summations are equivalent to summing from i=1 to n, i.e. 

$$
\sum_i \equiv \sum_{i=1}^{n}
$$