---
title: "HW5"
output: pdf_document
author: "Sam Olson"
---

# Outline 

  - Q1: Edited
  - Q2: Edited
  - Q3: Edited
  - Q4: Edited

# 1. 

In the attached article by Prof. M. Ghosh, read pages 509-512 (including example 1), examples 4-6 of Section 3, and Section 5.2 up to and including Examples 17-18. (This is sort of a technical article, so to read a bit of this material is not easy. Also, Example 17 should look like an example from class regarding Basu’s theorem.)  
   
In example 18, show that $T$ is a complete and sufficient statistic, while $U$ is an ancillary statistic.

## Example 18.

Let $X_1, \dots, X_n$ ($n \geq 2$) be iid with common Weibull pdf

$$
f_{\theta}(x) = \exp(-x^p/\theta) (p/\theta)x^{p-1}; \quad 0 < x < \infty, \quad 0 < \theta < \infty,
$$

$p(>0)$ being known. In this case, $T = \sum_{i=1}^{n} X_i^p$ is complete sufficient for $\theta$, while $U = X_1^p / T$ is ancillary. Also, since $X_1^p, \dots, X_n^p$ are iid exponential with scale parameter $\theta$, $U \sim \text{Beta}(1, n-1)$. Hence, the UMVUE of $P_{\theta}(X_1 \leq x) = P_{\theta}(X_1^p \leq x^p)$ is given by

$$
k(T) =
\begin{cases} 
1 - x^{np}/T^n  & \text{if } T > x^p, \\
1 & \text{if } T \leq x^p.
\end{cases}
$$

## Answer

A statistic $T$ is sufficient for $\theta$ if its conditional distribution given the sample does not depend on $\theta$. By the **Factorization Theorem**, a statistic $T$ is sufficient if the joint pdf can be factorized as:

$$
f_{\theta}(x_1, \dots, x_n) = g(T, \theta) h(x_1, \dots, x_n).
$$

For our case, the joint pdf of $X_1, \dots, X_n$ is:

$$
f_{\theta}(x_1, \dots, x_n) = \prod_{i=1}^{n} \left[ \exp(-x_i^p/\theta) (p/\theta) x_i^{p-1} \right].
$$

Rewriting,

$$
f_{\theta}(x_1, \dots, x_n) = \left( \frac{p}{\theta} \right)^n \exp\left( -\frac{T}{\theta} \right) \prod_{i=1}^{n} x_i^{p-1}.
$$

Here, $g(T, \theta) = \left( \frac{p}{\theta} \right)^n \exp\left( -\frac{T}{\theta} \right)$ depends on the data only through $T$, and $h(x_1, \dots, x_n) = \prod_{i=1}^{n} x_i^{p-1}$ does not depend on $\theta$. By the Factorization Theorem, $T$ is **sufficient** for $\theta$.

A statistic $T$ is complete if for any function $g(T)$,

$$
E_{\theta}[g(T)] = 0 \quad \forall \theta \Rightarrow P(g(T) = 0) = 1.
$$

Since $T = \sum_{i=1}^{n} X_i^p$ follows a **gamma distribution**:

$$
T \sim \text{Gamma}(n, \theta),
$$

and the gamma family is a **complete exponential family**, $T$ is **complete** for $\theta$.

We now turn to the question of $U$ being ancillary. A statistic $U$ is ancillary if its distribution does not depend on $\theta$. Consider

$$
U = \frac{X_1^p}{T}.
$$

Since $X_1^p, \dots, X_n^p$ are **iid** exponential($\theta$), we can write

$$
\left( \frac{X_1^p}{\theta}, \dots, \frac{X_n^p}{\theta} \right) \sim \text{iid Exp}(1).
$$

Thus, $T / \theta \sim \text{Gamma}(n,1)$, and $U = X_1^p/T$ follows a **Beta(1, n-1)** distribution, which **does not depend on** $\theta$. Hence, $U$ is **ancillary**.

\newpage 

# 2. 

Problem 7.60, Casella and Berger and the following:

## Base

Let $X_1, \dots, X_n$ be iid gamma$(\alpha, \beta)$ with $\alpha$ known. Find the best unbiased estimator of $1/\beta$.

### Answer

Let $X_1, \dots, X_n$ be iid gamma($\alpha, \beta$) with $\alpha$ known. We want to find the best unbiased estimator of $1/\beta$.

Since $X_i \sim \text{Gamma}(\alpha, \beta)$, the sum

$$
S_n = \sum_{i=1}^{n} X_i
$$

follows a **Gamma** distribution:

$$
S_n \sim \text{Gamma}(n\alpha, \beta).
$$

The expectation is:

$$
E_{\theta}(S_n) = n\alpha \beta.
$$

A natural unbiased estimator for $1/\beta$ is:

$$
\frac{n\alpha}{S_n},
$$

since:

$$
E_{\theta} \left[ \frac{n\alpha}{S_n} \right] = \frac{n\alpha}{E_{\theta}(S_n)} = \frac{n\alpha}{n\alpha \beta} = \frac{1}{\beta}.
$$

Since $S_n$ is a **complete sufficient statistic** for $\beta$ (by the **Factorization Theorem** and **Lehmann-Scheffé Theorem**), any unbiased estimator that is a function of $S_n$ is **UMVUE**. Thus,

$$
\frac{n\alpha}{S_n}
$$

is the **best unbiased estimator** of $1/\beta$.

## a) 

Let $S_n = \sum_{i=1}^{n} X_i$. Using Basu’s theorem, show $X_1/S_n$ and $S_n$ are independent.

### Answer

Using Basu’s theorem, we show that $X_1/S_n$ and $S_n$ are independent.

Basu’s theorem states that if $T$ is a **complete sufficient statistic** and $U$ is an **ancillary statistic**, then $T$ and $U$ are **independent**.

- We already know that $S_n$ is **complete and sufficient** for $\beta$.
- Consider the ratio:

$$
U = \frac{X_1}{S_n}.
$$

The distribution of $U$ does not depend on $\beta$. Specifically,

$$
U \sim \text{Beta}(\alpha, (n-1)\alpha),
$$

which is **free of $\beta$** and hence **ancillary**.

By **Basu’s theorem**, $U = X_1/S_n$ and $S_n$ are **independent**.

## b) 

Using the result in a) and $E_\theta(S_n) = n\alpha \beta$, find $E_\theta(X_1/S_n)$.

### Answer

Using the results in a), we compute:

$$
E_{\theta} \left( \frac{X_1}{S_n} \right).
$$

Since $X_1/S_n \sim \text{Beta}(\alpha, (n-1)\alpha)$, we use the expectation formula for a Beta distribution:

$$
E \left( \text{Beta}(a, b) \right) = \frac{a}{a+b}.
$$

Thus,

$$
E_{\theta} \left( \frac{X_1}{S_n} \right) = \frac{\alpha}{\alpha + (n-1)\alpha} = \frac{1}{n}.
$$

\newpage 

# 3. 

Problem 8.13(a)-(c), Casella and Berger (2nd Edition) and, in place of Problem 8.13(d), consider the following test:

Let $X_1, X_2$ be iid uniform$(\theta, \theta + 1)$. For testing $H_0: \theta = 0$ versus $H_1: \theta > 0$, we have two competing tests:

$$
\phi_1(X_1) : \text{Reject } H_0 \text{ if } X_1 > 0.95,
$$

$$
\phi_2(X_1, X_2) : \text{Reject } H_0 \text{ if } X_1 + X_2 > C.
$$

## a) 

Find the value of $C$ so that $\phi_2$ has the same size as $\phi_1$.

### Answer

The size of $\phi_1$ is:

$$
\alpha_1 = P(X_1 > 0.95 \mid \theta = 0) = 0.05.
$$

The size of $\phi_2$ is:

$$
\alpha_2 = P(X_1 + X_2 > C \mid \theta = 0).
$$

For $1 \leq C \leq 2$, the probability $P(X_1 + X_2 > C \mid \theta = 0)$ is computed as:

$$
\alpha_2 = \int_{1-C}^{1} \int_{C-x_1}^{1} 1 \, dx_2 \, dx_1 = \frac{(2 - C)^2}{2}.
$$

Setting $\alpha_2 = \alpha_1 = 0.05$ and solving for $C$:

$$
\frac{(2 - C)^2}{2} = 0.05 \implies (2 - C)^2 = 0.1 \implies C = 2 - \sqrt{0.1} \approx 1.68.
$$

Thus, the value of $C$ such that $\phi_2$ has the same size as $\phi_1$ is:

$$
C = 2 - \sqrt{0.1} \approx 1.68.
$$

## b) 

Calculate the power function of each test. Draw a well-labeled graph of each power function.

### Answer

#### Power Function of $\phi_1$

The power function of $\phi_1$ is:

$$
\beta_1(\theta) = P_\theta(X_1 > 0.95) =
\begin{cases} 
0 & \text{if } \theta \leq -0.05, \\ 
\theta + 0.05 & \text{if } -0.05 < \theta \leq 0.95, \\ 
1 & \text{if } \theta > 0.95.
\end{cases}
$$

#### Power Function of $\phi_2$

The distribution of $Y = X_1 + X_2$ is:

$$
f_Y(y \mid \theta) =
\begin{cases} 
y - 2\theta & \text{if } 2\theta \leq y < 2\theta + 1, \\ 
2\theta + 2 - y & \text{if } 2\theta + 1 \leq y < 2\theta + 2, \\ 
0 & \text{otherwise}.
\end{cases}
$$

The power function of $\phi_2$ is:

$$
\beta_2(\theta) = P_\theta(Y > C) =
\begin{cases} 
0 & \text{if } \theta \leq \frac{C}{2} - 1, \\ 
\frac{(2\theta + 2 - C)^2}{2} & \text{if } \frac{C}{2} - 1 < \theta \leq \frac{C - 1}{2}, \\ 
1 - \frac{(C - 2\theta)^2}{2} & \text{if } \frac{C - 1}{2} < \theta \leq \frac{C}{2}, \\ 
1 & \text{if } \theta > \frac{C}{2}.
\end{cases}
$$

```{r}
theta <- seq(0, 1.2, by = 0.01)
C <- 2 - sqrt(0.1)  # Computed value of C

# Power function for phi_1
beta1 <- pmax(0, pmin(1, theta + 0.05))

# Power function for phi_2
beta2 <- ifelse(theta <= (C/2) - 1, 0,
         ifelse(theta <= (C - 1)/2, ((2*theta + 2 - C)^2)/2,
         ifelse(theta <= C/2, 1 - ((C - 2*theta)^2)/2, 1)))

# Plot
plot(theta, beta1, type = "l", col = "blue", lwd = 2, ylim = c(0, 1),
     ylab = "Power", xlab = expression(theta), main = "Power Functions of Phi1 and Phi2")
lines(theta, beta2, col = "red", lwd = 2, lty = 2)
legend("bottomright", legend = c(expression(phi[1]), expression(phi[2])),
       col = c("blue", "red"), lty = c(1, 2), lwd = 2)
```

## c) 

Prove or disprove: $\phi_2$ is a more powerful test than $\phi_1$.

### Answer

From the graph in Part (b), we observe that:

- $\phi_1$ is more powerful for $\theta$ near 0.
- $\phi_2$ is more powerful for larger values of $\theta$.

Thus, $\phi_2$ is **not uniformly more powerful** than $\phi_1$.

## Extra 
   
$$
\phi_3(X_1, X_2) = \begin{cases} 
1 & \text{if } X_{(1)} > 1 - \sqrt{0.05} \text{ or } X_{(2)} > 1 \\
0 & \text{otherwise}
\end{cases}
$$
   
where $X_{(1)}, X_{(2)}$ are the min, max.
   
Find the size of this test and the power function for $\theta > 0$. Then, graph the power functions of $\phi_3$ and $\phi_2$ to determine which test is more powerful. (It’s enough to graph over the range $\theta \in [0, 1.2]$.)

### Answer

Define the test:

$$
\phi_3(X_1, X_2) = 
\begin{cases} 
1 & \text{if } X_{(1)} > 1 - \sqrt{0.05} \text{ or } X_{(2)} > 1, \\ 
0 & \text{otherwise},
\end{cases}
$$

where $X_{(1)}, X_{(2)}$ are the minimum and maximum of $X_1, X_2$, respectively.

#### Size of $\phi_3$

Under $H_0: \theta = 0$, the size of $\phi_3$ is:

$$
\alpha_3 = P(X_{(1)} > 1 - \sqrt{0.05} \mid \theta = 0) = (1 - (1 - \sqrt{0.05}))^2 = 0.05.
$$

#### Power Function of $\phi_3$

The power function of $\phi_3$ is:

$$
\beta_3(\theta) = P_\theta(X_{(1)} > 1 - \sqrt{0.05}) = (1 - (1 - \sqrt{0.05} - \theta))^2.
$$

#### Plot 

Via the below: 

- $\phi_3$ is more powerful than $\phi_2$ for all $\theta > 0$.

Thus, $\phi_3$ is **uniformly more powerful** than $\phi_2$.

```{r}
# Define theta range
theta <- seq(0, 1.2, by = 0.01)
C <- 2 - sqrt(0.1)  # Computed value of C for phi_2

# Power function for phi_2
beta2 <- ifelse(theta <= (C/2) - 1, 0,
         ifelse(theta <= (C - 1)/2, ((2*theta + 2 - C)^2)/2,
         ifelse(theta <= C/2, 1 - ((C - 2*theta)^2)/2, 1)))

# Power function for phi_3
phi3_power <- function(theta) {
  pmin(1, (1 - pmax(0, 1 - sqrt(0.05) - theta))^2)
}

beta3 <- sapply(theta, phi3_power)

# Plot
plot(theta, beta2, type = "l", col = "red", lwd = 2, ylim = c(0, 1),
     ylab = "Power", xlab = expression(theta), main = "Power Functions of Phi2 and Phi3")
lines(theta, beta3, col = "green", lwd = 2, lty = 3)
legend("bottomright", legend = c(expression(phi[2]), expression(phi[3])),
       col = c("red", "green"), lty = c(2, 3), lwd = 2)
```

\newpage

# 4. 

Problem 8.15, Casella and Berger (2nd Edition), though you can just assume the form given is most powerful (no need to show).

Show that for a random sample $X_1, \dots, X_n$ from a $\mathcal{N}(0, \sigma^2)$ population, the most powerful test of $H_0: \sigma = \sigma_0$ versus $H_1: \sigma = \sigma_1$, where $\sigma_0 < \sigma_1$, is given by  

$$
\phi \left( \sum X_i^2 \right) =
\begin{cases} 
1 & \text{if } \sum X_i^2 > c, \\
0 & \text{if } \sum X_i^2 \leq c.
\end{cases}
$$

For a given value of $\alpha$, the size of the Type I Error, show how the value of $c$ is explicitly determined.

## Answer

From the **Neyman-Pearson lemma**, the most powerful (UMP) test rejects $H_0$ if the likelihood ratio exceeds a threshold $k$. The likelihood ratio is:

$$
\frac{f(x \mid \sigma_1)}{f(x \mid \sigma_0)}
= \frac{(2\pi\sigma_1^2)^{-n/2} e^{-\sum_i x_i^2/(2\sigma_1^2)}}{(2\pi\sigma_0^2)^{-n/2} e^{-\sum_i x_i^2/(2\sigma_0^2)}}
= \left( \frac{\sigma_0}{\sigma_1} \right)^n \exp \left\{ \frac{1}{2} \sum_i x_i^2 \left( \frac{1}{\sigma_0^2} - \frac{1}{\sigma_1^2} \right) \right\} > k.
$$

After some algebra, this inequality simplifies to:

$$
\sum_i x_i^2 > \frac{2\log \left( k \, (\sigma_1/\sigma_0)^n \right)}{\left( \frac{1}{\sigma_0^2} - \frac{1}{\sigma_1^2} \right)} = c,
$$

where $c$ is a constant. This is because $\frac{1}{\sigma_0^2} - \frac{1}{\sigma_1^2} > 0$ (since $\sigma_0 < \sigma_1$).

Thus, the UMP test rejects $H_0$ if:

$$
\sum_i X_i^2 > c.
$$

The critical value $c$ is determined such that the Type I error probability is $\alpha$, i.e.,

$$
\alpha = P_{\sigma_0} \left( \sum_i X_i^2 > c \right).
$$

Under $H_0$, $\sum_i X_i^2 / \sigma_0^2$ follows a chi-squared distribution with $n$ degrees of freedom:

$$
\sum_i X_i^2 / \sigma_0^2 \sim \chi_n^2.
$$

Thus, the probability can be rewritten as:

$$
\alpha = P_{\sigma_0} \left( \sum_i X_i^2 / \sigma_0^2 > c/\sigma_0^2 \right) = P \left( \chi_n^2 > c/\sigma_0^2 \right).
$$

To find $c$, we solve for the $(1 - \alpha)$-quantile of the $\chi_n^2$ distribution:

$$
c = \sigma_0^2 \cdot \chi_{n, 1-\alpha}^2,
$$

where $\chi_{n, 1-\alpha}^2$ is the $(1 - \alpha)$-quantile of the $\chi_n^2$ distribution.