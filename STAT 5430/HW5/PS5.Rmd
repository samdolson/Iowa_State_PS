---
title: "HW5"
output: pdf_document
author: "Sam Olson"
---

# Outline 

  - Q1: G2G
  - Q2: G2G
  - Q3: Iffy
  - Q4: Iffy

# 1. 

In the attached article by Prof. M. Ghosh, read pages 509-512 (including example 1), examples 4-6 of Section 3, and Section 5.2 up to and including Examples 17-18. (This is sort of a technical article, so to read a bit of this material is not easy. Also, Example 17 should look like an example from class regarding Basu’s theorem.)  
   
In example 18, show that $T$ is a complete and sufficient statistic, while $U$ is an ancillary statistic.

## Example 18.

Let $X_1, \dots, X_n$ ($n \geq 2$) be iid with common Weibull pdf

$$
f_{\theta}(x) = \exp(-x^p/\theta) (p/\theta)x^{p-1}; \quad 0 < x < \infty, \quad 0 < \theta < \infty,
$$

$p(>0)$ being known. In this case, $T = \sum_{i=1}^{n} X_i^p$ is complete sufficient for $\theta$, while $U = X_1^p / T$ is ancillary. Also, since $X_1^p, \dots, X_n^p$ are iid exponential with scale parameter $\theta$, $U \sim \text{Beta}(1, n-1)$. Hence, the UMVUE of $P_{\theta}(X_1 \leq x) = P_{\theta}(X_1^p \leq x^p)$ is given by

$$
k(T) =
\begin{cases} 
1 - x^{np}/T^n  & \text{if } T > x^p, \\
1 & \text{if } T \leq x^p.
\end{cases}
$$

## Answer

By the Factorization Theorem, a statistic $T$ is sufficient if the joint pdf can be factorized as:

$$
f_{\theta}(x_1, \dots, x_n) = g(T, \theta) h(x_1, \dots, x_n)
$$

Or, two functions, one that is dependent upon $\theta$ and one that does not. 

As given, the joint pdf of $X_1, \dots, X_n$ is:

$$
f_{\theta}(x_1, \dots, x_n)
= \prod_{i=1}^{n} \left[ \exp(-x_i^p/\theta) (p/\theta) x_i^{p-1} \right]
= \left( \frac{p}{\theta} \right)^n \exp\left( -\frac{T}{\theta} \right) \prod_{i=1}^{n} x_i^{p-1}
$$

We note then that for $g(T, \theta) = \left( \frac{p}{\theta} \right)^n \exp\left( -\frac{T}{\theta} \right)$ depends on the data only through $T$, and $h(x_1, \dots, x_n) = \prod_{i=1}^{n} x_i^{p-1}$ does not depend on $\theta$. So we meet the conditions to note the Factorization Theorem, such that $T$ is sufficient for $\theta$.

Onto completeness: 

A statistic $T$ is complete if for any function $g(T)$,

$$
E_{\theta}[g(T)] = 0 \quad \forall \theta \Rightarrow P(g(T) = 0) = 1
$$

Or, the zero function is the only function to satisfy the above expression. 

We have $T = \sum_{i=1}^{n} X_i^p$, which we know follows a gamma distribution given the iid distribution of $X_i$ (sum of iid Exponential with common $\theta$ is a Gamma distribution), i.e.:

$$
T \sim \text{Gamma}(n, \theta)
$$

We then take advantage of knowing that the Gamma family is a complete exponential family, meaning $T$ is complete for $\theta$.

One last item to tackle then, $U$ being ancillary. 

A statistic $U$ is ancillary if its distribution does not depend on $\theta$. 

Let:

$$
U = \frac{X_1^p}{T}
$$

Since $X_1^p, \dots, X_n^p$ are iid Exponential($\theta$), we can write

$$
\left( \frac{X_1^p}{\theta}, \dots, \frac{X_n^p}{\theta} \right) \sim \text{iid Exp}(1)
$$

I believe this is called "pivoting", and apologies if this was not the anticipated method for this proof. That notwithstanding, $T / \theta \sim \text{Gamma}(n,1)$, and $U = X_1^p/T$ follows a Beta(1, n-1) distribution. Of note, this (the distribution of U) does not depend on $\theta$, meaning $U$ is ancillary.

\newpage 

# 2. 

Problem 7.60, Casella and Berger and the following:

## Extra

Let $X_1, \dots, X_n$ be iid gamma$(\alpha, \beta)$ with $\alpha$ known. Find the best unbiased estimator of $1/\beta$.

### Answer

Since $X_i \sim \text{Gamma}(\alpha, \beta)$, the sum:

$$
S_n = \sum_{i=1}^{n} X_i
$$

follows a Gamma distribution, with note of Problem 1:

$$
S_n \sim \text{Gamma}(n\alpha, \beta)
$$

Taking expectation:

$$
E_{\theta}(S_n) = n\alpha \beta
$$

Since:

$$
E_{\theta} \left[ \frac{n\alpha}{S_n} \right] = \frac{n\alpha}{E_{\theta}(S_n)} = \frac{n\alpha}{n\alpha \beta} = \frac{1}{\beta}
$$

One possible unbiased estimator for $1/\beta$ is:

$$
\frac{n\alpha}{S_n}
$$

Since $S_n$ is a complete sufficient statistic for $\beta$ (by the Factorization Theorem and Lehmann-Scheffé Theorem), any unbiased estimator that is a function of $S_n$ is UMVUE, making it the best unbiased estimator for the purpose of this problem. 

## a) 

Let $S_n = \sum_{i=1}^{n} X_i$. Using Basu’s theorem, show $X_1/S_n$ and $S_n$ are independent.

### Answer

Using Basu’s theorem, we show that $X_1/S_n$ and $S_n$ are independent.

Basu’s theorem: If $T$ is a complete sufficient statistic and $U$ is an ancillary statistic, then $T$ and $U$ are independent.

From the prior question, we know that $S_n$ is complete and sufficient for $\beta$.

We need to then find an ancillary statistic. To that end, let: 

$$
U = \frac{X_1}{S_n}
$$

Using given information, we know: 

$$
U \sim \text{Beta}(\alpha, (n-1)\alpha)
$$

Which does not depend on $\beta$ for any of its parameters! This means we have found U, our ancillary statistic. 

Then, by Basu’s theorem, $U = X_1/S_n$ and $S_n$ are independent.

## b) 

Using the result in a) and $E_\theta(S_n) = n\alpha \beta$, find $E_\theta(X_1/S_n)$.

### Answer

Using the results in a):

$$
E_{\theta} \left( \frac{X_1}{S_n} \right)
=E_{\theta} \left( U \right)
$$

Where:

$$
U \sim \text{Beta}(\alpha, (n-1)\alpha)
$$

Using the properties of a known distribution, we know that: 

$$
E_{\theta} \left( \frac{X_1}{S_n} \right) = \frac{\alpha}{\alpha + (n-1)\alpha} = \frac{1}{n}.
$$

\newpage 

# 3. 

Problem 8.13(a)-(c), Casella and Berger (2nd Edition) and, in place of Problem 8.13(d), consider the following test:

Let $X_1, X_2$ be iid uniform$(\theta, \theta + 1)$. For testing $H_0: \theta = 0$ versus $H_1: \theta > 0$, we have two competing tests:

$$
\phi_1(X_1) : \text{Reject } H_0 \text{ if } X_1 > 0.95,
$$

$$
\phi_2(X_1, X_2) : \text{Reject } H_0 \text{ if } X_1 + X_2 > C.
$$

## a) 

Find the value of $C$ so that $\phi_2$ has the same size as $\phi_1$.

### Answer

The size of $\phi_1$ is:

$$
\alpha_1 = P(X_1 > 0.95 \mid \theta = 0) = 0.05
$$

The size of $\phi_2$ is:

$$
\alpha_2 = P(X_1 + X_2 > C \mid \theta = 0)
$$

For $1 \leq C \leq 2$, the probability $P(X_1 + X_2 > C \mid \theta = 0)$ is:

$$
\alpha_2 = \int_{1-C}^{1} \int_{C-x_1}^{1} 1 \, dx_2 \, dx_1 = \frac{(2 - C)^2}{2}
$$

For $\alpha_2 = \alpha_1 = 0.05$ and solving for $C$:

$$
\frac{(2 - C)^2}{2} = 0.05 \implies (2 - C)^2 = 0.1 \implies C = 2 - \sqrt{0.1} \approx 1.68
$$

## b) 

Calculate the power function of each test. Draw a well-labeled graph of each power function.

### Answer

The power function of $\phi_1$ is:

$$
\beta_1(\theta) = P_\theta(X_1 > 0.95) =
\begin{cases} 
0 & \text{if } \theta \leq -0.05, \\ 
\theta + 0.05 & \text{if } -0.05 < \theta \leq 0.95, \\ 
1 & \text{if } \theta > 0.95.
\end{cases}
$$

The distribution of $Y = X_1 + X_2$ is:

$$
f_Y(y \mid \theta) =
\begin{cases} 
y - 2\theta & \text{if } 2\theta \leq y < 2\theta + 1, \\ 
2\theta + 2 - y & \text{if } 2\theta + 1 \leq y < 2\theta + 2, \\ 
0 & \text{otherwise}.
\end{cases}
$$

The power function of $\phi_2$ is:

$$
\beta_2(\theta) = P_\theta(Y > C) =
\begin{cases} 
0 & \text{if } \theta \leq \frac{C}{2} - 1, \\ 
\frac{(2\theta + 2 - C)^2}{2} & \text{if } \frac{C}{2} - 1 < \theta \leq \frac{C - 1}{2}, \\ 
1 - \frac{(C - 2\theta)^2}{2} & \text{if } \frac{C - 1}{2} < \theta \leq \frac{C}{2}, \\ 
1 & \text{if } \theta > \frac{C}{2}.
\end{cases}
$$

```{r}
theta <- seq(0, 1.2, by = 0.01)
C <- 2 - sqrt(0.1)  # Computed value of C

# Power function for phi_1
beta1 <- pmax(0, pmin(1, theta + 0.05))

# Power function for phi_2
beta2 <- ifelse(theta <= (C/2) - 1, 0,
         ifelse(theta <= (C - 1)/2, ((2*theta + 2 - C)^2)/2,
         ifelse(theta <= C/2, 1 - ((C - 2*theta)^2)/2, 1)))

# Plot
plot(theta, beta1, type = "l", col = "blue", lwd = 2, ylim = c(0, 1),
     ylab = "Power", xlab = expression(theta), main = "Power Functions of Phi1 and Phi2")
lines(theta, beta2, col = "red", lwd = 2, lty = 2)
legend("bottomright", legend = c(expression(phi[1]), expression(phi[2])),
       col = c("blue", "red"), lty = c(1, 2), lwd = 2)
```

## c) 

Prove or disprove: $\phi_2$ is a more powerful test than $\phi_1$.

### Answer

From the graph above, $\phi_1$ is more powerful for $\theta$ near 0, but $\phi_2$ is more powerful for larger values of $\theta$.

To be a more powerful test, you must be uniformly more powerful than the reference test. We do not meet this condition, meaning $\phi_2$ is not a more powerful test than $\phi_1$.

## Extra 
   
$$
\phi_3(X_1, X_2) = \begin{cases} 
1 & \text{if } X_{(1)} > 1 - \sqrt{0.05} \text{ or } X_{(2)} > 1 \\
0 & \text{otherwise}
\end{cases}
$$
   
where $X_{(1)}, X_{(2)}$ are the min, max.
   
Find the size of this test and the power function for $\theta > 0$. Then, graph the power functions of $\phi_3$ and $\phi_2$ to determine which test is more powerful. (It’s enough to graph over the range $\theta \in [0, 1.2]$.)

### Answer

Define the test:

$$
\phi_3(X_1, X_2) = 
\begin{cases} 
1 & \text{if } X_{(1)} > 1 - \sqrt{0.05} \text{ or } X_{(2)} > 1, \\ 
0 & \text{otherwise},
\end{cases}
$$

where $X_{(1)}, X_{(2)}$ are the minimum and maximum of $X_1, X_2$, respectively.

Under $H_0: \theta = 0$, the size of $\phi_3$ is:

$$
\alpha_3 = P(X_{(1)} > 1 - \sqrt{0.05} \mid \theta = 0) = (1 - (1 - \sqrt{0.05}))^2 = 0.05.
$$

The power function of $\phi_3$ is:

$$
\beta_3(\theta) = P_\theta(X_{(1)} > 1 - \sqrt{0.05}) = (1 - (1 - \sqrt{0.05} - \theta))^2.
$$

Via the below: $\phi_3$ is more powerful than $\phi_2$ for all $\theta > 0$. This means for the range being considered that $\phi_3$ is uniformly more powerful than $\phi_2$.

```{r}
# Define theta range
theta <- seq(0, 1.2, by = 0.01)
C <- 2 - sqrt(0.1)  # Computed value of C for phi_2

# Power function for phi_2
beta2 <- ifelse(theta <= (C/2) - 1, 0,
         ifelse(theta <= (C - 1)/2, ((2*theta + 2 - C)^2)/2,
         ifelse(theta <= C/2, 1 - ((C - 2*theta)^2)/2, 1)))

# Power function for phi_3
phi3_power <- function(theta) {
  pmin(1, (1 - pmax(0, 1 - sqrt(0.05) - theta))^2)
}

beta3 <- sapply(theta, phi3_power)

# Plot
plot(theta, beta2, type = "l", col = "red", lwd = 2, ylim = c(0, 1),
     ylab = "Power", xlab = expression(theta), main = "Power Functions of Phi2 and Phi3")
lines(theta, beta3, col = "green", lwd = 2, lty = 3)
legend("bottomright", legend = c(expression(phi[2]), expression(phi[3])),
       col = c("red", "green"), lty = c(2, 3), lwd = 2)
```

\newpage

# 4. 

Problem 8.15, Casella and Berger (2nd Edition), though you can just assume the form given is most powerful (no need to show).

Show that for a random sample $X_1, \dots, X_n$ from a $\mathcal{N}(0, \sigma^2)$ population, the most powerful test of $H_0: \sigma = \sigma_0$ versus $H_1: \sigma = \sigma_1$, where $\sigma_0 < \sigma_1$, is given by  

$$
\phi \left( \sum X_i^2 \right) =
\begin{cases} 
1 & \text{if } \sum X_i^2 > c, \\
0 & \text{if } \sum X_i^2 \leq c.
\end{cases}
$$

For a given value of $\alpha$, the size of the Type I Error, show how the value of $c$ is explicitly determined.

## Answer

From the Neyman-Pearson lemma, the most powerful (UMP) test rejects $H_0$ if the likelihood ratio exceeds a threshold $k$. 

The likelihood ratio is:

$$
\frac{f(x \mid \sigma_1)}{f(x \mid \sigma_0)}
= \frac{(2\pi\sigma_1^2)^{-n/2} e^{-\sum_i x_i^2/(2\sigma_1^2)}}{(2\pi\sigma_0^2)^{-n/2} e^{-\sum_i x_i^2/(2\sigma_0^2)}}
= \left( \frac{\sigma_0}{\sigma_1} \right)^n \exp \left\{ \frac{1}{2} \sum_i x_i^2 \left( \frac{1}{\sigma_0^2} - \frac{1}{\sigma_1^2} \right) \right\} > k
$$

This simplifies to:

$$
\sum_i x_i^2 > \frac{2\log \left( k \, (\sigma_1/\sigma_0)^n \right)}{\left( \frac{1}{\sigma_0^2} - \frac{1}{\sigma_1^2} \right)} = c
$$

where $c$ is a constant. 

Noting that: $\frac{1}{\sigma_0^2} - \frac{1}{\sigma_1^2} > 0$ (since $\sigma_0 < \sigma_1$).

Thus, the UMP test rejects $H_0$ if:

$$
\sum_i X_i^2 > c
$$

The critical value $c$ is determined such that the Type I error probability is $\alpha$:

$$
\alpha = P_{\sigma_0} \left( \sum_i X_i^2 > c \right)
$$

Under $H_0$, $\sum_i X_i^2 / \sigma_0^2$ follows a chi-squared distribution with $n$ degrees of freedom:

$$
\sum_i X_i^2 / \sigma_0^2 \sim \chi_n^2
$$

Thus, the probability can be rewritten as:

$$
\alpha = P_{\sigma_0} \left( \sum_i X_i^2 / \sigma_0^2 > c/\sigma_0^2 \right) = P \left( \chi_n^2 > c/\sigma_0^2 \right)
$$

We then solve for $c$, taking the $(1 - \alpha)$-quantile of the $\chi_n^2$ distribution:

$$
c = \sigma_0^2 \cdot \chi_{n, 1-\alpha}^2
$$

where $\chi_{n, 1-\alpha}^2$ is the $(1 - \alpha)$-quantile of the $\chi_n^2$ distribution.