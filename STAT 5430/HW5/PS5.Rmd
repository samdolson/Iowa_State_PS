---
title: "HW5"
output: pdf_document
author: "Sam Olson"
---

# 1. 

In the attached article by Prof. M. Ghosh, read pages 509-512 (including example 1), examples 4-6 of Section 3, and Section 5.2 up to and including Examples 17-18. (This is sort of a technical article, so to read a bit of this material is not easy. Also, Example 17 should look like an example from class regarding Basu’s theorem.)  
   
In example 18, show that $T$ is a complete and sufficient statistic, while $U$ is an ancillary statistic.

## Example 18.

Let $X_1, \dots, X_n$ ($n \geq 2$) be iid with common Weibull pdf

$$
f_{\theta}(x) = \exp(-x^p/\theta) (p/\theta)x^{p-1}; \quad 0 < x < \infty, \quad 0 < \theta < \infty,
$$

$p(>0)$ being known. In this case, $T = \sum_{i=1}^{n} X_i^p$ is complete sufficient for $\theta$, while $U = X_1^p / T$ is ancillary. Also, since $X_1^p, \dots, X_n^p$ are iid exponential with scale parameter $\theta$, $U \sim \text{Beta}(1, n-1)$. Hence, the UMVUE of $P_{\theta}(X_1 \leq x) = P_{\theta}(X_1^p \leq x^p)$ is given by

$$
k(T) =
\begin{cases} 
1 - x^{np}/T^n  & \text{if } T > x^p, \\
1 & \text{if } T \leq x^p.
\end{cases}
$$

\newpage 

# 2. 

Problem 7.60, Casella and Berger and the following:

## Base

Let $X_1, \dots, X_n$ be iid gamma$(\alpha, \beta)$ with $\alpha$ known. Find the best unbiased estimator of $1/\beta$.
   
## a) 

Let $S_n = \sum_{i=1}^{n} X_i$. Using Basu’s theorem, show $X_1/S_n$ and $S_n$ are independent.
   
## b) 

Using the result in (a) and $E_\theta(S_n) = n\alpha \beta$, find $E_\theta(X_1/S_n)$.

\newpage 

# 3. 

Problem 8.13(a)-(c), Casella and Berger (2nd Edition) and, in place of Problem 8.13(d), consider the following test:

Let $X_1, X_2$ be iid uniform$(\theta, \theta + 1)$. For testing $H_0: \theta = 0$ versus $H_1: \theta > 0$, we have two competing tests:

$$
\phi_1(X_1) : \text{Reject } H_0 \text{ if } X_1 > 0.95,
$$

$$
\phi_2(X_1, X_2) : \text{Reject } H_0 \text{ if } X_1 + X_2 > C.
$$

## a) 

Find the value of $C$ so that $\phi_2$ has the same size as $\phi_1$.

## b) 

Calculate the power function of each test. Draw a well-labeled graph of each power function.

## c) 

Prove or disprove: $\phi_2$ is a more powerful test than $\phi_1$.

## Extra 
   
$$
\phi_3(X_1, X_2) = \begin{cases} 
1 & \text{if } X_{(1)} > 1 - \sqrt{0.05} \text{ or } X_{(2)} > 1 \\
0 & \text{otherwise}
\end{cases}
$$
   
where $X_{(1)}, X_{(2)}$ are the min, max.
   
Find the size of this test and the power function for $\theta > 0$. Then, graph the power functions of $\phi_3$ and $\phi_2$ to determine which test is more powerful. (It’s enough to graph over the range $\theta \in [0, 1.2]$.)

\newpage

# 4. 

Problem 8.15, Casella and Berger (2nd Edition), though you can just assume the form given is most powerful (no need to show).

Show that for a random sample $X_1, \dots, X_n$ from a $\mathcal{N}(0, \sigma^2)$ population, the most powerful test of $H_0: \sigma = \sigma_0$ versus $H_1: \sigma = \sigma_1$, where $\sigma_0 < \sigma_1$, is given by  

$$
\phi \left( \sum X_i^2 \right) =
\begin{cases} 
1 & \text{if } \sum X_i^2 > c, \\
0 & \text{if } \sum X_i^2 \leq c.
\end{cases}
$$

For a given value of $\alpha$, the size of the Type I Error, show how the value of $c$ is explicitly determined.