---
title: "HW4"
output: pdf_document
author: "Sam Olson"
---

# Outline 

  - Q1: Editing
  - Q2: Editing
  - Q3: Editing
  - Q4: Editing
  - Q5: Editing

# Problem 1

**Problem 6.2, Casella and Berger (2nd Edition)**

**6.2** Let $X_1, \dots, X_n$ be independent random variables with densities  

$$
f_{X_i}(x | \theta) =
\begin{cases} 
e^{\theta - x} & x \geq i\theta \\
0 & x < i\theta.
\end{cases}
$$

Prove that $T = \min_i (X_i / i)$ is a sufficient statistic for $\theta$.  

## Answer 

(1): Factorization Theorem, a statistic $T(X)$ is sufficient for $\theta$ if the joint pdf can be expressed in the form:

$$
f(x_1, \dots, x_n | \theta) = g(T(X), \theta) h(x_1, \dots, x_n),
$$

where $g(T(X), \theta)$ is a function depending on $\theta$ and the data only through $T(X)$, and $h(x_1, \dots, x_n)$ is a function that does not depend on $\theta$.

We are given that $X_1, \dots, X_n$ are iid. 

$$
f_{X_i}(x | \theta) =
\begin{cases} 
e^{\theta - x} & x \geq i\theta, \\
0 & x < i\theta.
\end{cases}
$$

As given the joint pdf of $X_1, \dots, X_n$ is:

$$
f(x_1, \dots, x_n | \theta) = \prod_{i=1}^{n} f_{X_i}(x_i | \theta) = \prod_{i=1}^{n} e^{\theta - x_i} \cdot \mathbb{I}_{[i\theta, +\infty)}(x_i)
$$

Substituting the given densities:

$$
f(x_1, \dots, x_n | \theta) = \prod_{i=1}^{n} e^{\theta - x_i} \cdot \mathbb{I}_{[i\theta, +\infty)}(x_i).
$$

Rewriting the first product,

$$
\prod_{i=1}^{n} e^{\theta - x_i} = e^{n\theta - \sum_{i=1}^{n} x_i}.
$$

For the second product, we observe:

$$
\prod_{i=1}^{n} \mathbb{I}_{[i\theta, +\infty)}(x_i) = \mathbb{I}_{[\theta, +\infty)}\left( \min_i (x_i / i) \right).
$$

This follows because the conditions $x_i \geq i\theta$ for all $i$ are equivalent to requiring $\min_i (x_i / i) \geq \theta$.

Thus, the joint pdf becomes:

$$
f(x_1, \dots, x_n | \theta) = e^{n\theta} \cdot \mathbb{I}_{[\theta, +\infty)}\left( \min_i (x_i / i) \right) \cdot e^{-\sum_{i=1}^{n} x_i}.
$$

Defining $T(X) = \min_i (X_i / i)$, we can express this as:

$$
f(x_1, \dots, x_n | \theta) = \underbrace{e^{n\theta} \cdot \mathbb{I}_{[\theta, +\infty)}(T(X))}_{g(T(X), \theta)} \cdot \underbrace{e^{-\sum_{i=1}^{n} x_i}}_{h(x_1, \dots, x_n)}.
$$

Since the factor $g(T(X), \theta)$ depends on $\theta$ only through $T(X)$, and $h(x_1, \dots, x_n)$ is independent of $\theta$, it follows that $T(X) = \min_i (X_i / i)$ is a sufficient statistic for $\theta$. 

\newpage 

# Problem 2

**Example of Rao-Blackwell theorem, which is largely a STAT 5420 problem in computation.**

Let $X_1$ and $X_2$ be iid Bernoulli$(p)$, $0 < p < 1$.

## a) 

Show $S = X_1 + X_2$ is Sufficient for p

### Answer 

By the Factorization Theorem, a statistic $S$ is sufficient for $p$ if the joint pmf can be written in the form:

$$
f(x_1, x_2 | p) = g(S, p) \cdot h(x_1, x_2).
$$

The joint pmf of $X_1, X_2$, given they are iid Bernoulli$(p)$, is:

$$
f(x_1, x_2 | p) = p^{x_1} (1 - p)^{1 - x_1} \cdot p^{x_2} (1 - p)^{1 - x_2}.
$$

Rewriting:

$$
f(x_1, x_2 | p) = p^{x_1 + x_2} (1 - p)^{2 - (x_1 + x_2)}.
$$

Letting $S = X_1 + X_2$, we obtain:

$$
f(x_1, x_2 | p) = p^S (1 - p)^{2 - S}.
$$

Since this is of the form $g(S, p) \cdot h(x_1, x_2)$ with $h(x_1, x_2) = 1$, it follows that $S$ is sufficient for $p$ by the Factorization Theorem.

## b) 

Identify the conditional probability $P(X_1 = x | S = s)$; you should know which values of $x, s$ to consider.

### Answer 

We compute:

$$
P(X_1 = x | S = s) = \frac{P(X_1 = x, S = s)}{P(S = s)}.
$$

For possible values of $S$:

(0): If $S = 0$, then $X_1 = 0$ and $X_2 = 0$, so:

$$
P(X_1 = 0 | S = 0) = 1.
$$

(1): If $S = 2$, then $X_1 = 1$ and $X_2 = 1$, so:

$$
P(X_1 = 1 | S = 2) = 1.
$$

(2): If $S = 1$, then either:

$X_1 = 0, X_2 = 1$, or$X_1 = 1, X_2 = 0$.

Since both occur with equal probability,

$$
P(X_1 = 1 | S = 1) = P(X_1 = 0 | S = 1) = \frac{1}{2}.
$$

## c) 

Find the conditional expectation $T \equiv E(X_1 | S)$, i.e., as a function of the possibilities of $S$. Note that $T$ is a statistic.

### Answer 

Using the values from part (b):

$$
T = E(X_1 | S) =
\begin{cases}
0, & S = 0, \\
\frac{1}{2}, & S = 1, \\
1, & S = 2.
\end{cases}
$$

$T$ is a statistic, noted.

## d) 

Show $X_1$ and $T$ are both unbiased for $p$.

### Answer

For $X_1$:

$$
E_p(X_1) = p.
$$

For $T$, using linearity of expectation:

$$
E_p(T) = \sum_{s=0}^{2} E(X_1 | S = s) P(S = s).
$$

Substituting:

$$
E_p(T) = 0 \cdot (1 - p)^2 + \frac{1}{2} \cdot 2p(1 - p) + 1 \cdot p^2.
$$

$$
= p(1 - p) + p^2 = p.
$$

Thus, both $X_1$ and $T$ are unbiased for $p$.

## e) 

Show $\text{Var}_p(T) \leq \text{Var}_p(X_1)$, for any $p$.

### Answer

By the Rao-Blackwell theorem:

$$
\text{Var}_p(T) \leq \text{Var}_p(X_1).
$$

We verify this explicitly.

Since $X_1 \sim \text{Bernoulli}(p)$, we have:

$$
\text{Var}_p(X_1) = p(1 - p).
$$

For $T$:

$$
\text{Var}_p(T) = E_p(T^2) - (E_p(T))^2.
$$

Computing $E_p(T^2)$:

$$
E_p(T^2) = 0^2 \cdot (1 - p)^2 + \left(\frac{1}{2}\right)^2 \cdot 2p(1 - p) + 1^2 \cdot p^2.
$$

$$
= \frac{p(1 - p)}{2} + p^2.
$$

Thus,

$$
\text{Var}_p(T) = \left(\frac{p(1 - p)}{2} + p^2\right) - p^2.
$$

$$
= \frac{p(1 - p)}{2}.
$$

Since

$$
\frac{p(1 - p)}{2} \leq p(1 - p),
$$

it follows that:

$$
\text{Var}_p(T) \leq \text{Var}_p(X_1),
$$

as required.

\newpage

# Problem 3

**Problem 6.21 a)-b), Casella and Berger (2nd Edition)**

**6.21** Let $X$ be one observation from the pdf  

$$
f(x | \theta) = \left(\frac{\theta}{2}\right)^{|x|} (1 - \theta)^{1 - |x|}, \quad x = -1, 0, 1, \quad 0 \leq \theta \leq 1.
$$

## a) 

Is $X$ a complete sufficient statistic?

### Answer 

Since $X$ is the only observation, it is trivially sufficient for $\theta$. To determine whether $X$ is complete, we check whether the only function $g(X)$ satisfying $E[g(X)] = 0$ for all $\theta$ is the zero function.

We compute:

$$
E[g(X)] = \sum_{x \in \{-1,0,1\}} g(x) f(x | \theta).
$$

Using the given density,

$$
E[g(X)] = g(-1) P(X = -1) + g(0) P(X = 0) + g(1) P(X = 1).
$$

Substituting probabilities,

$$
E[g(X)] = \frac{\theta}{2} g(-1) + (1 - \theta) g(0) + \frac{\theta}{2} g(1).
$$

Since this must be zero for all $\theta \in [0,1]$, we rewrite:

$$
\theta \left(\frac{g(-1) + g(1)}{2} - g(0) \right) + g(0) = 0.
$$

For this equation to hold for all $\theta$, both coefficients must be zero:

$$
\frac{g(-1) + g(1)}{2} - g(0) = 0, \quad g(0) = 0.
$$

From $g(0) = 0$, the first equation simplifies to:

$$
\frac{g(-1) + g(1)}{2} = 0 \quad \Rightarrow \quad g(-1) + g(1) = 0.
$$

This allows nontrivial solutions, such as $g(-1) = 1, g(1) = -1, g(0) = 0$, showing that $X$ is not complete.

## b) 

Is $|X|$ a complete sufficient statistic?

### Answer

First, we check sufficiency using the Factorization Theorem. The pdf is:

$$
f(x | \theta) = \left(\frac{\theta}{2}\right)^{|x|} (1 - \theta)^{1 - |x|}.
$$

This depends on $X$ only through $|X|$, so the conditional distribution of $X$ given $|X|$ does not depend on $\theta$. Thus, $|X|$ is sufficient.

Next, we check completeness. The distribution of $|X|$ follows a Bernoulli distribution:

$$
P(|X| = 0) = 1 - \theta, \quad P(|X| = 1) = \theta.
$$

It is a standard result (Casella & Berger, Example 6.2.11) that the Bernoulli family is complete. Thus, $|X|$ is complete sufficient.

\newpage 

# Problem 4

**Problem 6.24, Casella and Berger (2nd Edition)**

**6.24** Consider the following family of distributions:  

$$
\mathcal{P} = \{ P_{\lambda}(X = x) : P_{\lambda}(X = x) = \frac{\lambda^x e^{-\lambda}}{x!}; x = 0,1,2,\dots ; \lambda = 0 \text{ or } 1 \}.
$$

This is a Poisson family with $\lambda$ restricted to be $0$ or $1$. Show that the family $\mathcal{P}$ is not complete, demonstrating that completeness can be dependent on the range of the parameter. (See Exercises 6.15 and 6.18.)  

## Answer

To show that $\mathcal{P}$ is not complete, we must find a nonzero function $h(X)$ such that:

$$
E_{\lambda}[h(X)] = 0, \quad \text{for all } \lambda \in \{0,1\}.
$$

A family of distributions is complete if the only function satisfying this expectation condition is the zero function.

For $\lambda = 0$, the Poisson distribution degenerates to:

$$
P_{\lambda=0}(X = x) = 
\begin{cases} 
1 & \text{if } x = 0, \\
0 & \text{if } x \neq 0.
\end{cases}
$$

Thus, expectation simplifies to:

$$
E_{\lambda=0}[h(X)] = h(0).
$$

For $E_{\lambda=0}[h(X)] = 0$, we must have:

$$
h(0) = 0.
$$

For $\lambda = 1$, $X \sim \text{Poisson}(1)$, so:

$$
E_{\lambda=1}[h(X)] = \sum_{x=0}^{\infty} h(x) \cdot \frac{1^x e^{-1}}{x!} = e^{-1} \sum_{x=0}^{\infty} \frac{h(x)}{x!}.
$$

Since we already established $h(0) = 0$, this simplifies to:

$$
\sum_{x=1}^{\infty} \frac{h(x)}{x!} = 0.
$$

We seek a function $h(X)$ that satisfies:

$$
\sum_{x=1}^{\infty} \frac{h(x)}{x!} = 0, \quad h(0) = 0.
$$

A simple choice is:

$$
h(0) = 0, \quad h(1) = 1, \quad h(2) = -2, \quad h(x) = 0 \text{ for } x \geq 3.
$$

Computing the sum:

$$
\sum_{x=1}^{\infty} \frac{h(x)}{x!} = \frac{h(1)}{1!} + \frac{h(2)}{2!} + \sum_{x=3}^{\infty} \frac{h(x)}{x!}.
$$

$$
= \frac{1}{1} + \frac{-2}{2} + 0 = 1 - 1 = 0.
$$

Thus, $E_{\lambda}[h(X)] = 0$ for both $\lambda = 0$ and $\lambda = 1$, yet $h(X)$ is not identically zero. This demonstrates that the family $\mathcal{P}$ is not complete.

This result highlights that completeness depends on the range of the parameter. The Poisson family is typically complete when $\lambda > 0$, but when restricted to just two values ($\lambda = 0$ or $\lambda = 1$), the condition $E[h(X)] = 0$ no longer forces $h(X)$ to be identically zero.

Thus, the restriction on $\lambda$ reduces the richness of the family and leads to incompleteness.

\newpage 

# Problem 5

**Problem 7.57, Casella and Berger (2nd Edition)** You may assume $n \geq 3$.

One has to Rao-Blackwellize on the complete/sufficient statistic here

$$\sum_{i=1}^{n+1} X_i.$$

**7.57** Let $X_1, \dots, X_{n+1}$ be iid Bernoulli$(p)$, and define the function $h(p)$ by  

$$
h(p) = P \left( \sum_{i=1}^{n} X_i > X_{n+1} \middle| p \right),
$$

the probability that the first $n$ observations exceed the $(n+1)$st.  

## a) 

Show that

$$
T(X_1, \dots, X_{n+1}) =  
\begin{cases} 
1 & \text{if } \sum_{i=1}^{n} X_i > X_{n+1}, \\
0 & \text{otherwise},
\end{cases}
$$

is an unbiased estimator of $h(p)$.

### Answer 

We define:

$$
T(X_1, \dots, X_{n+1}) =  
\begin{cases} 
1, & \text{if } \sum_{i=1}^{n} X_i > X_{n+1}, \\
0, & \text{otherwise}.
\end{cases}
$$

Since $T$ is an indicator function of the event $\sum_{i=1}^{n} X_i > X_{n+1}$, we compute its expectation:

$$
E_p[T] = P_p \left( \sum_{i=1}^{n} X_i > X_{n+1} \right) = h(p).
$$

Thus, $T(X)$ is an unbiased estimator of $h(p)$.

## b) 

Find the best unbiased estimator of $h(p)$.

### Answer 

Since $\sum_{i=1}^{n+1} X_i$ is a complete sufficient statistic for $p$, the Rao-Blackwell theorem states that the best unbiased estimator of $h(p)$ is:

$$
E \left[ T \mid \sum_{i=1}^{n+1} X_i = y \right].
$$

$$
E \left[ T \mid \sum_{i=1}^{n+1} X_i = y \right] = P \left( \sum_{i=1}^{n} X_i > X_{n+1} \Bigg| \sum_{i=1}^{n+1} X_i = y \right).
$$

Since $X_{n+1}$ is binary, we analyze two cases:

(0): $X_{n+1} = 0$* 

$\sum_{i=1}^{n} X_i = y - X_{n+1} = y - 0 = y$, which means the event $\sum_{i=1}^{n} X_i > X_{n+1}$ always holds when $y \geq 1$.

$$
P \left( \sum_{i=1}^{n} X_i > X_{n+1} \mid \sum_{i=1}^{n+1} X_i = y, X_{n+1} = 0 \right) = 1.
$$

(1): $X_{n+1} = 1$ 

Here, $\sum_{i=1}^{n} X_i = y - 1$, so the event $\sum_{i=1}^{n} X_i > X_{n+1}$ holds if $y - 1 > 1$, i.e., $y \geq 2$.

$$
P \left( \sum_{i=1}^{n} X_i > X_{n+1} \mid \sum_{i=1}^{n+1} X_i = y, X_{n+1} = 1 \right) = \mathbb{I}_{y \geq 2}.
$$

Since $X_{n+1} \sim \text{Bernoulli}(p)$, we compute the total probability using law of total probability:

$$
P \left( \sum_{i=1}^{n} X_i > X_{n+1} \Bigg| \sum_{i=1}^{n+1} X_i = y \right)
$$

$$
= P \left( \sum_{i=1}^{n} X_i > X_{n+1} \mid X_{n+1} = 0 \right) P(X_{n+1} = 0 \mid \sum X_i = y)
$$

$$
+ P \left( \sum_{i=1}^{n} X_i > X_{n+1} \mid X_{n+1} = 1 \right) P(X_{n+1} = 1 \mid \sum X_i = y).
$$

Using:

$$
P(X_{n+1} = 1 \mid \sum_{i=1}^{n+1} X_i = y) = \frac{y}{n+1}, \quad P(X_{n+1} = 0 \mid \sum_{i=1}^{n+1} X_i = y) = \frac{n+1 - y}{n+1},
$$

we compute:

$$
P \left( \sum_{i=1}^{n} X_i > X_{n+1} \mid \sum_{i=1}^{n+1} X_i = y \right) =
\left( 1 \cdot \frac{n+1 - y}{n+1} \right) + \left( \mathbb{I}_{y \geq 2} \cdot \frac{y}{n+1} \right).
$$

Simplifying,

$$
E \left[ T \mid \sum_{i=1}^{n+1} X_i = y \right] =
\begin{cases}
0, & y = 0, \\
\frac{n+1 - y}{n+1}, & y = 1, \\
1, & y \geq 2.
\end{cases}
$$

$$
E \left[ T \mid \sum_{i=1}^{n+1} X_i = y \right] =
\begin{cases}
0, & y = 0, \\
\frac{n+1 - y}{n+1}, & y = 1, \\
1, & y \geq 2.
\end{cases}
$$

Thus, the best unbiased estimator of $h(p)$ is:

$$
\delta(X) =
\begin{cases}
0, & y = 0, \\
\frac{n+1 - y}{n+1}, & y = 1, \\
1, & y \geq 2.
\end{cases}
$$
