---
title: "HW3"
output: pdf_document
author: "Sam Olson"
---

# 1. 

Suppose $X_1, \dots, X_n$ are iid Bernoulli$(p)$, $0 < p < 1$.

## a) 

Find the information number $I_n(p)$ and make a rough sketch of $I_n(p)$ as a function of $p \in (0,1)$.

## b) 

Find the value of $p \in (0,1)$ for which $I_n(p)$ is minimal. (This value of $p$ corresponds to the "hardest" case for estimating $p$. That is, when data are generated under this value of $p$ from the model, the variance of an UE of $p$ is potentially largest.)

## c) 

Show that $\hat{X}_n = \sum_{i=1}^{n} X_i /n$ is the UMVUE of $p$.

\newpage 

# 2.

Suppose that the random variables $Y_1, \dots, Y_n$ satisfy

$$ Y_i = \beta x_i + \varepsilon_i, \quad i = 1, \dots, n $$

where $x_1, \dots, x_n$ are fixed constants and $\varepsilon_1, \dots, \varepsilon_n$ are iid $N(0,\sigma^2)$; here we assume $\sigma^2 > 0$ is known.

## a) 

Find the MLE of $\beta$.

## b) 

Find the distribution of the MLE.

## c) 

Find the CRLB for estimating $\beta$. (Hint: you'll have to work with the joint distribution $f(y_1, \dots, y_n | \beta)$ directly, since $Y_1, \dots, Y_n$ are not iid.)

## d) 

Show the MLE is the UMVUE of $\beta$.

\newpage

# 3. 

Suppose $X_1, \dots, X_n$ are iid normal $N(0,1)$, where $\theta \in \mathbb{R}$. It turns out that $T = (\bar{X}_n)^2 - n^{-1}$ is the UMVUE of $\gamma(\theta) = \theta^2$. (We can show this later in the course; our goal here is to show that the UMVUE can exist without obtaining the CRLB.)

## a) 

Show $T$ is an UE of $\gamma(\theta) = \theta^2$ and find the variance $\text{Var}_\theta(T)$ of $T$. (Note $Z = \sqrt{n}(\bar{X}_n - \theta) \sim N(0,1)$ and one can write $T = (Z^2/n) + (2\theta Z/\sqrt{n}) + \theta^2 - n^{-1}$, where $Z^2 \sim \chi_1^2$, $E_\theta Z^2 = 1$, $\text{Var}_\theta(Z^2) = 2$.)

## b) 

Find the CRLB for an UE of $\gamma(\theta) = \theta^2$.

## c) 

Show that $\text{Var}_\theta(T) > \text{CRLB}$ for all values of $\theta \in \mathbb{R}$.

\newpage

# 4. 

Casella & Berger 7.58

("better" here refers to MSE as a criterion.)

Let $X$ be an observation from the pdf

$$ f(x|\theta) = \left(\frac{\theta}{2}\right)^{|x|} (1 - \theta)^{1 - |x|}, \quad x = -1,0,1; \quad 0 \leq \theta \leq 1. $$

## a) 

Find the MLE of $\theta$.

## b) 

Define the estimator $T(X)$ by

$$ T(X) = \begin{cases} 
2 & \text{if } x = 1 \\ 
0 & \text{otherwise.} 
\end{cases} $$

Show that $T(X)$ is an unbiased estimator of $\theta$.

## c) 

Find a better estimator than $T(X)$ and prove that it is better.

\newpage

# 5. 

Let $X_1, \dots, X_n$ be iid Bernoulli$(\theta)$, $\theta \in (0,1)$. Find the Bayes estimator of $\theta$ with respect to the uniform$(0,1)$ prior under the loss function

$$L(t, \theta) = \frac{(t - \theta)^2}{\theta(1 - \theta)}.$$


