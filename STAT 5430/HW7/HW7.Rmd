---
title: "HW7"
output: pdf_document
author: "Sam Olson"
---

# Q1 

Problem 8.6 a) - b), Casella and Berger (2nd Edition)

Suppose that we have two independent random samples: $X_1, \ldots, X_n$ are exponential$(\theta)$, and $Y_1, \ldots, Y_m$ are exponential$(\mu)$.

## a) 

Find the LRT of

$$
H_0: \theta = \mu \quad \text{versus} \quad H_1: \theta \ne \mu.
$$

### Answer

The LRT statistic is of the form:  

$$
\lambda(x, y) = \frac{\text{max}_{\theta} L(\theta \mid \underset{\sim}{x}, \underset{\sim}{y})}{\text{max}_{\theta, \mu} L(\theta, \mu \mid \underset{\sim}{x}, \underset{\sim}{y})}
$$

Where, under $H_0$ ($\theta = \mu$).

Generally, we know that, the MLE will be some weighted average of the observations, taking advantage of the one parameter exponential families known to be complete and their MLEs of a general form. 

Under $H_0$ (to get the numerator of the LRT) the MLE for $\theta$ is of the form:  

$$
\hat{\theta}_{H_0} = \frac{\sum_{i=1}^n X_i + \sum_{j=1}^m Y_j}{n + m}
$$

And, under the full model (the denominator of the LRT), the MLEs are the individual sample means, i.e.:  

$$
\hat{\theta}_{MLE} = \bar{X} = \frac{\sum X_i}{n}, \quad \hat{\mu}_{MLE} = \bar{Y} = \frac{\sum Y_j}{m}
$$

Returning to the original expression, we then have:  

$$
\lambda(x, y) = \frac{(\hat{\theta}_0)^{-(n+m)} e^{-(n+m)}}{(\hat{\theta}_{MLE})^{-n} e^{-n} (\hat{\mu}_{MLE})^{-m} e^{-m}} = \frac{(\bar{X})^n (\bar{Y})^m}{\left( \frac{\sum X_i + \sum Y_j}{n + m} \right)^{n+m}}
= \frac{(n+m)^{n+m} (\sum X_i)^n (\sum Y_j)^m}{n^n m^m (\sum X_i + \sum Y_j)^{n+m}}
$$

We may then construct our test function, where our rejection rule is to "Reject $H_0$ if $\lambda(x, y) \leq c$", where $c$ is calibrated based on the significance level $\alpha$, i.e. our test function is of the form:

$$
\varphi(x, y) = 
\begin{cases} 
1 & \text{if } \lambda(x, y) \leq c, \\
0 & \text{otherwise}
\end{cases}
$$

Where (to save space above):

$$
\lambda(x, y) = \frac{(n+m)^{n+m} (\sum X_i)^n (\sum Y_j)^m}{n^n m^m (\sum X_i + \sum Y_j)^{n+m}}
$$

And $c$ is chosen such that $P(\varphi(X, Y) = 1 \mid H_0) = \alpha$. 

## b) 

Show that the test in part a) can be based on the statistic

$$
T = \frac{\sum X_i}{\sum X_i + \sum Y_i}
$$

### Answer

Let $T = \frac{\sum X_i}{\sum X_i + \sum Y_j}$.  

Rewriting the LRT from part a) in terms of $T$:  

$$
\lambda(x, y) = \frac{(n+m)^{n+m}}{n^n m^m} \left( \frac{\sum X_i}{\sum X_i + \sum Y_j} \right)^n \left( \frac{\sum Y_j}{\sum X_i + \sum Y_j} \right)^m = \frac{(n+m)^{n+m}}{n^n m^m} T^n (1-T)^m
$$

Since $\lambda(x, y)$ depends on the data only through $T$, the LRT can be based entirely on $T$.

Using the above, we may define the rejection region where the test rejects $H_0$ when $T$ is "too small" or "too large" with constants a and b, where:  

$$
T \leq a \quad \text{or} \quad T \geq b
$$

And where $a$ and $b$ are values satisfying:  

$$
P(T \leq a \mid H_0) + P(T \geq b \mid H_0) = \alpha
$$

Under $H_0$ ($\theta = \mu$), $\sum X_i \sim \text{Gamma}(n, \theta)$, $\sum Y_j \sim \text{Gamma}(m, \theta)$. 

The above is taken as known because that the sum of iid Exponentials is Gamma, and a linear combination, specifically a ratio, of Gamma distributions with common rate parameter $\theta$ is a Beta. 

Also, since both X and Y are independent of one another, their sums are also independent, and determining the parameters of the T Beta distribution becomes a matter of algebra (and the distribution of T does not involve $\theta$ in its parameters).

Specifically, we know:  

$$
T = \frac{\sum X_i}{\sum X_i + \sum Y_j} \sim \text{Beta}(n, m)
$$

So the critical values being referenced above may be found via taking critical regions of the Beta distribution when n and m are known values (numbers of observations of X and Y respectively). 

\newpage

# Q2 

Problem 8.28, Casella and Berger (2nd Edition)

Let $f(x|\theta)$ be the logistic location probability density function:

$$
f(x|\theta) = \frac{e^{(x - \theta)}}{(1 + e^{(x - \theta)})^2}, \quad -\infty < x < \infty, \quad -\infty < \theta < \infty.
$$

## a) 

Show that this family has an MLR. 

### Answer

Let $\theta_2 > \theta_1$. 

We know the likelihood ratio statistic is given by:

$$
\Lambda = \frac{f(x|\theta_2)}{f(x|\theta_1)} = e^{\theta_1 - \theta_2} \left[ \frac{1 + e^{x - \theta_1}}{1 + e^{x - \theta_2}} \right]^2
$$

The derivative wrt X is of the form: 

$$
\Lambda' = \frac{e^{x - \theta_1}(1 + e^{x - \theta_2}) - e^{x - \theta_2}(1 + e^{x - \theta_1})}{(1 + e^{x - \theta_2})^2} = \frac{e^{x - \theta_1} - e^{x - \theta_2}}{(1 + e^{x - \theta_2})^2} > 0
$$

And the inequality holds because of the assumption $\theta_2 > \theta_1$, which is allowed in the full parameter space. 

Thus, our likelihood ratio is strictly increasing in $x$, meaning it is monotonic, i.e. that the family $f(x|\theta)$ from the logistic location probability density function has MLR in $x$.

## b) 

Based on one observation $X$, find the most powerful size $\alpha$ test of

$$
H_0: \theta = 0 \quad \text{versus} \quad H_1: \theta = 1.
$$

For $\alpha = 0.2$, find the size of the Type II error.

### Answer 

By the Neyman-Pearson Lemma, the MP test rejects $H_0$ when:

$$
\Lambda = \frac{f(x|1)}{f(x|0)} = e^{-1}\left(\frac{1 + e^x}{1 + e^{x - 1}}\right)^2 > k
$$

From from part a), since the likelihood ratio is increasing in $x$, the MP test rejects if $X > k_1$, where $k_1$ is determined by the size $\alpha$.

As we know the underlying distributions, let us consider the CDF of the logistic distribution:

$$
F(x|\theta) = \frac{e^{x - \theta}}{1 + e^{x - \theta}}
$$

Under $H_0$, the size is given by the expression:

$$
P(X > k_1 \mid \theta = 0) = 1 - F(k_1|0) = \frac{1}{1 + e^{k_1}} = \alpha
$$

Solving for $k_1$:

$$
k_1 = \log\left( \frac{1 - \alpha}{\alpha} \right) = \log(\alpha^{-1} - 1)
$$

For $\alpha = 0.2$: 

$$
k_1 = \log(0.2^{-1} - 1) = \log(4) \approx 1.386
$$

Under $H_1$, to calculate the Type II Error Rate:

$$
\beta = P(X \leq k_1 \mid \theta = 1) = F(k_1|1) = \frac{e^{k_1 - 1}}{1 + e^{k_1 - 1}} \approx \frac{e^{0.386}}{1 + e^{0.386}} \approx 0.595
$$

So, the MP level test of size $\alpha = 0.2$ rejects when our single observation $X > 1.386$, with a Type II error rate of 0.595.

## c) 

Show that the test in part b) is UMP size $\alpha$ for testing

$$
H_0: \theta \leq 0 \quad \text{versus} \quad H_1: \theta > 0.
$$

What can be said about UMP tests in general for the logistic location family?

### Answer 

Via MLR: From part a), the family has MLR in $X$.

Via Karlin-Rubin Thm. (Knew it would come up again!): Since the MP test for $\theta = 0$ vs $\theta = 1$ rejects for large $X$ and does not depend on the specific parameter value, i.e., $\theta_1 = ...$ (alternative hypothesis parameter value in particular), the rejection region depends solely upon the observed value X, meaning the MP test is also the UMP test for $H_0: \theta \leq 0$ vs $H_1: \theta > 0$.

The above results extend to similar distributions within the the logistic location family, i.e., UMP tests for one-sided hypotheses both exist and take the form "Reject $H_0$ if $X > c$." I do not believe it would necessarily extend to rate parameter family of distributions however, as that tends to be a bit more complicated. 

\newpage

# Q3 

Problem 8.29 a) - b), Casella and Berger (2nd Edition)

Let $X$ be one observation from a Cauchy$(\theta)$ distribution.

The Cauchy$(\theta)$ density is given by:

$$
f(x|\theta) = \frac{1}{\pi} \cdot \frac{1}{1 + (x - \theta)^2}, \quad x \in \mathbb{R}, -\infty < \theta < \infty.
$$

## a)

Show that this family does not have an MLR.

### Hint: 

Show that the Cauchy$(\theta)$ family $\{ f(x|\theta) : \theta \in \mathbb{R} = \Theta \}$, based on one observation $X$, does not have monotone likelihood ratio (MLR) in $t(X) = X$ or $t(X) = -X$. That is, the ratio

$$
\frac{f(x|\theta_2)}{f(x|\theta_1)}
$$

might not be monotone (either increasing or decreasing) in $x$.

### Answer 

Let $\theta_2 > \theta_1$ under the setup of the problem. 

The likelihood ratio is of the form:

$$
\Lambda = \frac{f(x|\theta_2)}{f(x|\theta_1)} = \frac{1 + (x - \theta_1)^2}{1 + (x - \theta_2)^2}
$$

And it has limit(s):  

$$
\lim_{x \to \pm \infty} \frac{f(x|\theta_2)}{f(x|\theta_1)} = 1
$$

As we seek to disprove that the ratio is not monotonic, we need only one example that displays non-monotonicity. 

For example, let $\theta_1 = 0$, $\theta_2 = 1$ such that our base assumption that $\theta_2 > \theta_1$ holds. 

Then:

$$
\Lambda = \frac{f(x|1)}{f(x|0)} = \frac{1 + x^2}{1 + (x - 1)^2}
$$

```{r}
cauchy <- function(x) {
  numerator <- 1 + x^2 
  denominator <- 1 + (x-1)^2
  numerator/denominator
}
```

```{r, eval = F}
cauchy(0)
cauchy(1) 
cauchy(2)
cauchy(3)
cauchy(1000)
```

```{r}
x_vals <- seq(0, 100, length.out = 500)
y_vals <- cauchy(x_vals)

plot(x_vals, y_vals, type = "l", lwd = 2, col = "black",
     main = "Cauchy Ratio - Increasing Values of X",
     xlab = "x", ylab = expression(Lambda))
abline(h = 1, col = "red", lty = 2)
```

```{r}
x_vals <- seq(0, -100, length.out = 500)
y_vals <- cauchy(x_vals)

plot(x_vals, y_vals, type = "l", lwd = 2, col = "black",
     main = "Cauchy Ratio - Decreasing Values of X",
     xlab = "x", ylab = expression(Lambda))
abline(h = 1, col = "red", lty = 2)
```

At $x = 0$, $\Lambda = 0.5$. At $x = 1$, $\Lambda = 2$. At $x = 2$, $\Lambda = 2.5$. At $x = 1000$, $\Lambda = 1.002$ (as $x \to \infty$, $\Lambda \to 1$).  

The ratio increases from 0 to around 2 and then decreases. So the ratio is not monotonic.

A similar argument can be made, and is shown above, for decreasing values of X also exhibiting non-monotonicity for this example. 

Because the likelihood ratio is not monotonic, then the $\text{Cauchy}(\theta)$ family lacks MLR in $X$ or $-X$.

## b)

Show that the test

$$
\phi(x) =
\begin{cases}
1 & \text{if } 1 < x < 3 \\
0 & \text{otherwise}
\end{cases}
$$

is most powerful of its size for testing

$$
H_0 : \theta = 0 \quad \text{versus} \quad H_1 : \theta = 1.
$$

Calculate the Type I and Type II error probabilities.

### Hint: 

Show that the test given is equivalent to rejecting $H_0$ if

$$
f(x|\theta = 1) > 2f(x|\theta = 0)
$$

and not rejecting otherwise. Conclude that this must be the most powerful (MP) test for its size. Justify why.

### Answer 

Consider the test provided in the hint:  

$$
\varphi(x) = \begin{cases} 
1 & \text{if } 1 < x < 3, \\
0 & \text{otherwise}
\end{cases}
$$

By the Neyman-Pearson Lemma, the MP test rejects $H_0$ when:  

$$
\frac{f(x|1)}{f(x|0)} = \frac{1 + x^2}{1 + (x - 1)^2} > k
$$


We know that the ratio $\frac{f(x|1)}{f(x|0)}$ has critical points at $x = \frac{1 \pm \sqrt{5}}{2}$, because: 

$$
\Lambda' = \frac{d\Lambda}{dx} = \frac{(2x)(x^2 - 2x + 2) - (1 + x^2)(2x - 2)}{(x^2 - 2x + 2)^2}
$$

$$
\Lambda' = 0 \rightarrow 2x(x^2 - 2x + 2) - (1 + x^2)(2x - 2) = 0 \rightarrow 
x = \frac{1 \pm \sqrt{5}}{2}
$$

At any rate, at $x = 1$ and $x = 3$:  

$$
\frac{f(1|1)}{f(1|0)} = \frac{f(3|1)}{f(3|0)} = 2
$$  

And the set $\{x: \frac{f(x|1)}{f(x|0)} > 2\} = (1, 3)$ exactly matches the closed form expression of our test function, $\varphi(x)$.  

Since these are one and the same, then $\varphi(x)$ is the most powerful test for its size.

Let us then consider the hypotheses we're dealing with. 

Under $H_0$, the Type I Error Rate is:  

$$
\alpha = P(1 < X < 3 \mid \theta = 0) = \frac{1}{\pi} \left( \tan^{-1}(3) - \tan^{-1}(1) \right) \approx 0.1476
$$

Under $H_1$, the Type II Error Rate is: 

$$
\beta = 1 - P(1 < X < 3 \mid \theta = 1) = 1 - \frac{1}{\pi} \left( \tan^{-1}(2) - \tan^{-1}(0) \right) \approx 0.6476
$$

So $\varphi(x)$ as defined is MP with $\alpha \approx 0.1476$ (Type I Error Rate) and $\beta \approx 0.6476$ (Type II Error Rate).  

### Additional Justification For Most Powerful Test

I believe the above is an appropriate solution, but for the sake of completeness I wanted to make the connection a bit more explicit to the hint provided. 

To that end: 

The Neyman–Pearson Lemma tells us the MP test for testing simple hypotheses $H_0$ vs $H_1$ is:

$$
\varphi(x) =
\begin{cases}
1 & \text{if } \Lambda > k \\
0 & \text{otherwise}
\end{cases}
$$

where the likelihood ratio is given by the expression:

$$
\Lambda = \frac{f(x \mid \theta = 1)}{f(x \mid \theta = 0)} = \frac{1 + x^2}{1 + (x - 1)^2}
$$

Given the hint, let us see where this ratio exceeds 2, i.e. when:

$$
\frac{1 + x^2}{1 + (x - 1)^2} > 2
$$

"Solving" this inequality, i.e., finding the appropriate range of x values:

$$
\frac{1 + x^2}{x^2 - 2x + 2} > 2
\quad \rightarrow \quad
1 + x^2 > 2(x^2 - 2x + 2)
\quad \rightarrow \quad
1 + x^2 > 2x^2 - 4x + 4
\rightarrow
0 > x^2 - 4x + 3
$$

We then have: 

$$
x^2 - 4x + 3 < 0 \rightarrow  
(x - 1)(x - 3) < 0
\quad \rightarrow \quad
x \in (1, 3)
$$

Thus, the likelihood ratio exceeds 2 exactly when $x \in (1, 3)$, matching directly with the hint provided. 

Connecting this back to the test function, we then know: 

$$
\varphi(x) = 
\begin{cases}
1 & \text{if } 1 < x < 3 \\
0 & \text{otherwise}
\end{cases}
$$

is equivalent to the Neyman–Pearson test with k = 2, and since the test rejects $H_0$ when $\Lambda > 2$, with $\Lambda$ as defined previously. And we know the size of this test is fixed!  

Such that we have the Type I and Type II errors derived previously, but now with a more explicit connection to the test equivalency. 

\newpage

# Q4

Consider one observation $X$ from the probability density function

$$
f(x \mid \theta) = 1 - \theta^2 \left( x - \frac{1}{2} \right), \quad 0 \leq x \leq 1,\quad 0 \leq \theta \leq 1.
$$

We wish to test:

$$
H_0: \theta = 0 \quad \text{vs.} \quad H_1: \theta > 0
$$

## a)

Find the UMP test of size $\alpha = 0.05$ based on $X$. Carefully justify your answer.

### Answer 

Under $H_0$ ($\theta = 0$):

$$
f(x \mid 0) = 1 \quad \Rightarrow \quad X \sim \text{Uniform}(0,1).
$$

And under $H_1$ ($\theta > 0$):

$$
f(x \mid \theta) = 1 - \theta^2 \left( x - \frac{1}{2} \right).
$$

$x = \frac{1}{2}$ is an "inflection point" of sorts, such that the behavior of the pdf around $\frac{1}{2}$ will provide insight. 

When $x < \frac{1}{2}$, $f(x \mid \theta) > 1$. So we observe larger density near $x=0$ under the alternative.   

When $x > \frac{1}{2}$, $f(x \mid \theta) < 1$. So we observe smaller density near $x=1$ under the alternative. 

Turning then to the likelihood ratio for $H_0: \theta = 0$ vs. $H_1: \theta = \theta_1 > 0$:

$$
\Lambda = \frac{f(x \mid \theta_1)}{f(x \mid 0)} = 1 - \theta_1^2 \left( x - \frac{1}{2} \right)
$$

$\Lambda$ is decreasing in $x$, maximized at $x = 0$ (where $\Lambda = 1 + \frac{\theta_1^2}{2} > 1$), and minimized at $x = 1$ (where $\Lambda = 1 - \frac{\theta_1^2}{2} < 1$).

Via Neyman-Pearson, the MP test rejects $H_0$ when $\Lambda > k$, which occurs when small $x$ is observed.

Rejection Region: The MP test rejects for $x < c$, where $c$ is chosen to control the size of a given $\alpha$, $\alpha = 0.05$.  

Under $H_0$, $X \sim \text{Uniform}(0,1)$, so we can calculate the probability explicitly:

$$
P_{\theta=0}(X < c) = c = 0.05 \rightarrow c = 0.05.
$$

Via the above rejection region, we may then construct the test function:  

$$
\varphi(x) = 
\begin{cases} 
1 & \text{if } x < 0.05, \\ 
0 & \text{otherwise}
\end{cases}
$$

As this test function does not depend on $\theta_1$, it is also UMP for all $\theta > 0$.  

## b)

Find the likelihood ratio test statistic $\lambda(X)$ based on $X$, expressed as a function of $X$.

### Answer

The LRT is:

$$
\lambda(X) = \frac{f(X \mid 0)}{\text{max}_{\theta \in [0,1]} f(X \mid \theta)} = \frac{1}{\text{max}_{\theta} \left[1 - \theta^2 (X - \frac{1}{2})\right]}
$$

Again, our critical value is at $\frac{1}{2}$, so we consider the behavior of the LRT at the value of, less than, and greater than $x = \frac{1}{2}$. 

For $X \geq \frac{1}{2}$, the maximum occurs at $\theta = 0$, i.e.,:

$$
\text{max}_{\theta} f(X \mid \theta) = 1
$$

For $X < \frac{1}{2}$, the maximum occurs at $\theta = 1$:

$$
\text{max}_{\theta} f(X \mid \theta) = 1 + \left( \frac{1}{2} - X \right) = 1.5 - X
$$

Incorporating the two cases together, our LRT is of the form: 

$$
\lambda(X) =
\begin{cases}
\frac{1}{1.5 - X} & \text{if } X < \frac{1}{2} \\
1 & \text{if } X \geq \frac{1}{2}
\end{cases}
$$

## c)

Find the likelihood ratio test (LRT) of size $\alpha = 0.05$ for the above hypotheses.

### Answer

Rejection Region: From part b), $\lambda(X) = 1$ for $X \geq \frac{1}{2}$, and is increasing for $X < \frac{1}{2}$. So to make the test most powerful while maintaining the correct size, we reject for large values of $X$, where the "large values" are determined by the size condition, which is:

$$
P_{\theta=0}(X > k) = 1 - k = 0.05 \quad \Rightarrow \quad k = 0.95
$$

Taken together, we reject $H_0$ when $X > 0.95$. So the test of size $\alpha = 0.05$ is given by:

$$
\varphi(x) =
\begin{cases}
1 & \text{if } x > 0.95 \\
0 & \text{otherwise}
\end{cases}
$$
