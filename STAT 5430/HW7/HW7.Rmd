---
title: "HW7"
output: pdf_document
author: "Sam Olson"
---

# Q1 

Problem 8.6 a) - b), Casella and Berger (2nd Edition)

Suppose that we have two independent random samples: $X_1, \ldots, X_n$ are exponential$(\theta)$, and $Y_1, \ldots, Y_m$ are exponential$(\mu)$.

## a) 

Find the LRT of

$$
H_0: \theta = \mu \quad \text{versus} \quad H_1: \theta \ne \mu.
$$

### Answer

The LRT statistic is of the form:  

$$
\lambda(x, y) = \frac{\text{max}_{\theta} L(\theta \mid x, y)}{\text{max}_{\theta, \mu} L(\theta, \mu \mid x, y)}
$$

Where, under $H_0$ ($\theta = \mu$).

Generally, we know that, the MLE will be some weighted average of the observations, taking advantage of the one parameter exponential families known to be complete and their MLEs of a general form. That is, the MLE will be some weighted average of the observations, taking advantage of the one parameter exponential families known to be complete and their MLEs of a general form. 

Under $H_0$ (to get the numerator of the LRT) the MLE for $\theta$ is of the form:  

$$
\hat{\theta}_{H_0} = \frac{\sum_{i=1}^n X_i + \sum_{j=1}^m Y_j}{n + m}
$$

And, under the full model (the denominator of the LRT), the MLEs are the individual sample means, i.e.:  

$$
\hat{\theta} = \bar{X} = \frac{\sum X_i}{n}, \quad \hat{\mu} = \bar{Y} = \frac{\sum Y_j}{m}
$$

Returning to the original LRT expression, we then have:  

$$
\lambda(x, y) = \frac{(\hat{\theta}_0)^{-(n+m)} e^{-(n+m)}}{(\hat{\theta})^{-n} e^{-n} (\hat{\mu})^{-m} e^{-m}} = \frac{(\bar{X})^n (\bar{Y})^m}{\left( \frac{\sum X_i + \sum Y_j}{n + m} \right)^{n+m}}
= \frac{(n+m)^{n+m} (\sum X_i)^n (\sum Y_j)^m}{n^n m^m (\sum X_i + \sum Y_j)^{n+m}}
$$

We may then construct our test function, where our rejection rule is to "Reject $H_0$ if $\lambda(x, y) \leq c$", where $c$ is calibrated based on the significance level $\alpha$, i.e.:

$$
\varphi(x, y) = 
\begin{cases} 
1 & \text{if } \lambda(x, y) \leq c, \\
0 & \text{otherwise}
\end{cases}
$$

Where (to save space above), we have:

$$
\lambda(x, y) = \frac{(n+m)^{n+m} (\sum X_i)^n (\sum Y_j)^m}{n^n m^m (\sum X_i + \sum Y_j)^{n+m}}
$$

And $c$ is chosen such that $P(\varphi(X, Y) = 1 \mid H_0) = \alpha$. 

## b) 

Show that the test in part a) can be based on the statistic

$$
T = \frac{\sum X_i}{\sum X_i + \sum Y_i}
$$

### Answer

Let $T = \frac{\sum X_i}{\sum X_i + \sum Y_j}$.  

Rewriting the LRT from part a) in terms of $T$:  

$$
\lambda(x, y) = \frac{(n+m)^{n+m}}{n^n m^m} \left( \frac{\sum X_i}{\sum X_i + \sum Y_j} \right)^n \left( \frac{\sum Y_j}{\sum X_i + \sum Y_j} \right)^m = \frac{(n+m)^{n+m}}{n^n m^m} T^n (1-T)^m
$$

Since $\lambda(x, y)$ depends on the data only through $T$, the LRT can be based entirely on $T$.

Using the above, we may define the rejection region where the test rejects $H_0$ when $T$ is too small or too large with constants a and b, where:  

$$
T \leq a \quad \text{or} \quad T \geq b
$$

And where $a$ and $b$ are critical values satisfying:  

$$
P(T \leq a \mid H_0) + P(T \geq b \mid H_0) = \alpha
$$

Under $H_0$ ($\theta = \mu$), $\sum X_i \sim \text{Gamma}(n, \theta)$, $\sum Y_j \sim \text{Gamma}(m, \theta)$, as the sum of iid Exponentials is Gamma, and a linear combination of Gamma distributions with common rate parameter $\theta$ is a Beta. Since both X and Y are independent of one another, their sums are also independent, and determining the parameters of the T Beta distribution becomes a matter of algebra (the distribution of T does not involve $\theta$).

And:  

$$
T = \frac{\sum X_i}{\sum X_i + \sum Y_j} \sim \text{Beta}(n, m)
$$

So the critical values being referenced above may be found via taking critical regions of the Beta distribution when n and m are known. 

\newpage

# Q2 

Problem 8.28, Casella and Berger (2nd Edition)

Let $f(x|\theta)$ be the logistic location probability density function:

$$
f(x|\theta) = \frac{e^{(x - \theta)}}{(1 + e^{(x - \theta)})^2}, \quad -\infty < x < \infty, \quad -\infty < \theta < \infty.
$$

## a) 

Show that this family has an MLR. 

### Answer

Let $\theta_2 > \theta_1$. 

We know the likelihood ratio statistic is given by:

$$
\Lambda = \frac{f(x|\theta_2)}{f(x|\theta_1)} = e^{\theta_1 - \theta_2} \left[ \frac{1 + e^{x - \theta_1}}{1 + e^{x - \theta_2}} \right]^2
$$

The derivative wrt X is of the form: 

$$
\Lambda' = \frac{e^{x - \theta_1}(1 + e^{x - \theta_2}) - e^{x - \theta_2}(1 + e^{x - \theta_1})}{(1 + e^{x - \theta_2})^2} = \frac{e^{x - \theta_1} - e^{x - \theta_2}}{(1 + e^{x - \theta_2})^2} > 0
$$

And the inequality holds because of the assumption $\theta_2 > \theta_1$, which is allowed in the full parameter space. 

Thus, our likelihood ratio is strictly increasing in $x$, meaning it is monotonic, i.e. that the family $\{f(x|\theta)\}$ has MLR in $x$.

## b) 

Based on one observation $X$, find the most powerful size $\alpha$ test of

$$
H_0: \theta = 0 \quad \text{versus} \quad H_1: \theta = 1.
$$

For $\alpha = 0.2$, find the size of the Type II error.

### Answer 

By the Neyman-Pearson Lemma, the MP test rejects $H_0$ when:

$$
\Lambda = \frac{f(x|1)}{f(x|0)} = e^{-1}\left(\frac{1 + e^x}{1 + e^{x - 1}}\right)^2 > k
$$

From from part a), since the likelihood ratio is increasing in $x$, the MP test rejects if $X > k_1$, where $k_1$ is determined by the size $\alpha$.

As we know the underlying distributions, let us consider the CDF of the logistic distribution:

$$
F(x|\theta) = \frac{e^{x - \theta}}{1 + e^{x - \theta}}
$$

Under $H_0$, the size condition is:

$$
P(X > k_1 \mid \theta = 0) = 1 - F(k_1|0) = \frac{1}{1 + e^{k_1}} = \alpha.
$$

Solving for $k_1$:

$$
k_1 = \log\left( \frac{1 - \alpha}{\alpha} \right) = \log(\alpha^{-1} - 1)
$$

For $\alpha = 0.2$: 

$$
k_1 = \log(0.2^{-1} - 1) = \log(4) \approx 1.386
$$

Under $H_1$:

$$
\beta = P(X \leq k_1 \mid \theta = 1) = F(k_1|1) = \frac{e^{k_1 - 1}}{1 + e^{k_1 - 1}} \approx \frac{e^{0.386}}{1 + e^{0.386}} \approx 0.595
$$

So, the MP level test of size $\alpha = 0.2$ rejects when our single observation $X > 1.386$, with a Type II error rate of 0.595.

## c) 

Show that the test in part b) is UMP size $\alpha$ for testing

$$
H_0: \theta \leq 0 \quad \text{versus} \quad H_1: \theta > 0.
$$

What can be said about UMP tests in general for the logistic location family?

### Answer 

Via MLR: From part a), the family has MLR in $X$.

Via Karlin-Rubin Thm. (Knew it would come up again!): Since the MP test for $\theta = 0$ vs $\theta = 1$ rejects for large $X$ and does not depend on the specific parameter value, i.e., $\theta_1 = ...$, it depends solely upon the observed value X, the MP test is also the UMP test for $H_0: \theta \leq 0$ vs $H_1: \theta > 0$.

The above results extend to similar distributions within the the logistic location family, i.e., UMP tests for one-sided hypotheses both exist and take the form "Reject $H_0$ if $X > c$."

\newpage

# Q3 

Problem 8.29 a) - b), Casella and Berger (2nd Edition)

Let $X$ be one observation from a Cauchy$(\theta)$ distribution.

The Cauchy$(\theta)$ density is given by:

$$
f(x|\theta) = \frac{1}{\pi} \cdot \frac{1}{1 + (x - \theta)^2}, \quad x \in \mathbb{R}, -\infty < \theta < \infty.
$$

## a)

Show that this family does not have an MLR.

### Hint: 

Show that the Cauchy$(\theta)$ family $\{ f(x|\theta) : \theta \in \mathbb{R} = \Theta \}$, based on one observation $X$, does not have monotone likelihood ratio (MLR) in $t(X) = X$ or $t(X) = -X$. That is, the ratio

$$
\frac{f(x|\theta_2)}{f(x|\theta_1)}
$$

might not be monotone (either increasing or decreasing) in $x$.

### Answer 

Let $\theta_2 > \theta_1$ under the setup of the problem. 

The likelihood ratio is of the form:

$$
\Lambda = \frac{f(x|\theta_2)}{f(x|\theta_1)} = \frac{1 + (x - \theta_1)^2}{1 + (x - \theta_2)^2}
$$

We know that:  

$$
\lim_{x \to \pm \infty} \frac{f(x|\theta_2)}{f(x|\theta_1)} = 1
$$

So, the ratio achieves a maximum at 1. 

As we seek to disprove that the ratio is not monotonic, we need only one example that displays non-monotonicity. 

For example, let $\theta_1 = 0$, $\theta_2 = 1$ such that our base assumption that $\theta_2 > \theta_1$ holds. 

Then:

$$
\Lambda = \frac{f(x|1)}{f(x|0)} = \frac{1 + x^2}{1 + (x - 1)^2}
$$

```{r}
cauchy <- function(x) {
  numerator <- 1 + x^2 
  denominator <- 1 + (x-1)^2
  numerator/denominator
}
```

```{r, eval = F}
cauchy(0)
cauchy(1) 
cauchy(2)
cauchy(3)
cauchy(1000)
```

```{r}
x_vals <- seq(0, 10, length.out = 500)
y_vals <- cauchy(x_vals)

plot(x_vals, y_vals, type = "l", lwd = 2, col = "black",
     main = "Cauchy Ratio",
     xlab = "x", ylab = expression(Lambda))
```

At $x = 0$, $\Lambda = 0.5$. At $x = 1$, $\Lambda = 2$. At $x = 2$, $\Lambda = 2.5$. At $x = 1000$, $\Lambda = 1.002$ (as $x \to \infty$, $\Lambda \to 1$).  

The ratio increases from 0 to around 2 and then decreases. So the ratio is not monotonic.

Because the likelihood ratio is not monotonic, then the $\text{Cauchy}(\theta)$ family lacks MLR in $X$ or $-X$.

## b)

Show that the test

$$
\phi(x) =
\begin{cases}
1 & \text{if } 1 < x < 3 \\
0 & \text{otherwise}
\end{cases}
$$

is most powerful of its size for testing

$$
H_0 : \theta = 0 \quad \text{versus} \quad H_1 : \theta = 1.
$$

Calculate the Type I and Type II error probabilities.

### Hint: 

Show that the test given is equivalent to rejecting $H_0$ if

$$
f(x|\theta = 1) > 2f(x|\theta = 0)
$$

and not rejecting otherwise. Conclude that this must be the most powerful (MP) test for its size. Justify why.

### Answer 

Consider the test provided in the hint:  

$$
\phi(x) = \begin{cases} 
1 & \text{if } 1 < x < 3, \\
0 & \text{otherwise}
\end{cases}
$$

By the Neyman-Pearson Lemma, the MP test rejects $H_0$ when:  

$$
\frac{f(x|1)}{f(x|0)} = \frac{1 + x^2}{1 + (x - 1)^2} > k
$$


We know that the ratio $\frac{f(x|1)}{f(x|0)}$ has critical points at $x = \frac{1 \pm \sqrt{5}}{2}$, because: 

$$
\Lambda' = \frac{d\Lambda}{dx} = \frac{(2x)(x^2 - 2x + 2) - (1 + x^2)(2x - 2)}{(x^2 - 2x + 2)^2}
$$

$$
\Lambda' = 0 \rightarrow 2x(x^2 - 2x + 2) - (1 + x^2)(2x - 2) = 0 \rightarrow 
x = \frac{1 \pm \sqrt{5}}{2}
$$

At any rate, at $x = 1$ and $x = 3$:  

$$
\frac{f(1|1)}{f(1|0)} = \frac{f(3|1)}{f(3|0)} = 2
$$  

And the set $\{x: \frac{f(x|1)}{f(x|0)} > 2\} = (1, 3)$ exactly matches the closed form expression of $\phi(x)$.  

Since these are one and the same, then $\phi(x)$ is the most powerful test for its size.

Let us then consider the hypotheses we're dealing with. 

Under $H_0$:  

$$
\alpha = P(1 < X < 3 \mid \theta = 0) = \frac{1}{\pi} \left( \tan^{-1}(3) - \tan^{-1}(1) \right) \approx 0.1476
$$

Under $H_1$: 

$$
\beta = 1 - P(1 < X < 3 \mid \theta = 1) = 1 - \frac{1}{\pi} \left( \tan^{-1}(2) - \tan^{-1}(0) \right) \approx 0.6476
$$

So $\phi(x)$ is MP with $\alpha \approx 0.1476$ (Type I Error Rate) and $\beta \approx 0.6476$ (Type II Error Rate).  

\newpage

# Q4

Consider one observation $X$ from the probability density function

$$
f(x \mid \theta) = 1 - \theta^2 \left( x - \frac{1}{2} \right), \quad 0 \leq x \leq 1,\quad 0 \leq \theta \leq 1.
$$

We wish to test:
$$
H_0: \theta = 0 \quad \text{vs.} \quad H_1: \theta > 0
$$

## a)

Find the UMP test of size $\alpha = 0.05$ based on $X$. Carefully justify your answer.

### Answer 

To find a UMP test of size $\alpha = 0.05$, we first turn to the question of monotonicity, specifically to determine if the family of distribution has MLR in x. 

Let $\theta_2 > \theta_1 \geq 0$, as we do. 

Then, the likelihood ratio is given by:

$$
\Lambda = \frac{f(x \mid \theta_2)}{f(x \mid \theta_1)} = \frac{1 - \theta_2^2 (x - \frac{1}{2})}{1 - \theta_1^2 (x - \frac{1}{2})}
$$

To analyze monotonicity of the ratio, consider first that $x = \frac{1}{2}$ is a critical point. So, we analyze the behavior of the ratio when x is less than or greater than the critical point. Specifically:

When $x < \frac{1}{2}$: both numerator and denominator are increasing in $x$, and the ratio increases.

When $x > \frac{1}{2}$: both numerator and denominator are decreasing in $x$, and the ratio decreases.

So unfortunately, $\Lambda$ is not monotonic across the domain, i.e., the direction of monotonicity changes at $x = \frac{1}{2}$. And consequently the family does not MLR in $x$ (no Karlin-Rubin to make things easier). 

However, we can still turn to the Neyman–Pearson Lemma to construct the most powerful test for:

$$
H_0: \theta = 0 \quad \text{vs.} \quad H_1: \theta = \theta_1 > 0
$$

Under $H_0$:

$$
f(x \mid 0) = 1, \quad \text{so } X \sim \text{Uniform}(0, 1)
$$

Under $H_1$:

$$
f(x \mid \theta_1) = 1 - \theta_1^2 \left( x - \frac{1}{2} \right)
$$

This density under the alternative hypothesis is greater than 1 when $x < \frac{1}{2}$, and less than 1 when $x > \frac{1}{2}$. So $f(x \mid \theta_1)$ has more mass in the tails (0 and 1) than under $H_0$. So we'd expect the LRT to be much larger (tend to reject $H_0$) for observations near the tails of the distribution (near 0 or near 1).  

So, generally, the most powerful test at level $\alpha$ will reject $H_0$ for values of $x$ far from $1/2$, i.e. when values are "close" to 0 or "close" to 1.

Under $H_0$, $X \sim \text{Unif}(0, 1)$, we construct a level-$\alpha = 0.05$ test that rejects for extreme values of $x$. 

We define our test function as:

$$
\varphi(x) =
\begin{cases}
1 & \text{if } x < c_1 \text{ or } x > c_2 \\
0 & \text{otherwise}
\end{cases}
$$

Noting that X is a continuous random variable such that our "coin flip" case in the above test is able to be simplified to $\gamma = 0$. 

At any rate, our next objective is to further specify the values of $c_1$ and $c_2$ such that:

$$
P_{\theta = 0}(X < c_1) + P_{\theta = 0}(X > c_2) = 0.05
$$

Since the Uniform(0,1) (distribution under the null) is symmetric, we can divide the error equally to both sides of the density, i.e.,:

$$
c_1 = \frac{\alpha}{2} = 0.025, \quad \text{ and } c_2 = 1 - \frac{\alpha}{2} = 0.975
$$

Giving us the test function of the form: 

$$
\varphi(x) =
\begin{cases}
1 & \text{if } x < 0.025 \text{ or } x > 0.975 \\
0 & \text{otherwise}
\end{cases}
$$

This test has size $\alpha = 0.05$, and by the Neyman–Pearson Lemma since the above MP test does not depend on the parameter $\theta_1$, it is also the UMP test of size $\alpha = 0.05$ for testing $H_0: \theta = 0$ vs. $H_1: \theta > 0$.

## b)

Find the likelihood ratio test statistic $\lambda(X)$ based on $X$, expressed as a function of $X$.

### Answer

The LRT is:

$$
\lambda(X) = \frac{f(X \mid 0)}{\text{max}_{\theta \in [0,1]} f(X \mid \theta)} = \frac{1}{\text{max}_{\theta} \left[1 - \theta^2 (X - \frac{1}{2})\right]}
$$

Again, our critical value is at $\frac{1}{2}$, so we consider the behavior of the LRT at the value of, less than, and greater than $x = \frac{1}{2}$. 

For $X \geq \frac{1}{2}$, the maximum occurs at $\theta = 0$, i.e.,:

$$
\text{max}_{\theta} f(X \mid \theta) = 1
$$

For $X < \frac{1}{2}$, the maximum occurs at $\theta = 1$:

$$
\text{max}_{\theta} f(X \mid \theta) = 1 + \left( \frac{1}{2} - X \right) = 1.5 - X
$$

Incorporating the two cases together, our LRT is of the form: 

$$
\lambda(X) =
\begin{cases}
\frac{1}{1.5 - X} & \text{if } X < \frac{1}{2} \\
1 & \text{if } X \geq \frac{1}{2}
\end{cases}
$$

## c)

Find the likelihood ratio test (LRT) of size $\alpha = 0.05$ for the above hypotheses.

### Answer

Rejection Region: From part b), $\lambda(X) = 1$ for $X \geq \frac{1}{2}$, and is increasing for $X < \frac{1}{2}$. So to make the test most powerful while maintaining the correct size, we reject for large values of $X$, where the "large values" are determined by the size condition, which is:

$$
P_{\theta=0}(X > k) = 1 - k = 0.05 \quad \Rightarrow \quad k = 0.95
$$

Taken together, we reject $H_0$ when $X > 0.95$. So the test of size $\alpha = 0.05$ is given by:

$$
\varphi(x) =
\begin{cases}
1 & \text{if } x > 0.95 \\
0 & \text{otherwise}
\end{cases}
$$
