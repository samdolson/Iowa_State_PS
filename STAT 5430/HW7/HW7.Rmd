---
title: "HW7"
output: pdf_document
author: "Sam Olson"
---

# Outline 

  - Q1: Edits
  - Q2: Edits
  - Q3: Edits
  - Q4: Edits

# Q1 

Problem 8.6 a) - b), Casella and Berger (2nd Edition)

Suppose that we have two independent random samples: $X_1, \ldots, X_n$ are exponential$(\theta)$, and $Y_1, \ldots, Y_m$ are exponential$(\mu)$.

## a) 

Find the LRT of

$$
H_0: \theta = \mu \quad \text{versus} \quad H_1: \theta \ne \mu.
$$

### Answer

The likelihood ratio test (LRT) statistic is:  

$$
\lambda(x, y) = \frac{\text{max}_{\theta} L(\theta \mid x, y)}{\text{max}_{\theta, \mu} L(\theta, \mu \mid x, y)}
$$

Under $H_0$ ($\theta = \mu$):  

The MLE for $\theta$ is obtained from the combined sample:  

$$
\hat{\theta}_0 = \frac{\sum_{i=1}^n X_i + \sum_{j=1}^m Y_j}{n + m}
$$

Under the full model:  

The MLEs are the sample means:  

$$
\hat{\theta} = \bar{X} = \frac{\sum X_i}{n}, \quad \hat{\mu} = \bar{Y} = \frac{\sum Y_j}{m}
$$

Substituting the MLEs, we get:  

$$
\lambda(x, y) = \frac{(\hat{\theta}_0)^{-(n+m)} e^{-(n+m)}}{(\hat{\theta})^{-n} e^{-n} (\hat{\mu})^{-m} e^{-m}} = \frac{(\bar{X})^n (\bar{Y})^m}{\left( \frac{\sum X_i + \sum Y_j}{n + m} \right)^{n+m}}
$$

Simplifying, this becomes:  

$$
\lambda(x, y) = \frac{(n+m)^{n+m} (\sum X_i)^n (\sum Y_j)^m}{n^n m^m (\sum X_i + \sum Y_j)^{n+m}}
$$

Rejection Rule: Reject $H_0$ if $\lambda(x, y) \leq c$, where $c$ is chosen for significance level $\alpha$.

$$
\varphi(x, y) = 
\begin{cases} 
1 & \text{if } \lambda(x, y) \leq c, \\
0 & \text{otherwise},
\end{cases}
$$

where 

$$
\lambda(x, y) = \frac{(n+m)^{n+m} (\sum X_i)^n (\sum Y_j)^m}{n^n m^m (\sum X_i + \sum Y_j)^{n+m}}
$$

And $c$ is chosen such that $\mathbb{P}(\varphi(X, Y) = 1 \mid H_0) = \alpha$. 

## b) 

Show that the test in part a) can be based on the statistic

$$
T = \frac{\sum X_i}{\sum X_i + \sum Y_i}.
$$

### Answer

Let $T = \frac{\sum X_i}{\sum X_i + \sum Y_j}$.  

Rewriting $\lambda(x, y)$ in terms of $T$:  

$$
\lambda(x, y) = \frac{(n+m)^{n+m}}{n^n m^m} \left( \frac{\sum X_i}{\sum X_i + \sum Y_j} \right)^n \left( \frac{\sum Y_j}{\sum X_i + \sum Y_j} \right)^m = \frac{(n+m)^{n+m}}{n^n m^m} T^n (1-T)^m
$$

Since $\lambda(x, y)$ depends on the data only through $T$, the LRT can be based entirely on $T$.

Rejection Region:  

The test rejects $H_0$ when $T$ is too small or too large, i.e.,  

$$
T \leq a \quad \text{or} \quad T \geq b
$$

where $a$ and $b$ are critical values satisfying:  

$$
P(T \leq a \mid H_0) + P(T \geq b \mid H_0) = \alpha
$$

Distribution of $T$ under $H_0$:  

Under $H_0$ ($\theta = \mu$):  

- $\sum X_i \sim \text{Gamma}(n, \theta)$  
- $\sum Y_j \sim \text{Gamma}(m, \theta)$  

Thus,  

$$
T = \frac{\sum X_i}{\sum X_i + \sum Y_j} \sim \text{Beta}(n, m)
$$

Alternatively, $\frac{T}{1-T} = \frac{\sum X_i / n}{\sum Y_j / m} \sim F_{2n, 2m}$, which can be used to compute critical values. 

\newpage

# Q2 

Problem 8.28, Casella and Berger (2nd Edition)

Let $f(x|\theta)$ be the logistic location probability density function:

$$
f(x|\theta) = \frac{e^{(x - \theta)}}{(1 + e^{(x - \theta)})^2}, \quad -\infty < x < \infty, \quad -\infty < \theta < \infty.
$$

## a) 

Show that this family has an MLR. 

### Answer

To show the family has a monotone likelihood ratio (MLR) in $x$, consider the likelihood ratio for $\theta_2 > \theta_1$:

$$
\frac{f(x|\theta_2)}{f(x|\theta_1)} = e^{\theta_1 - \theta_2} \left[ \frac{1 + e^{x - \theta_1}}{1 + e^{x - \theta_2}} \right]^2.
$$

Define $g(x) = \frac{1 + e^{x - \theta_1}}{1 + e^{x - \theta_2}}$. Its derivative is:

$$
g'(x) = \frac{e^{x - \theta_1}(1 + e^{x - \theta_2}) - e^{x - \theta_2}(1 + e^{x - \theta_1})}{(1 + e^{x - \theta_2})^2} = \frac{e^{x - \theta_1} - e^{x - \theta_2}}{(1 + e^{x - \theta_2})^2} > 0,
$$

where the inequality holds because $\theta_2 > \theta_1$. Thus, $g(x)$ is strictly increasing in $x$, and so is the likelihood ratio.  

Conclusion: The family $\{f(x|\theta)\}$ has MLR in $x$.

## b) 

Based on one observation $X$, find the most powerful size $\alpha$ test of

$$
H_0: \theta = 0 \quad \text{versus} \quad H_1: \theta = 1.
$$

For $\alpha = 0.2$, find the size of the Type II error.

### Answer 

By the Neyman-Pearson Lemma, the most powerful (MP) test rejects $H_0$ when:

$$
\frac{f(x|1)}{f(x|0)} = e^{-1}\left(\frac{1 + e^x}{1 + e^{x - 1}}\right)^2 > k.
$$

Since the likelihood ratio is increasing in $x$ (from part (a)), the MP test rejects if $X > k'$, where $k'$ is determined by the size $\alpha$.

The CDF of the logistic distribution is:

$$
F(x|\theta) = \frac{e^{x - \theta}}{1 + e^{x - \theta}}.
$$

Under $H_0$, the size condition is:

$$
\mathbb{P}(X > k' \mid \theta = 0) = 1 - F(k'|0) = \frac{1}{1 + e^{k'}} = \alpha.
$$

Solving for $k'$:

$$
k' = \log\left( \frac{1 - \alpha}{\alpha} \right).
$$

For $\alpha = 0.2$:  
$$
k' = \log(4) \approx 1.386.
$$

Under $H_1$:

$$
\beta = \mathbb{P}(X \leq k' \mid \theta = 1) = F(k'|1) = \frac{e^{k' - 1}}{1 + e^{k' - 1}} \approx \frac{e^{0.386}}{1 + e^{0.386}} \approx 0.595.
$$

Conclusion: The MP level-0.2 test rejects when $X > 1.386$, with a Type II error rate of approximately 0.595.

## c) 

Show that the test in part b) is UMP size $\alpha$ for testing

$$
H_0: \theta \leq 0 \quad \text{versus} \quad H_1: \theta > 0.
$$

What can be said about UMP tests in general for the logistic location family?

### Answer 

1. MLR Property: From part (a), the family has MLR in $X$.
2. Karlin-Rubin Theorem: Since the MP test for $\theta = 0$ vs $\theta = 1$ rejects for large $X$ and does not depend on the specific $\theta_1 = 1$, it is uniformly most powerful (UMP) for $H_0: \theta \leq 0$ vs $H_1: \theta > 0$.

General Conclusion:  
For the logistic location family, UMP tests exist for one-sided hypotheses and take the form "Reject $H_0$ if $X > c$."

\newpage

# Q3 

Problem 8.29 a) - b), Casella and Berger (2nd Edition)

Let $X$ be one observation from a Cauchy$(\theta)$ distribution.

The Cauchy$(\theta)$ density is given by:

$$
f(x|\theta) = \frac{1}{\pi} \cdot \frac{1}{1 + (x - \theta)^2}, \quad x \in \mathbb{R}, -\infty < \theta < \infty.
$$

## a)

Show that this family does not have an MLR.

### Hint: 

Show that the Cauchy$(\theta)$ family $\{ f(x|\theta) : \theta \in \mathbb{R} = \Theta \}$, based on one observation $X$, does not have monotone likelihood ratio (MLR) in $t(X) = X$ or $t(X) = -X$. That is, the ratio

$$
\frac{f(x|\theta_2)}{f(x|\theta_1)}
$$

might not be monotone (either increasing or decreasing) in $x$.

### Answer 

For $\theta_2 > \theta_1$, the likelihood ratio is:

$$
\frac{f(x|\theta_2)}{f(x|\theta_1)} = \frac{1 + (x - \theta_1)^2}{1 + (x - \theta_2)^2}.
$$

1. Limiting Behavior:  
   $$
   \lim_{x \to \pm \infty} \frac{f(x|\theta_2)}{f(x|\theta_1)} = 1.
   $$

2. Non-Monotonicity:  
   - The ratio achieves a maximum at finite $x$. For example, let $\theta_1 = 0$, $\theta_2 = 1$:  
     $$
     \frac{f(x|1)}{f(x|0)} = \frac{1 + x^2}{1 + (x - 1)^2}.
     $$
     - At $x = 0$: Ratio $= 1$.  
     - At $x = 1$: Ratio $= 2$.  
     - As $x \to \infty$: Ratio $\to 1$.  
   - The ratio increases from $x = 0$ to $x = 1$ and then decreases, proving non-monotonicity.

Conclusion: The $\text{Cauchy}(\theta)$ family lacks MLR in $X$ or $-X$.

## b)

Show that the test

$$
\phi(x) =
\begin{cases}
1 & \text{if } 1 < x < 3 \\
0 & \text{otherwise}
\end{cases}
$$

is most powerful of its size for testing

$$
H_0 : \theta = 0 \quad \text{versus} \quad H_1 : \theta = 1.
$$

Calculate the Type I and Type II error probabilities.

### Hint: 

Show that the test given is equivalent to rejecting $H_0$ if

$$
f(x|\theta = 1) > 2f(x|\theta = 0)
$$

and not rejecting otherwise. Conclude that this must be the most powerful (MP) test for its size. Justify why.

### Answer 

Consider the test:  
$$
\phi(x) = \begin{cases} 
1 & \text{if } 1 < x < 3, \\
0 & \text{otherwise}.
\end{cases}
$$

By the Neyman-Pearson Lemma, the MP test rejects $H_0$ when:  
$$
\frac{f(x|1)}{f(x|0)} = \frac{1 + x^2}{1 + (x - 1)^2} > k.
$$

- The ratio $\frac{f(x|1)}{f(x|0)}$ has critical points at $x = \frac{1 \pm \sqrt{5}}{2}$.  
- At $x = 1$ and $x = 3$:  
  $$
  \frac{f(1|1)}{f(1|0)} = \frac{f(3|1)}{f(3|0)} = 2.
  $$  
- The set $\{x: \frac{f(x|1)}{f(x|0)} > 2\} = (1, 3)$ exactly matches $\phi(x)$.  

Thus, $\phi(x)$ is the most powerful test for its size.

Under $H_0$:  
$$
\alpha = \mathbb{P}(1 < X < 3 \mid \theta = 0) = \frac{1}{\pi} \left( \tan^{-1}(3) - \tan^{-1}(1) \right) \approx 0.1476.
$$

Under $H_1$:  
$$
\beta = 1 - \mathbb{P}(1 < X < 3 \mid \theta = 1) = 1 - \frac{1}{\pi} \left( \tan^{-1}(2) - \tan^{-1}(0) \right) \approx 0.6476.
$$

Conclusion:  
- $\phi(x)$ is MP with $\alpha \approx 0.1476$ and $\beta \approx 0.6476$.  
- The rejection region $(1, 3)$ is unique for this $\alpha$, as guaranteed by Neyman-Pearson.

\newpage

# Q4

Consider one observation $X$ from the probability density function

$$
f(x \mid \theta) = 1 - \theta^2 \left( x - \frac{1}{2} \right), \quad 0 \leq x \leq 1,\quad 0 \leq \theta \leq 1.
$$

We wish to test:

$$
H_0: \theta = 0 \quad \text{vs.} \quad H_1: \theta > 0
$$

## a)

Find the UMP test of size $\alpha = 0.05$ based on $X$. Carefully justify your answer.

### Answer 

1. Likelihood Ratio Analysis:

For $\theta_2 > \theta_1 \geq 0$, the likelihood ratio is:

$$
\frac{f(x \mid \theta_2)}{f(x \mid \theta_1)} = \frac{1 - \theta_2^2 (x - \frac{1}{2})}{1 - \theta_1^2 (x - \frac{1}{2})}
$$

2. Monotonicity Properties:

- When $x > \frac{1}{2}$: the ratio is decreasing in $x$
- When $x < \frac{1}{2}$: the ratio is increasing in $x$

So, the family does not have global monotone likelihood ratio in $X$, but the likelihood function tilts rightward under $H_1$, which suggests rejecting for large $X$ values is most powerful.

3. UMP Test Construction:

Under $H_0: \theta = 0$, $X \sim \text{Uniform}(0, 1)$. So we define the test:

$$
\phi(x) =
\begin{cases}
1 & \text{if } x > c \\
0 & \text{otherwise}
\end{cases}
$$

4. Critical Value Calculation:

$$
P_{\theta=0}(X > c) = 1 - c = 0.05 \quad \Rightarrow \quad c = 0.95
$$

Final UMP Test:

$$
\phi(x) =
\begin{cases}
1 & \text{if } x > 0.95 \\
0 & \text{otherwise}
\end{cases}
$$

## b)

Find the likelihood ratio test statistic $\lambda(X)$ based on $X$, expressed as a function of $X$.

### Answer

The likelihood ratio test statistic is:

$$
\lambda(X) = \frac{f(X \mid 0)}{\text{max}_{\theta \in [0,1]} f(X \mid \theta)} = \frac{1}{\text{max}_{\theta} \left[1 - \theta^2 (X - \frac{1}{2})\right]}
$$

Case 1: $X \geq \frac{1}{2}$

Maximum occurs at $\theta = 0$:

$$
\text{max}_{\theta} f(X \mid \theta) = 1
$$

Case 2: $X < \frac{1}{2}$

Maximum occurs at $\theta = 1$:

$$
\text{max}_{\theta} f(X \mid \theta) = 1 + \left( \frac{1}{2} - X \right) = 1.5 - X
$$

Final LRT Statistic:

$$
\lambda(X) =
\begin{cases}
\frac{1}{1.5 - X} & \text{if } X < \frac{1}{2} \\
1 & \text{if } X \geq \frac{1}{2}
\end{cases}
$$

## c)

Find the likelihood ratio test (LRT) of size $\alpha = 0.05$ for the above hypotheses.

### Answer

1. Rejection Region:

From part (b), $\lambda(X) = 1$ for $X \geq \frac{1}{2}$, and is increasing for $X < \frac{1}{2}$. So to make the test most powerful while maintaining the correct size, we reject for large values of $X$.

2. Size Condition:

$$
P_{\theta=0}(X > k) = 1 - k = 0.05 \quad \Rightarrow \quad k = 0.95
$$

Final LRT:

Reject $H_0$ when $X > 0.95$

Note: The LRT coincides with the UMP test derived in part (a) because:

1. Although the MLR is not strictly monotone in $X$, the density under $H_1$ favors larger values of $X$
2. The test based on rejecting for large $X$ values maximizes power subject to size, satisfying both Neyman-Pearson and UMP conditions